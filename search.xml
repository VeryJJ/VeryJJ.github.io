<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL子查询很慢的问题分析]]></title>
    <url>%2F2018%2F05%2F04%2FMySQL%E5%AD%90%E6%9F%A5%E8%AF%A2%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[慢查询案例1DELETE FROM settlement_invoice_attachment g1 WHERE demand_id in (SELECT id FROM settlement_invoice_demand g2 WHERE statement_id = 1802065000000074956) 乍眼一看，上述SQL如此简单，且demand_id和statement_id字段都是建了索引，即使是Review也会认为是OK没问题的。 然而，实际情况却是个慢查询，情况如下： explain明细 settlement_invoice_attachment是全表查 注：rows 2689 是因为用的测试环境，真线环境数据是几十万级别 子查询 原理分析(上述SQL子查询为什么这么慢)经验之谈 当看到SQL执行计划中select_type字段出现“DEPENDENT SUBQUERY”的时候，要打起精神了！着重分析下潜在风险！ 基础知识：Dependent SubQuery意味着什么？ 官方含义为： SUBQUERY: 子查询中的第一个SELECT； DEPENDENT SUBQUERY: 子查询中的第一个SELECT， 取决于外面的查询。 换句话说，就是子查询的g2查询执行方式依赖于外层g1的查询结果什么意思呢？它以为着两步走： 第一步：【先执行外部SQL查询】MySQL根据”DELETE FROM settlement_invoice_attachment g1 WHERE” 得到一个大结果集t1，其数据量就是全表所有行了，假设是85万行。 第二步：【后执行内部SQL子查询】第一步的大结果集t1中的每一条记录，都将与子查询SQL组成新的查询语句：SELECT id FROM settlement_invoice_demand g2 WHERE statement_id = 1802065000000074956 AND id = %t1.demand_id%。等于说，子查询要执行85万次……即使这两部查询都用到了索引，也是巨慢的。 优化策略 改写SQL为JOIN的方式 12DELETE ah FROM settlement_invoice_attachment ah INNER JOIN settlement_invoice_demand de ON ah.demand_id = de.id WHERE de.statement_id = 1802065000000074956; 拆成独立SQL多次执行 平时怎么识别？ 看子查询出现的位置 若子查询出现在WHERE从句中，而且是出现在IN（）中，则需要引起注意，用Explain瞧瞧（并不是子查询放IN（）里就一定是全表扫，本案例用，将DELETE改成SELECT就不是DEPENDENT SUBQUERY） 数据库原理 MySQL处理子查询时，会(优化)改写子查询，但优化的不是很友好，一直受业界批评比较多 有时候优化的挺糟糕的，特别是WHERE从句中的IN（）子查询 MySQL 子查询的弱点 mysql 在处理子查询时，会改写子查询。通常情况下，我们希望由内到外，先完成子查询的结果，然后再用子查询来驱动外查询的表，完成查询。 例如：select * from test where tid in(select fk_tid from sub_test where gid=10)通常我们会感性地认为该 sql 的执行顺序是： 1、sub_test 表中根据 gid 取得 fk_tid(2,3,4,5,6)记录。2、然后再到 test 中，带入 tid=2,3,4,5,6，取得查询数据。 但是实际mysql的处理方式为：select from test where exists (select from sub_test where gid=10 and sub_test.fk_tid=test.tid)mysql 将会扫描 test 中所有数据，每条数据都将会传到子查询中与 sub_test 关联，子查询不会先被执行，所以如果 test 表很大的话，那么性能上将会出现问题。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Provider暴露服务]]></title>
    <url>%2F2018%2F05%2F02%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Provider%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Dubbo Provider暴露服务的流程中，需要掌握几个核心抽象对象 未完待续]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务端业务处理不成功，应该返回HTTP 200 还是 HTTP 4XXX系列？]]></title>
    <url>%2F2018%2F04%2F23%2F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E4%B8%9A%E5%8A%A1%E5%A4%84%E7%90%86%E4%B8%8D%E6%88%90%E5%8A%9F%EF%BC%8C%E5%BA%94%E8%AF%A5%E8%BF%94%E5%9B%9EHTTP-200-%E8%BF%98%E6%98%AF-HTTP-4XXX%E7%B3%BB%E5%88%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[场景其实，纠结只出现在例如保存表单的场景，如果服务端因各种业务上的原因（校验不通过，状态不满足等）导致保存未成功，并要返回对应的提示信息，此时服务端回应此HTTP 请求时，是用 “200 + json” 还是用“400 + 错误信息”？ 在公司内不同项目间，两种风格都有，且小伙伴们各执己见。 我这么看首先，我先表达我赞同“200 + json”的方式。 更具体些，服务端所有的Controller Method对返回值做统一的Response包装 1234567891011121314样例1：&#123; "code": 200, "data": &#123;...&#125;, "message":"操作成功"&#125;样例2：&#123; "success": false "data": &#123;...&#125;, "code": 100409, "message":"数据已存在"&#125; 我的观点1. 协议分层对于RPC请求，存在两个层面的操作结果 [1] HTTP请求本身的结果 ———— 业务无关性，与网络、框架层面相关[2] 业务处理的结果 ———— 强业务逻辑相关性，与网络、框架层面无关 为什么我们会有本贴讨论的话题与分歧，或者说为什么大部分人觉得http code不够试用，是因为实际开发应用场景中，尝试着只用http code 去表达上述两个层面的结果。 分层表示的优点： RPC请求， 是可以基于不同的底层协议的， 比如我们用的HTTP协议，很容易替换成ZeroMQ, RabbitMQ, UDP， 基于TCP的自定义协议…… 只要能实现一问一答模型的协议，都是可以用的。这个时候， HTTP协议只是一种底层协议， 底层协议的错误号，并不应该被上层协议使用。 2. HTTP Code 表达能力局限性虽然HTTP协议非常友好的定义了诸多的HTTP Code码，但在实际开发应用中，对于繁多的应用场景，HTTP Code的表达能力显得力不从心，部分场景仍旧不能避免的辅以Response Body信息。加之这些HTTP Code并不是应用开发中的绝对标准。 3. HTTP Code 语义表达的不统一性[1] 同样是HTTP 4XX系列，不同系统的解释也是不一样的[2] 同样是”参数校验不通过”的业务问题，不同系统使用的HTTP码也是不一样的 4. Http Code数量有限，表达能力有限这个应该很好理解，大家应该也有体会。 5. 系统集成友好性如果我们把HTTP协议当作一种传输层协议看待，200 可以很好表达， 整个底层传输都是没有问题， 包括负载均衡系统， nginx， 反向代理， fast cgi守护程序都是工作正常的。 而返回各种HTTP Status Code经常会让外部使用者非常的困惑，特别是他们对HTTP Status Code有一定了解，却对你的系统不甚了解的情况下。 所以，除了考虑ajax请求的处理，还要考虑整个调用的中间链路以及框架集成方面的因素 返回200能避免CDN等中间商替换或缓存 国内的通信运营商画蛇添足根据HTTP状态码给替换成导航页或广告推广页面 对于系统审计程序不友好，例如 HTTP Response Code = 4XX的请求算请求成功？请求失败？请求异常？————无法区分！ 6. 扩展性 返回200OK，扩展性更强，修改的时候只需要修改字段而不需要特别处理Status Code 易于与真正的400错误区分，方便审计和分析。而实际上，当服务器能够正常返回，证明服务器已经正确的理解并得出相应的结果（并且这个结果也是预定义的，并非未知），这显然与400的定义不符。 返回200更优。保不准哪天某种状态是HTTP协议不支持的，保不准哪个需要新增的字段是HTTP协议没有的]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>WEB</tag>
        <tag>REST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Spring Bean注册]]></title>
    <url>%2F2018%2F04%2F22%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Spring-Bean%E6%B3%A8%E5%86%8C%2F</url>
    <content type="text"><![CDATA[相信大家对于Dubbo Provider/Consumer的配置非常熟练，但这背后的实现原理清楚吗？如果有不太清楚的朋友，可以再往下阅读下。 知识点 自定义 Spring XML Bean机制 背景据我们所知 Spring 注解方式声明Bean的方式是在Class上打上@Component注解（@Component的扩展注解也可），当Spring容器启动时，Spring会自动扫面所有带有@Component注解的Class，自动注册到Bean容器中。 例如： 1234@Componentpublic class Student &#123; //do something&#125; 也可以通过XML文件配置的方式声明Bean 例如： 1&lt;bean id="Student" class="com.test.spring.beans.Student"&gt;&lt;/bean&gt; 但，回过头来看Dubbo，我们并没有通过上述的方式去声明Dubbo配置中的Bean，却也能像使用Spring Bean一样用@Autowire去注入Dubbo服务的Bean，这其中的原理是什么呢？让我们从源码中找答案。 原理启动Spring容器在SpringBoot+Dubbo的搭配中，Java应用的启动入口main方法一般会这么写。通过此步骤去启动Java程序并将Dubbo Bean注册Spring容器。 12345678910111213141516package com.sample;@ComponentScan(basePackages = &#123; "com.sample.myapp"&#125;)@SpringBootApplication@EnableSchedulingpublic class MyApplication &#123; public static void main(String[] args) &#123; //启动Spring容器 SpringApplication application = new SpringApplication(MyApplication.class, "classpath:/spring/dubbo-config.xml"); //指定Dubbo配置文件 application.run(args); &#125;&#125; Spring如何识别Dubbo 自定义Bean标签Spring为了支持用户自定义类加载到Spring容器，提供了org.springframework.beans.factory.xml.NamespaceHandler接口和org.springframework.beans.factory.xml.NamespaceHandlerSupport抽象类，NamespaceHandler#init方法会在对象的构造函数调用之后、属性初始化之前被DefaultNamespaceHandlerResolver调用。dubbo的DubboNamespaceHandler类正是继承了NamespaceHandlerSupport，其代码实现如下： 1234567891011121314151617181920public class DubboNamespaceHandler extends NamespaceHandlerSupport &#123; static &#123; Version.checkDuplicate(DubboNamespaceHandler.class); &#125; public void init() &#123; registerBeanDefinitionParser("application", new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser("module", new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser("registry", new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser("monitor", new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser("provider", new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser("consumer", new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser("protocol", new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser("service", new DubboBeanDefinitionParser(ServiceBean.class, true)); registerBeanDefinitionParser("reference", new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser("annotation", new AnnotationBeanDefinitionParser()); &#125;&#125; registerBeanDefinitionParser方法使用的是父抽象类NamespaceHandlerSupport的默认实现，第一个参数是elementName，即元素名称，即告诉Spring你要解析哪个标签，第二个参数是BeanDefinitionParser的实现类，BeanDefinitionParser是Spring用来将xml元素转换成BeanDefinition对象的接口。dubbo的DubboBeanDefinitionParser类就实现了这个接口，负责将标签转换成bean定义对象BeanDefinition。 所以，以后想要了解Dubbo Bean初始化相关细节，可以查看DubboBeanDefinitionParser#parse的代码实现。 例如： Dubbo Bean 会有哪些默认设置 dubbo服务提供者使用dubbo:service标签时，如果既不设置id，也不设置name，则dubbo给ServiceBean在Spring容器中定义的ID是什么？ Dubbo xml文件中的配置是怎么作用到Dubbo Bean中去的 关于NamespaceHandlerSupport spring.handlers # 指定xml namespace的解析handler类 spring.schemas # 指定xml xsd文件位置 dubbo.xsd # 设计你要的xml配置格式 DubboNamespaceHandler # 自定义NamespaceHandler,完成从xml中读取配置内容，并转换成Spring Bean进行注册 Spring容器会默认加载classpath/META-INF下的spring.handlers和spring.schemas两个文件，来加载xsd和对应的NamespaceHandler,所以dubbo-config-spring包下的META-INF目录下也有这两个文件 练习DEMO1. 设计配置属性和JavaBean 设计好配置项，并通过JavaBean来建模，本例中需要配置People实体，配置属性name和age（id是默认需要的） 12345public class People &#123; private String id; private String name; private Integer age; &#125; 2. 编写XSD文件 为上一步设计好的配置项编写XSD文件，XSD是schema的定义文件，配置的输入和解析输出都是以XSD为契约，本例中XSD如下 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;xsd:schema xmlns="http://veryjj/cutesource/schema/people" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:beans="http://www.springframework.org/schema/beans" targetNamespace="http://veryjj/cutesource/schema/people" elementFormDefault="qualified" attributeFormDefault="unqualified"&gt; &lt;xsd:import namespace="http://www.springframework.org/schema/beans" /&gt; &lt;xsd:element name="people"&gt; &lt;xsd:complexType&gt; &lt;xsd:complexContent&gt; &lt;xsd:extension base="beans:identifiedType"&gt; &lt;xsd:attribute name="name" type="xsd:string" /&gt; &lt;xsd:attribute name="age" type="xsd:int" /&gt; &lt;/xsd:extension&gt; &lt;/xsd:complexContent&gt; &lt;/xsd:complexType&gt; &lt;/xsd:element&gt; &lt;/xsd:schema&gt; 关于xsd:schema的各个属性具体含义就不作过多解释，可以参见http://www.w3school.com.cn/schema/schema_schema.asp &lt;xsd:element name=”people”&gt;对应着配置项节点的名称，因此在应用中会用people作为节点名来引用这个配置 &lt;xsd:attribute name=”name” type=”xsd:string” /&gt;和&lt;xsd:attribute name=”age” type=”xsd:int” /&gt;对应着配置项people的两个属性名，因此在应用中可以配置name和age两个属性，分别是string和int类型 完成后需把xsd存放在classpath下，一般都放在META-INF目录下（本例就放在这个目录下） 3. 编写NamespaceHandler和BeanDefinitionParser完成解析工作 1234567891011121314151617181920212223242526 public class MyNamespaceHandler extends NamespaceHandlerSupport &#123; public void init() &#123; registerBeanDefinitionParser("people", new PeopleBeanDefinitionParser()); &#125; &#125; public class PeopleBeanDefinitionParser extends AbstractSingleBeanDefinitionParser &#123; protected Class getBeanClass(Element element) &#123; return People.class; &#125; protected void doParse(Element element, BeanDefinitionBuilder bean) &#123; String name = element.getAttribute("name"); String age = element.getAttribute("age"); String id = element.getAttribute("id"); if (StringUtils.hasText(id)) &#123; bean.addPropertyValue("id", id); &#125; if (StringUtils.hasText(name)) &#123; bean.addPropertyValue("name", name); &#125; if (StringUtils.hasText(age)) &#123; bean.addPropertyValue("age", Integer.valueOf(age)); &#125; &#125; &#125; 4. 编写spring.handlers和spring.schemas串联起所有部件 spring提供了 spring.handlers和spring.schemas这两个配置文件来完成这项工作，这两个文件需要我们自己编写并放入META-INF文件夹 中，这两个文件的地址必须是META-INF/spring.handlers和META-INF/spring.schemas，spring会默认去 载入它们，本例中spring.handlers如下所示： spring.handlers 1http\://veryjj/cutesource/schema/people=study.schemaExt.MyNamespaceHandler spring.schemas 1http\://veryjj/cutesource/schema/people.xsd=META-INF/people.xsd 以上就是载入xsd文件 5. 使用自定义schema定义Spring Bean 到此为止一个简单的自定义配置以完成，可以在具体应用中使用了。使用方法很简单，和配置一个普通的spring bean类似，只不过需要基于我们自定义schema，本例中引用方式如下所示： 12345678&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:cutesource="http://veryjj/cutesource/schema/people" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://veryjj/cutesource/schema/people http://veryjj/cutesource/schema/people.xsd"&gt; &lt;cutesource:people id="cutesource" name="黄老师" age="27"/&gt; &lt;/beans&gt; 其中xmlns:cutesource=”http://veryjj/cutesource/schema/people&quot; 是用来指定自定义schema，xsi:schemaLocation用来指定xsd文件。&lt;cutesource:people id=”cutesource” name=”黄老师” age=”27”/&gt;是一个具体的自定义配置使用实例。 6. 注入自定义schema定义的Spring Bean 跟Spring Bean的注入方式完全一样，按你喜欢的方式来。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Dubbo可以这么学]]></title>
    <url>%2F2018%2F04%2F22%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Dubbo%E5%8F%AF%E4%BB%A5%E8%BF%99%E4%B9%88%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[最近面试工作颇多，Dubbo作为的微服务主流技术架构，也是分布式系统中面试的高频考题之一。但从面试的过程中得到的反馈，大家对于Dubbo的关注以及掌握程度基本都处于会基本使用的程度，基本没遇到有对Dubbo框架做学习研究的求职者。 求职者一般只会聊下面两个话题： Dubbo 是什么东西？ 答：RPC框架/微服务框架，在实际工作中用Dubbo做业务功能服务化。 Dubbo的工作原理是什么样的？ 答：Provider端将服务注册到Zookeeper中，Consumer端从Zookeeper获取Provider，然后就可以调用API了。 一般情况下关于Dubbo的基本都聊到此结束了，虽然说没回答错，但也忒简洁了吧，连Dubbo架构图中（下图）的内容都没说完整，而这并不是面试官想得到的讯息。 Dubbo作为主流的微服务技术框架，必然有其优秀的一面，也是学习RPC框架思想很好的素材 Dubbo 应该掌握哪些内容？（个人思路） 阅读Dubbo的用户手册以及开发手册。Dubbo.io 知晓Dubbo支持的功能 知晓Dubbo的各种扩展点 知晓Dubbo的设计思想（这里不得不说Dubbo.io的文档说明写的非常详细、到位，甚至一度让我觉得没有写Blog的必要） Dubbo 核心流程源码实现 Dubbo Bean的集成 Provider 注册、暴露服务 Consumer 注册、订阅服务 Consumer 调用实现 Provider 处理请求 Dubbo SPI机制 Dubbo Filter机制 思考些高级的 Dubbo各可配机制主流选择的优缺点 register remoting rpc Dubbo Cluster Dubbo 怎么做服务治理 策略路由 降级 熔断 Dubbo 性能基线&amp;性能调优 框架扩展 服务监控 流量分析 那么，逐步的去落实吧！如果开始、请务必坚持！]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty 核心对象梳理-1]]></title>
    <url>%2F2018%2F04%2F08%2FNetty-%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1%E6%A2%B3%E7%90%86-1%2F</url>
    <content type="text"><![CDATA[《Netty In Action》阅读笔记摘要 What is NettyNetty是一款用于快速开发高性能的网络应用程序的Java框架 它封装了网络编程的复杂性 Key words: 是一款Java语言的开发框架 封装、提供程序快速网络编程的能力 高性能 Netty是完全异步和事件驱动的 Netty 核心组件ChannelChannel 是Java NIO的一个基本构造 可以将Channel 看作是连接的载体。因此，它可以被打开、被关闭、连接、断开连接。 事件Channel连接上发生的事件。可以等价的理解为 epoll 中关于每个Socket事件，例如：EPOLLIN, EPOLLOUT, EPOLLHUP的回调 回调类似于常见的回调，Netty内部用回调来处理事件； 可理解为 epoll 中关于每个Socket事件的回调，例如：EPOLLIN, EPOLLOUT, EPOLLHUP的回调。 应用程序可以自定义回调，感知Netty网络通信的事件。 ChannelFutureFuture提供了另一种在操作完成时通知应用程序的方式。 JDK中的java.util.concurrent.Future 在使用上是阻塞调用的，不优雅。Netty 提供了另一种实现：ChannelFuture，用于在执行异步操作的时候使用。 Netty的每个出站I/O都将返回一个ChannelFuture。 使用ChannelFuture时，可以配合使用ChannelFutureListener。只要实现operationComplete() 回调即可，非常方便。且支持多ChannelFuture。 1234567891011121314151617Channel channel = ...;//Does not block ChannelFuture future = channel.connect(new InetSocketAddress("192.168.0.1", 25));​future.addListener(new ChannelFuturelistener()&#123; @Override public void operationComplete(ChannelFuture future)&#123; if (future.isSuccess())&#123; ByteBuf buffer = Unpooled.copiedBuffer("Hello", Charset.defaultCharset()); ChannelFuture wf = future.channel().writeAndFlush(buffer); ... &#125; else &#123; Throwable cause = future.cause(); cause.printStackTrace(); &#125; &#125;&#125;) ChannelHandler可以初步理解为每个ChannelHandler实例都类似于一种为了响应特定事件而被执行的回调。 PSNetty 的内部实现细节跟Linux epoll的用法很相似，熟悉Linux Epoll以及编程模型的朋友来说可以对比着来学习，寻找类同点、差异点以及差异的原因。 Netty 同时支持OIO, NIO, EPOLL等多路复用模式，我是以熟悉的epoll作为切入熟悉的内部流程原理。 Netty的组件和设计 Channel ———— Socket； EventLoop ———— 控制流、多线程处理、并发； ChannelFuture ———— 异步通知； Channel 接口Netty的Channel接口所提供的API，大大地降低了使用Socket类的复杂性。 EventLoopGroup 接口主要作用 用于注册Channel 执行部分Runnable任务 这里重点讲下“注册Channel”，在实际编程或应用时，每个Channel都是向EventLoopGroup注册的，由EventLoopGroup按照指定的策略方法，将Channel注册到EventLoopGroup下某个具体的EventLoop当中去。 12345678910111213141516171819202122232425262728293031323334public interface EventLoopGroup extends EventExecutorGroup &#123; ... /** * Register a &#123;@link Channel&#125; with this &#123;@link EventLoop&#125;. The returned &#123;@link ChannelFuture&#125; * will get notified once the registration was complete. */ ChannelFuture register(Channel channel); ...&#125;public abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup &#123; ... @Override public ChannelFuture register(Channel channel) &#123; return next().register(channel); &#125; ...&#125;public abstract class MultithreadEventExecutorGroup extends AbstractEventExecutorGroup &#123; ... @Override public EventExecutor next() &#123; return chooser.next(); &#125; ... EventLoop 接口EventLoop定义了Netty的核心抽象，用于处理连接的生命周期中所发生的事件。 一个EventLoopGroup包含一个或者多个EventLoop； 一个EventLoop在它的生命周期内只和一个Thread绑定； 所有由EventLoop处理的I/O事件都将在它专有的Thread上被处理； 一个Channel在它的生命周期内只注册于一个EventLoop； 一个EventLoop可能会被分配给一个或多个Channel； 在这种设计中，一个给定的Channel的I/O操作都是由相同的Thread执行的，实际上消除了对于同步的需要。 ChannelFuture 接口Netty中所有的I/O操作都是异步的。所有我们需要一种用于在之后的某个时间点确定其结果的方法。 为此，Netty提供了ChannelFuture接口，其addListener()方法注册了一个ChannelFutureListener，以便在某个操作完成时得到通知。 ChannelHandler 接口顾名思义，Channel的Handler，它充当了所有处理入站和出站数据的应用程序逻辑的容器。ChannelHandler的方法是由网络事件触发的。 ChannelPipeline 接口ChannelPipeline为ChannelHandler链提供了容器，并定义了用于在该链上传播入站和出站事件流的API。当Channel被创建时，它会被自动的分配到它专属的ChannelPipeline。 ChannelPipleline中的ChannelHandler的执行顺序是由它们被添加的顺序所决定的。 编码器和解码器Netty用于网络通信，天然需要编码和解码。也是用ChannelPipeline + ChannelHandler的机制实现的。 所有由Netty提供的编码器/解码器适配器类都实现了ChannelOutboundHandler或者ChannelInboundHandler接口。 引导（Bootstrap）Netty的引导类为应用程序的网络层配置提供了容器。 类别 Bootstrap ServerBootstrap 网络编程中的作用 连接到远程主机和端口 绑定到一个本地端口 EventLoopGroup的数目 1 2 使用Netty的ChannelOption和属性 在每个Channel创建时都手动配置它可能会变得相当乏味。幸运的是，你不必这样做。相反，你可以使用option()方法来将ChannelOption应用到Bootstrap上。你所提供的值将会被自动应用到Bootstrap所创建的所有Channel。 引导DatagramChannel Bootstrap除了引导基于TCP协议的SocketChannel，也可以用于引导无连接的协议。Netty提供了各种DatagramChannel的实现。与面向连接的TCP相比，唯一区别是不再调用connect()方法，而是只调用bind()方法 123456789101112131415161718192021222324//使用Bootstrap和DatagramChannelBootstrap bootstrap = new Bootstrap();bootstap.group(new OioEventLoopGroup()) .channel(OioDatagramChannel.class) .handler(new SimpleChannelInboundHandler&lt;DatagramPacket&gt;()&#123; @Override public void channelRead0(ChannelHandlerContext ctx, DatagramPacket msg) throws Exception &#123; //Do something with the packet &#125; &#125;);ChannelFuture future = bootstrap.bind(new InetSocketAddress(0));future.addListener(new ChannelFutureListener()&#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception&#123; if (channelFuture.isSuccess())&#123; System.out.println("Channel bound"); &#125; else &#123; System.out.println("Bind attempt failed"); channelFuture.cause().printStackTrace(); &#125; &#125;&#125;) 我的理解 引导的根对象是 EventLoopGroup，间接的负责监听、处理所有Channel的网络事件。 EventLoop是EventLoopGroup内的成员，每个EventLoop与具体的线程绑定。也可以理解一个线程，一个EventLoop。 EventLoop直接负责处理其下所有Channel的网络事件。 ChannelHadler是Channel网络事件逻辑处理的容器，应用逻辑开发的重点就在此。 当一个Channel上来一个网络事件时，对应的EventLoop首先进行响应，并找到Channel所属的ChannelPipeline，Channel作为输入驱动一次ChannelPipeline。 ChannelPipeline 遍历其下ChannelHandler，逐个处理Channel的网络事件。 ChannelFuture可以同步等结果，也可以异步通知结果，都支持，自己选！ ByteBuf网络数据的基本单位是字节。Java NIO提供了ByteBuffer作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。 Netty的ByteBuffer替代品是ByteBuf，一个强大的实现，既解决了JDK API的局限性，又为网络应用程序的开发者提供了更好的API。 ByteBuf优点： 对于同一个数据buffer，维护readIndex, writeIndex两份索引 ByteBuf模式 堆缓冲区模式： 将数据存储在JVM的堆空间中，应用代码可直接访问缓冲区中的数据。 直接缓冲区模式： JDK 1.4引入的ByteBuffer类允许JVM实现直接使用操作系统的本地内容，这就避免了JAVA 应用在每次调用本地I/O操作前/后 需要将缓冲区的内容复制到一个与操作系统结合的中间缓冲区中。 缺点：因为数据不是在堆上，所以业务代码处理时不得不经过一次复制。 复合缓冲区： 为多个ByteBuf提供一个统一的聚合视图，可以根据需要向复合缓冲区中添加或者删除ByteBuf实例。 字节级操作 可以以字节的操作方式使用ByteBuf 随机访问索引 顺序访问索引 可丢弃字节 可读字节 可写字节 索引管理 indexOf / ByteBufProcessor 派生缓冲区 读/写操作 ByteBuf池化分配 为了降低分配和释放内存的开销，Netty通过interface ByteBufAllocator实现了ByteBuf的池化。 Unpooled 缓冲区 如果未能获取到一个ByteBufAllocator的引用，Netty提供一个简单的Unpooled工具类，它提供创建未池化的ByteBuf实例。 关于 ChannelFuture 和 ChannelPromise ChannelFuture read-only 没有返回值的异步通知、调用 DefaultFutureListeners -&gt; listeners[N] ChannelPromise writeable 可写异步执行结果的通知、调用 notifyListenerNow -&gt; 回到Listeners -&gt; 取出对应的Channel进行回调操作]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅聊分布式事务]]></title>
    <url>%2F2018%2F03%2F30%2F%E6%B5%85%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前言 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近很久没有写博客了，一方面是因为公司事情最近比较忙，另外一方面是因为在进行 CAP 的下一阶段的开发工作，不过目前已经告一段落了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来还是开始我们今天的话题，说说分布式事务，或者说是我眼中的分布式事务，因为每个人可能对其的理解都不一样。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式事务是企业集成中的一个技术难点，也是每一个分布式系统架构中都会涉及到的一个东西，特别是在微服务架构中，几乎可以说是无法避免，本文就分布式事务来简单聊一下。 数据库事务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等… &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接着，我们就说一下分布式事务。 分布式理论 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？ CAP定理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。 这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个时候有同学可能会把数据库的2PC（两阶段提交）搬出来说话了。OK，我们就来看一下数据库的两阶段提交。 对数据库分布式事务有了解的同学一定知道数据库支持的2PC，又叫做 XA Transactions。 MySQL从5.5版本开始支持，SQL Server 2005 开始支持，Oracle 7 开始支持。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，XA 是一个两阶段提交协议，该协议分为以下两个阶段： 第一阶段：事务协调器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交. 第二阶段：事务协调器要求每个数据库提交数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，如果有任何一个数据库否决此次提交，那么所有数据库都会被要求回滚它们在此事务中的那部分信息。这样做的缺陷是什么呢? 咋看之下我们可以在数据库分区之间获得一致性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果CAP 定理是对的，那么它一定会影响到可用性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果说系统的可用性代表的是执行某项操作相关所有组件的可用性的和。那么在两阶段提交的过程中，可用性就代表了涉及到的每一个数据库中可用性的和。我们假设两阶段提交的过程中每一个数据库都具有99.9%的可用性，那么如果两阶段提交涉及到两个数据库，这个结果就是99.8%。根据系统可用性计算公式，假设每个月43200分钟，99.9%的可用性就是43157分钟, 99.8%的可用性就是43114分钟，相当于每个月的宕机时间增加了43分钟。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上，可以验证出来，CAP定理从理论上来讲是正确的，CAP我们先看到这里，等会再接着说。 BASE理论&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是BASE理论，它是用来对CAP定理进行进一步扩充的。BASE理论指的是： Basically Available（基本可用） Soft state（软状态） Eventually consistent（最终一致性） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有了以上理论之后，我们来看一下分布式事务的问题。 分布式事务&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，要实现分布式事务，无外乎那几种解决方案。 一、两阶段提交（2PC）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和上一节中提到的数据库XA事务一样，两阶段提交就是使用XA协议的原理，我们可以从下面这个图的流程来很容易的看出中间的一些比如commit和abort的细节。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;两阶段提交这种解决方案属于牺牲了一部分可用性来换取的一致性。在实现方面，在 .NET 中，可以借助 TransactionScop 提供的 API 来编程实现分布式系统中的两阶段提交，比如WCF中就有实现这部分功能。不过在多服务器之间，需要依赖于DTC来完成事务一致性，Windows下微软搞的有MSDTC服务，Linux下就比较悲剧了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外说一句，TransactionScop 默认不能用于异步方法之间事务一致，因为事务上下文是存储于当前线程中的，所以如果是在异步方法，需要显式的传递事务上下文。 优点： 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能100%保证强一致） 缺点： 实现复杂，牺牲了可用性，对性能影响较大，不适合高并发高性能场景，如果分布式系统跨接口调用，目前 .NET 界还没有实现方案。 二、补偿事务（TCC）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段： Try 阶段主要是对业务系统做检测及资源预留 Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 举个例子，假入 Bob 要向 Smith 转账，思路大概是：我们有一个本地方法，里面依次调用1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些 缺点： 缺点还是比较明显的，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。 三、本地消息表（异步确保）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于ebay。我们可以从下面的流程图中看出其中的一些细节： 基本思路就是： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。 优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。在 .NET中 有现成的解决方案。 缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。 四、MQ 事务消息&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有一些第三方的MQ是支持事务消息的，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。 以阿里的 RocketMQ 中间件为例，其思路大致为： 第一阶段Prepared消息，会拿到消息的地址。 第二阶段执行本地事务，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。 也就是说在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了RocketMQ会定期扫描消息集群中的事务消息，这时候发现了Prepared消息，它会向消息发送者确认，所以生产方需要实现一个check接口，RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。 优点： 实现了最终一致性，不需要依赖本地数据库事务。 缺点： 实现难度大，主流MQ不支持，没有.NET客户端，RocketMQ事务消息部分代码也未开源。 五、Sagas 事务模型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Saga事务模型又叫做长时间运行的事务（Long-running-transaction）, 它是由普林斯顿大学的H.Garcia-Molina等人提出，它描述的是另外一种在没有两阶段提交的的情况下解决分布式系统中复杂的业务事务问题。你可以在这里看到 Sagas 相关论文。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们这里说的是一种基于 Sagas 机制的工作流事务模型，这个模型的相关理论目前来说还是比较新的，以至于百度上几乎没有什么相关资料。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该模型其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，那么Sagas工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如我们一次关于购买旅游套餐业务操作涉及到三个操作，他们分别是预定车辆，预定宾馆，预定机票，他们分别属于三个不同的远程接口。可能从我们程序的角度来说他们不属于一个事务，但是从业务角度来说是属于同一个事务的。 他们的执行顺序如上图所示，所以当发生失败时，会依次进行取消的补偿操作。 因为长事务被拆分了很多个业务流，所以 Sagas 事务模型最重要的一个部件就是工作流或者你也可以叫流程管理器（Process Manager），工作流引擎和Process Manager虽然不是同一个东西，但是在这里，他们的职责是相同的。在选择工作流引擎之后，最终的代码也许看起来是这样的 12345678910111213SagaBuilder saga = SagaBuilder.newSaga(&quot;trip&quot;) .activity(&quot;Reserve car&quot;, ReserveCarAdapter.class) .compensationActivity(&quot;Cancel car&quot;, CancelCarAdapter.class) .activity(&quot;Book hotel&quot;, BookHotelAdapter.class) .compensationActivity(&quot;Cancel hotel&quot;, CancelHotelAdapter.class) .activity(&quot;Book flight&quot;, BookFlightAdapter.class) .compensationActivity(&quot;Cancel flight&quot;, CancelFlightAdapter.class) .end() .triggerCompensationOnAnyError();camunda.getRepositoryService().createDeployment() .addModelInstance(saga.getModel()) .deploy(); 这里有一个 C# 相关示例，有兴趣的同学可以看一下。 优缺点 这里我们就不说了，因为这个理论比较新，目前市面上还没有什么解决方案，即使是 Java 领域，我也没有搜索的太多有用的信息。 转自：https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识鱼骨图]]></title>
    <url>%2F2018%2F03%2F29%2F%E8%AE%A4%E8%AF%86%E9%B1%BC%E9%AA%A8%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[今天，偶然的机会在同事电脑上漂见“鱼骨图”。晚上吃完饭就在思考这个问题：“为什么我从来没用过鱼骨图分析问题？鱼骨图适用于什么类型的分析？” 查了些资料追求普及，介绍的内容大同小异 概念摘要鱼骨分析法，又名 因果分析法，是一种发现问题”根本原因”的分析方法。 鱼骨图可以进一步被划分为： 整理问题型鱼骨图（各要素与特性值间不存在原因关系，而是结构构成关系） 原因型鱼骨图（鱼头在右，特性值通常以“为什么……”来写） 对策型鱼骨图（鱼头在左，特性值通常以“如何提高/改善……”来写） 怎么作图分析结构 A、针对问题点，选择层别方法（如人机料法环等）； B、按头脑风暴分别对各层别类别找出所有可能原因（因素）； C、将找出的各要素进行归类、整理，明确其从属关系； D、分析选取重要因素； E、检查各要素的描述方法，确保语法简明、意思明确; 分析要点 A、确定大要因（大骨）时，现场作业一般从“人机料法环”着手,管理类问题一般从“人事时地物”层别，应视具体情况决定； B、大要因必须用中性词描述（不说明好坏），中、小要因必须使用价值判断（如…不良）； C、头脑风暴时，应尽可能多而全地找出所有可能原因，而不仅限于自己能完全掌控或正在执行的内容。对人的原因，宜从行动而非思想态度面着手分析； D、中要因跟特性值、小要因跟中要因间有直接的原因-问题关系，小要因应分析至可以直接下对策； E、如果某种原因可同时归属于两种或两种以上因素，请以关联性最强者为准（必要时考虑三现主义：即现时到现场看现物，通过相对条件的比较，找出相关性最强的要因归类。）； F、选取重要原因时，不要超过7项，且应标识在最未端原因。 绘图过程 A、填写鱼头（按为什么不好的方式描述），画出主骨； B、画出大骨，填写大要因； C、画出中骨、小骨，填写中小要因； D、用特殊符号标识重要因素； 使用步骤(1) 查找要解决的问题； (2) 把问题写在鱼骨的头上； (3) 召集同事共同讨论问题出现的可能原因，尽可能多地找出问题； (4) 把相同的问题分组，在鱼骨上标出； (5) 根据不同问题征求大家的意见，总结出正确的原因； (6) 拿出任何一个问题，研究为什么会产生这样的问题； (7) 针对问题的答案再问为什么？这样至少深入五个层次（连续问五个问题）； (8) 当深入到第五个层次后，认为无法继续进行时，列出这些问题的原因，而后列出至少20个解决方法。 看了鱼骨图的概念介绍后，在脑海中立马出现了新的问题：”鱼骨图和思维导图有什么区别？” 鱼骨图和思维导图有什么区别？类同点 都是基于主题逐步分解、细化的过程 都是类树形结构 我的理解理解-1首先、思维导图和鱼骨图都是图形思维，而图形思维最大的特点就是将我们的思维结构化，并由此实现图形化。 而所谓的结构化一般就是构建逻辑思维，任何的逻辑思维都是基于这两个出发点而来的：1、分类；2、顺序；所以只要你的分类与顺序是一样的，那么你的逻辑思维是一样的，由此产生的图形思维也是一样的。而图形、分列、表格……只是图形思维具体的呈现方式。换句话说，在图形思维一样的前提下，不同的人选择了自己认为好的呈现方式让图形思维更可视化、更清晰化、更感性化。 另外，还有不同之处在于思维导图不仅仅能用于构建逻辑思维，还能扩展发散思维，以及还能强化思维记忆。这是和鱼骨图的区别之一！ 理解-2 【思维导图】在实际应用中，思维导图常用于个人，侧重于个人思维的发散，以求思维的全面性，思维导图的层次感是逻辑思维自然的产出。应用场景广泛。 【鱼骨图】常用在多人同时参与的头脑风暴场景，侧重于发挥团队智力逐层解剖问题，找到问题原因。往往会以”问题原因分析 + 跟进Action”作为结果产出。 鱼骨图作图工具 Xmind]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>分析图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写作技巧]]></title>
    <url>%2F2018%2F03%2F17%2FHexo%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[Hexo 写作的技巧与备忘，不喜勿喷。 链接： Hexo命令 文章插入图片原生Markdown语法原生Markdown语法插入图片有三种方式 1. 插入本地图片只需要在基础语法的括号中填入图片的位置路径即可，支持绝对路径和相对路径。例如： 1![image](/home/picture/1.png) 评价：不灵活不好分享，本地图片的路径更改或丢失都会造成markdown文件调不出图。 2. 插入网络图片只需要在基础语法的括号中填入图片的网络链接即可，现在已经有很多免费/收费图床和方便传图的小工具可选。例如： 1![image](http://baidu.com/pic/doge.png) 评价：将图片存在网络服务器上，非常依赖网络和网络图片存储 3. 把图片存入markdown文件用base64转码工具把图片转成一段字符串，然后把字符串填到基础格式中链接的那个位置。基础用法： 1![avatar](data:image/png;base64,iVBORw0......) 这个时候会发现插入的这一长串字符串会把整个文章分割开，非常影响编写文章时的体验。如果能够把大段的base64字符串放在文章末尾，然后在文章中通过一个id来调用，文章就不会被分割的这么乱了。比如： 12![avatar][doge] [doge]:data:image/png;base64,iVBORw0...... 评价：麻烦，费劲。 Hexo方式安装插件与配置 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save，这是下载安装一个可以上传本地图片的插件 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 使用方式在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片 1![你想输入的替代文字](xxxx/图片名.jpg) 注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo装修记录]]></title>
    <url>%2F2018%2F03%2F17%2FHexo%E8%A3%85%E4%BF%AE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前篇《个人博客搭建-结缘Hexo》 从时间投入上来说，搭建Hexo可谓是分分钟的事情，装修可花了我近一天的时间。 Hexo 是一个开放、扩展性强的框架，样式风格、功能都可以通过主题包、插件来实现。完全可以根据个人口味来装修你的博客。 选主题关于Hexo的主题，你可以问度娘”Hexo Themes”网友好评度高的主题，也可以去官网自行挑选https://hexo.io/themes/ 需要提前说明的是：不是选上对应主题，所有功能就有了，这也是我为什么会单独写这篇备忘的原因。 经过各种试用和对比，我最终选择了：NexT这个主题 Go Github，理由如下： （Most Important）界面符合我口味。NexT 中的Pisces主题，样例：IIssNan’s Notes NexT主题集成的插件多，极大的方便了初级的小白用户，详见：NexT主题的_config.xml配置项 博客评论 Baidu Analytics / Google Analytics 文章阅读数量 baidu push algolia_search / local_search 背景画布特效 highlight_theme 等等 支持手机端 安装主题Hexo theme 统一放在Hexo根目录的themes目录下，每个主题一个子目录。 如果主题是Github上的，推荐使用如下命令下载 1git clone https://github.com/iissnan/hexo-theme-next.git themes/next 然后修改hexo根目录下的_config.xml，切换主题 OK, hexo g -&gt; hexo s 看下效果吧 主题配置找到 hexo/themes/next/_config.xml，一项项阅读熟悉吧，注释写的很详细。 基础配置基础配置项： 菜单配置：主页、关于、标签、分类、归档等 头像 打赏 社交主页 侧边栏 主题的_config.xml会引用hexo的_config.xml中基础配置，所以请同时配置hexo的_config.xml，例如：博客抬头、语种、相关数据目录等 背景特效效果图示 配置next主题的_config.xml 支持搜索 文章阅读计数NexT提供两种插件方式：1、leancloud_visitors（国内的） 和 2、firestore(谷歌的)我选用的是leancloud_visitors，配置相对简单一些 配置LeanCloud 注册：https://leancloud.cn打开LeanCloud官网，进入注册页面注册。完成邮箱激活后，点击头像，进入控制台页面创建新应用，如下： 创建名称为Counter的Class 修改NexT的_config.xml配置文件 123456# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: "你的App Id" app_key: "你的App Key" PV/UV Google Analytics12# Google Analyticsgoogle_analytics: '你的Google Analytics Code' 问题解决标签和分类页面不显示问题当时切换NexT主题后，侧边栏的标签、分类点击时，是无法正常显示标签和分类的，属：Cannot Get /tags/ 若出现此问题，请按下方式解决在hexo 目录下执行 步骤一： 1hexo new page 'tags' 步骤二： 编辑刚新建的页面，将页面的类型设置为tags，主题会自动为这个页面显示标签云。 123456---title: TagClouddate: 2018-03-17 15:31:21type: "tags"comments: false #注意：如果有启动多说或Disqus评论，需要关闭评论，添加comments字段并设置为false--- “分类”同理~ 其他推荐 hexo的next主题个性化配置教程]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客搭建-结缘Hexo]]></title>
    <url>%2F2018%2F03%2F17%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA-%E7%BB%93%E7%BC%98Hexo%2F</url>
    <content type="text"><![CDATA[Github Pages + Hexo 搭建个人博客偶然的机会，看到”***.github.io”域名的个人博客，瞬间就来了兴趣，莫非Github能方便的创建个人博客？ 咨询了下度娘，知道了用GitHub Pages + Hexo 搭建个人博客的方式，于是说干就干！ PS: 我用的是Macbook + Shell，某些操作Windows的朋友可能要转化翻译成Windows上的命令。 1. 创建Github仓库首先，个人Github账号应该有吧？如果没有，先去注册一个。 然后，在Github上新建一个仓库，如下图： 确保新建的仓库以Github Pages方式发布 OK，此步骤完成了！ 2. 安装Hexo正式使用Hexo前，请先安装Node.js 和Git 安装Node.js去官网下载并安装：Link 安装Git 作为玩Github的程序员，默认你已经安装了Git 安装Hexo 新建一个目录作为Hexo的根目录（PS: 后续Hexo相关的功能以及写博客都基于此目录） 进入新建的目录 12345npm install hexo-cli -ghexo init #初始化网站npm installhexo g #hexo generate的简写，意思生成博客站点hexo s #hexo server的简写，即启动运行hexo的站点，这一步之后就可以通过http://localhost:4000 查看了 常用Hexo命令 1234hexo c : hexo clean 清除hexo已生成的publichexo g : hexo generate 重新生成hexo站点hexo s : hexo server 运行hexo站点。注：本地运行时，在hexo上做的修改保存后即生效的，不用重新hexo ghexo d : hexo deploy 将hexo发布到github上去。 3. Hexo deploy 到Github 编辑根目录下_config.yml文件 1234deploy: type: git repo: https://github.com/VeryJJ/VeryJJ.github.io.git #这里的网址填你自己的 branch: master 安装hexo deploy插件： npm install hexo-deployer-git –save 在Hexo目录下执行hexo d hexo d 成功后，就大工告成拉！你可以在浏览器输入***.github.io(你新建的github.io仓库)，就能看到你的个人博客拉！ 以后写博客的步骤为： 在电脑本地hexo new ‘文章名’ 丰富你的文章 hexo g hexo d 发布 博客搭建好了，但相信你会觉得它好丑，没关系，请继续阅读下一篇Hexo的装修总结 参考链接 我是如何利用Github Pages搭建起我的博客，细数一路的坑]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 场景化下的命令备忘]]></title>
    <url>%2F2018%2F03%2F15%2FGit%20%E5%9C%BA%E6%99%AF%E5%8C%96%E4%B8%8B%E7%9A%84%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[查看分支1234git branchgit branch -a 查看远端所有分支git branch -v 查看本地分支，以及分支上最新的commit提交信息git branch -vv 在-v基础上，多现实本地分支和远程分支的关联关系 查看某次commit的修改1git show commit号 查看某个文件的历史修改记录12345678910111213141516171819202122232425262728293031323334git log 文件名git log -p 文件名git log --author=提交人 只查看提交人的提交记录git log --pretty=oneline 单行显示提交记录git log --name-only 显示每次commit修改的文件列表git log --name-status 查看commit记录里的文件修改状态git log --grep=&apos;abc&apos; 显示commit描述匹配abc的commit记录git log -S &quot;代码内容&quot; 按代码内容搜索commit记录，如果代码内容部分想用正则表达式，则将-S换成-Ggit log --pretty=&apos;%H %Cblue%cd %C(yellow)%cn %Cred%s&apos; 按commit号+提交日期+提交人+commit标题 显示pretty格式%H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字 %_ae 作者的电子邮件地址 （由于新浪博客显示问题，请去除 %_ae 中的 _ ）%_ad 作者修订日期（可以用 -date= 选项定制格式）（由于新浪博客显示问题，请去除 % ad 中的 _ ）%ar 作者修订日期，按多久以前的方式显示 %cn 提交者(committer)的名字 %_ce 提交者的电子邮件地址（由于新浪博客显示问题，请去除 %_ce 中的 _ ）%_cd 提交日期 （由于新浪博客显示问题，请去除 %_cd 中的 _ ）%cr 提交日期，按多久以前的方式显示 %d: ref名称%s: 提交的信息标题%b: 提交的信息内容%Cred: 切换到红色 %Cgreen: 切换到绿色 %_Cblue: 切换到蓝色 （由于新浪博客显示问题，请去除 %_Cblue 中的 _）%Creset: 重设颜色 %C(...): 制定颜色, as described in color.branch.* config option %n: 换行 查看未commit的本地修改12git diff git diff 文件名 git diff 比较commit之间的差异123git diff commit 比较HEAD与commit之间的差异git diff commit_1 commit_2 比较两个commit之间的差异git diff commit_1..commit_2 与git diff commit_1 commit_2 一样效果 从服务器拉代码git pull --rebase (推荐)会把本地未push得commit放到缓冲区，然后把远程最新版本拉过来，再应用本地commit，这样不会造成本地有新commit时，merge的效果。 git pull 直接更新，若本地和远端都有新commit，都执行自动merge。 拉分支git checkout -b branchName 创建本地新分支 git checkout -b branchName remotes/origin/branchName 以远端分支创建本地新分支 git push origin $newBranch:$newBranch 将本地分支提交到远端进行创建 删除远程分支123$ git push origin :master# 等同于$ git push origin --delete master 提交修改git add . 将修改加到stage状态区 git commint -m &quot;注释&quot; git push 推送所有分支 git push origin develop 只推送develop分支 添加文件git add -A 删除文件git rm 文件名 git rm -r 目录名 Pushgit push push所有分支 git push origin master 将本地主分支推到远程主分支 git push –u origin master 将本地主分支推到远程（如无远程主分支则创建，用于初始化远程仓库） git push origin &lt;local_branch&gt; 创建远程分支，origin是远程仓库名。 git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; 创建远程分支 强制push如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做git pull合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用–force选项。 git push --force origin 合并分支mergegit merge remotes/origin/mc-s-3 将远端mc-s-3分支merge到本地 rebasegit rebase develop git rebase remotes/origin/develop 配置mergetoolgit config –global merge.tool bc3 git config –global mergetool.bc3.path 软件执行文件地址 merge策略1234567891011Git merge 策略的总结:1、使用 -s 指定策略，使用 -X 指定策略的选项2、默认策略是recursive3、策略有 ours，但是没有theirs (Git老版本好像有)4、策略ours直接 忽略 合并分支的任何内容，只做简单的合并，保留分支改动的存在5、默认策略recursive有选项ours 和 theirs6、-s recursive -X ours 和 -s ours 不同，后者如第3点提到直接忽略内容，但是前者会做合并，遇到冲突时以自己的改动为主7、-s recursive -X theirs的对立面是 -s recursive -X ours`注：-s recursive -X ours 合并分支，冲突时以本地为主` 回退未commit的修改git checkout [path] 将指定路径的修改还原到最新版本 回退已commit，未push的修改git reset HEAD &lt;file&gt; --mixed 选项：默认的 --soft 选项：改动会回退到stage状态 --hard 选项：改动会直接丢失。 git rebase -i 想要删除的commit的前一个commit号。 出来的界面里，将想要删除的commit描述改为drop，保存即可。 回退已push的修改git revert 指定的commit号。跳出来的界面，选择要回退的commit内容（取消前面的#） 可以随便选某个commit删除 若revert一个merge的commit，则要指定parent 号 git revert commit 号 -m 1。 这样就选parent 1，那么parent 1又是哪一个呢？一般来说，如果你在master上mergezhc_branch,那么parent 1就是master，parent 2就是zhc_branch. 重排commit顺序git rebase -i commit号 出来的界面中，将列出来的commit行重新排序再保存，就等于修改commit顺序了。 修改commit的描述未push方法一： git rebase -i commit号 对应commit号前改为edit，保存。出来后git commit --amend。将commit描述修改掉，保存。 出来后再git rebase --continue即可。 方法二： git commit --amend 修改最近的一次commit 代码仓库迁移git clone --bare robbin_site robbin_site.git git remote remove origin git remote add origin git@120.27.160.167:ZCY/doc-round-1.git git push –-all -–progress origin 导出指定版本的代码版本 git archive -o ../updated.zip HEAD $(git diff --name-only HEAD^) 例如：git archive -o ./version.zip 指定commit号 或者 git archive --format zip -output &quot;./archive.zip&quot; HEAD tag功能创建taggit tag -a v1.0.0 -m &apos;备注&apos; 查看taggit tag 切换taggit checkout tag名 删除taggit tag -d v1.0.0 指定commit打taggit tag -a v1.0.0 commit号 发布标签git push origin v1.0.0 将本地v1.0.0标签推送到git服务器 git push origin -tags 将本地所有tag一次性推送到git服务器 创建补丁当前分支所有超前master的提交：git format-patch -M master 某次提交以后的所有patch:git format-patch 4e16 --4e16指的是commit名 从根到指定提交的所有patch:git format-patch --root 4e16 某两次提交之间的所有patch:git format-patch 365a..4e16 -o &lt;patch_dir&gt; --365a和4e16分别对应两次提交的名称 某次提交（含）之前的几次提交：git format-patch –n 07fe --n指patch数，07fe对应提交的名称 故，单次提交即为： git format-patch -1 07fe 应用补丁方法一（推荐）12345678910111、在同一个仓库下找到对应的commit号2、切换到对应分支下，git cherry-pick commit 号3、如果冲突，git mergetool 解决冲突。4、git status根据提示commit代码，并pushcherry-pick 一个commit区间git cherry-pick &lt;start-commit-id&gt;^..&lt;end-commit-id&gt; start-commit-id是版本树里较早的commitcherry-pick一个merge commitgit cherry-pick &lt;commit-id&gt; -m parent-number -m代表 --mainline实际例子：git cherry-pick 32b234 -m 1 1，2分别代表什么 查看未push到远程仓库的commit1、查看到未传送到远程代码库的提交次数12345 git status //只能看次数显示结果类似于这样：# On branch master# Your branch is ahead of &apos;origin/master&apos; by 2 commits. 2、查看到未传送到远程代码库的提交描述/说明12345git cherry -v显示结果类似于这样：+ b6568326134dc7d55073b289b07c4b3d64eff2e7 add default charset for table items_has_images+ 4cba858e87752363bd1ee8309c0048beef076c60 move Savant3 class into www/includes/class/ 3、查看到未传送到远程代码库的提交详情1234567891011121314git log master ^origin/master这是一个git log命令的过滤，^origin/master可改成其它分支。显示结果类似于这样：commit 4cba858e87752363bd1ee8309c0048beef076c60Author: Zam &lt;zam@iaixue.com&gt;Date: Fri Aug 9 16:14:30 2013 +0800 move Savant3 class into www/includes/class/commit b6568326134dc7d55073b289b07c4b3d64eff2e7Author: Zam &lt;zam@iaixue.com&gt;Date: Fri Aug 9 16:02:09 2013 +0800 add default charset for table items_has_images 查看两个分支的差异查看dev中有，而master中没有的1234git log dev ^master反之：查看master中有，dev中没有的git log master ^dev 查看dev中比master多了哪些提交（A比B多了哪些，就把A放..右边）1git log master..dev 不在乎谁多谁少，只想看差异的提交1git log --left-right dev...master #--left-right 会帮助显示差异的commit属于哪个分支 整个目录比较差异详情1git difftool develop..pre-online --dir Git stash 暂存1234567891011121314151617git stash 将当前工作区里未commit的修改放到暂存区，将代码恢复到最近的一次修改git stash list 查看暂存区的列表git show stash@&#123;0&#125; see the last stash git stash pop apply lastest stash and remove it from th list git stash clear 清空暂存栈git stash apply stash@&#123;1&#125; 指定暂存区里的某一次stash，应用到本地 删除本地git branch -a 能看到，而远程已经删掉的分支记录1git fetch -p 更改时间显示方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647--date=(relative|local|default|iso|rfc|short|raw) Only takes effect for dates shown in human-readable format, such as when using &quot;--pretty&quot;. log.date config variable sets a default value for log command’s --date option.--date=relative shows dates relative to the current time, e.g. &quot;2 hours ago&quot;.--date=local shows timestamps in user’s local timezone.--date=iso (or --date=iso8601) shows timestamps in ISO 8601 format.--date=rfc (or --date=rfc2822) shows timestamps in RFC 2822 format, often found in E-mail messages.--date=short shows only date but not time, in YYYY-MM-DD format.--date=raw shows the date in the internal raw git format %s %z format.--date=default shows timestamps in the original timezone (either committer’s or author’s).####格式化显示例子：--date=format:&apos;%Y-%m-%d %H:%M:%S&apos;参数：%a Abbreviated weekday name%A Full weekday name%b Abbreviated month name%B Full month name%c Date and time representation appropriate for locale%d Day of month as decimal number (01 – 31)%H Hour in 24-hour format (00 – 23)%I Hour in 12-hour format (01 – 12)%j Day of year as decimal number (001 – 366)%m Month as decimal number (01 – 12)%M Minute as decimal number (00 – 59)%p Current locale&apos;s A.M./P.M. indicator for 12-hour clock%S Second as decimal number (00 – 59)%U Week of year as decimal number, with Sunday as first day of week (00 – 53)%w Weekday as decimal number (0 – 6; Sunday is 0)%W Week of year as decimal number, with Monday as first day of week (00 – 53)%x Date representation for current locale%X Time representation for current locale%y Year without century, as decimal number (00 – 99)%Y Year with century, as decimal number%z, %Z Either the time-zone name or time zone abbreviation, depending on registry settings; no characters if time zone is unknown%% Percent sign 全局更改方式1git config --global log.date relative 代码量统计当天提交的代码量1git log --author=&quot;$(git config --get user.name)&quot; --no-merges --since=1am --stat 统计报告-gitstats 用GitStatX图形化工具查看 统计报告-gitinspector1gitinspector --format=html --since=2018-01-01 --until=2018-12-30 --timeline --localize-output -w ./ &gt; ~/tmp/gitinspector/zcy-payment-center-201801.html gitinspector命令说明123456789101112131415161718192021222324252627282930313233343536➜ car-manage git:(master) gitinspector --help用法：/usr/local/bin/gitinspector [选项]... [目录] 在目录列出有关库的信息,如果没有指定目录，那么将使用现目录。如果有多个目录，将采用指定的最后一个目录长选项的强制性参数对短选项也适用布尔参数只能给予长选项 -f, --file-types=EXTENSIONS 一串逗号分隔的文件类型 这些文件将会被用于计算统计数据. 默认文件类型: java,c,cc,cpp,h,hh,hpp,py,glsl,rb,js,sql -F, --format=FORMAT 指定生成的输出文件的格式； 默认格式是&apos;text&apos; 和 可选格式: html,htmlembedded,text,xml --grading[=BOOL] 按照学生成评判项目的格式， 显示统计数据和信息； 等同于 -HlmrTw 选项 -H, --hard[=BOOL] 记录行数并且寻找重复的内容; 如果数据库较大，这个可能会需要一些时间 -l, --list-file-types[=BOOL] 列出所有现在的数据库分支的文件格式 -L, --localize-output[=BOOL] 在翻译版本存在的前提下，将输出结果翻译到系统语言 -m --metrics[=BOOL] 在分析提交时，检查特定指标 -r --responsibilities[=BOOL] 显示每位作者主要职责 --since=DATE 只显示从特定时间起的结果 -T, --timeline[=BOOL] 显示提交时间轴, 包括作者名称 --until=DATE 只显示特定时间前的结果 -w, --weeks[=BOOL] 按周来显示统计数据，而非月 -x, --exclude=PATTERN 按特定格式排除不应该被统计 的文件，作者名字或邮箱;可以按文件名，作者名， 作者邮箱。可以重复 -h, --help 显示这个帮助信息并退出 --version 显示版本信息并退出gitinspector 会过滤信息并且仅统计那些修改，增加或减少，指定文件类型的提交，如需详细信息，请参考 -f 或 --file-types 选项]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
</search>
