<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Dubbo源码解析-Consumer启动]]></title>
    <url>%2F2018%2F06%2F02%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Consumer%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[Dubbo Consumer的启动过程和Provider一样，以DubboNamespaceHandler为起点，去解析代码配置中的ReferenceBean。 1public class ReferenceBean&lt;T&gt; extends ReferenceConfig&lt;T&gt; implements FactoryBean, ApplicationContextAware, InitializingBean, DisposableBean &#123; ReferenceBean同样既继承了ReferenceConfig，又实现了InitializingBean。也是在afterProperitesSet()中去执行服务引用 ReferenceBean afterPropertiesSet ReferenceConfig init() 完成service interface的class， methods解析 获取Service 注册中心registeries配置信息，用于向注册中西订阅service 检测是否配置有Dubbo Mock， Dubbo Stub createProxy()完成ReferenceConfig + Registeries ——》 Dubbo Service Invoker的转化。createProxy()返回时，返回的是被Proxy后的Invoker，即外层加了Dubbo Filter Chain。 DubboProtocol.refer(…) DubboProtocolDubboProtocol.class 作为Dubbo RPC层的具体实现协议，尤其完成Consumer中向注册中心真正订阅的动作。 1234567891011121314151617181920212223242526272829303132public class DubboProtocol extends AbstractProtocol &#123; ... public &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; serviceType, URL url) throws RpcException &#123; optimizeSerialization(url); // create rpc invoker with url. DubboInvoker&lt;T&gt; invoker = new DubboInvoker&lt;T&gt;(serviceType, url, getClients(url), invokers); invokers.add(invoker); return invoker; &#125; private ExchangeClient[] getClients(URL url) &#123; // whether to share connection boolean service_share_connect = false; int connections = url.getParameter(Constants.CONNECTIONS_KEY, 0); // if not configured, connection is shared, otherwise, one connection for one service if (connections == 0) &#123; service_share_connect = true; connections = 1; &#125; ExchangeClient[] clients = new ExchangeClient[connections]; for (int i = 0; i &lt; clients.length; i++) &#123; if (service_share_connect) &#123; clients[i] = getSharedClient(url); &#125; else &#123; clients[i] = initClient(url); &#125; &#125; return clients; &#125; ...&#125; Invoker12345678910111213141516171819public interface Invoker&lt;T&gt; extends Node &#123; /** * get service interface. * * @return service interface. */ Class&lt;T&gt; getInterface(); /** * invoke. * * @param invocation * @return result * @throws RpcException */ Result invoke(Invocation invocation) throws RpcException;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class DubboInvoker&lt;T&gt; extends AbstractInvoker&lt;T&gt; &#123; //与Service Provider端的连接 private final ExchangeClient[] clients; //同一service的invokers集合，集群时用到。 private final Set&lt;Invoker&lt;?&gt;&gt; invokers; ... public DubboInvoker(Class&lt;T&gt; serviceType, URL url, ExchangeClient[] clients, Set&lt;Invoker&lt;?&gt;&gt; invokers) &#123; super(serviceType, url, new String[]&#123;Constants.INTERFACE_KEY, Constants.GROUP_KEY, Constants.TOKEN_KEY, Constants.TIMEOUT_KEY&#125;); this.clients = clients; // get version. this.version = url.getParameter(Constants.VERSION_KEY, "0.0.0"); this.invokers = invokers; &#125; @Override protected Result doInvoke(final Invocation invocation) throws Throwable &#123; RpcInvocation inv = (RpcInvocation) invocation; final String methodName = RpcUtils.getMethodName(invocation); inv.setAttachment(Constants.PATH_KEY, getUrl().getPath()); inv.setAttachment(Constants.VERSION_KEY, version); ExchangeClient currentClient; if (clients.length == 1) &#123; currentClient = clients[0]; &#125; else &#123; currentClient = clients[index.getAndIncrement() % clients.length]; &#125; try &#123; boolean isAsync = RpcUtils.isAsync(getUrl(), invocation); boolean isOneway = RpcUtils.isOneway(getUrl(), invocation); int timeout = getUrl().getMethodParameter(methodName, Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT); if (isOneway) &#123; boolean isSent = getUrl().getMethodParameter(methodName, Constants.SENT_KEY, false); currentClient.send(inv, isSent); RpcContext.getContext().setFuture(null); return new RpcResult(); &#125; else if (isAsync) &#123; ResponseFuture future = currentClient.request(inv, timeout); RpcContext.getContext().setFuture(new FutureAdapter&lt;Object&gt;(future)); return new RpcResult(); &#125; else &#123; RpcContext.getContext().setFuture(null); return (Result) currentClient.request(inv, timeout).get(); &#125; &#125; catch (TimeoutException e) &#123; throw new RpcException(RpcException.TIMEOUT_EXCEPTION, "Invoke remote method timeout. method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125; catch (RemotingException e) &#123; throw new RpcException(RpcException.NETWORK_EXCEPTION, "Failed to invoke remote method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125; &#125; ...&#125; ReferenceConfig 核心数据12345678910111213141516171819202122232425262728public class ReferenceConfig&lt;T&gt; extends AbstractReferenceConfig &#123; //核心是DubboProtocol private static final Protocol refprotocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); //集群模式下使用，此处不解释 private static final Cluster cluster = ExtensionLoader.getExtensionLoader(Cluster.class).getAdaptiveExtension(); //注册中心地址 private final List&lt;URL&gt; urls = new ArrayList&lt;URL&gt;(); // interface name private String interfaceName; private Class&lt;?&gt; interfaceClass; // client type private String client; // url for peer-to-peer invocation private String url; // Service的方法列表 private List&lt;MethodConfig&gt; methods; // default config private ConsumerConfig consumer; private String protocol; // invoker 的代理类 private transient volatile T ref; //原生的service invoker private transient volatile Invoker&lt;?&gt; invoker; private transient volatile boolean initialized; private transient volatile boolean destroyed;&#125; ConsumerModel经过ReferenceConfig一番处理后，最终会得到：Reference Dubbo Service Name, InvokerRef, Service Methods, ReferenceConfig Instance。 这些信息会封装成ConsumerModel，放到ApplicationModel.class中去全局统一记录Consumer的情况。 1234567public class ConsumerModel &#123; private ReferenceConfig metadata; private Object proxyObject; private String serviceName; private final Map&lt;Method, ConsumerMethodModel&gt; methodModels = new IdentityHashMap&lt;Method, ConsumerMethodModel&gt;(); ...&#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[翻译] REST API必须是超文本驱动的]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%BF%BB%E8%AF%91-REST-API%E5%BF%85%E9%A1%BB%E6%98%AF%E8%B6%85%E6%96%87%E6%9C%AC%E9%A9%B1%E5%8A%A8%E7%9A%84%2F</url>
    <content type="text"><![CDATA[原文地址：Roy T. Fielding: REST APIs must be hypertext-driven I am getting frustrated by the number of people calling any HTTP-based interface a REST API. Today’s example is the SocialSite REST API. That is RPC. It screams RPC. There is so much coupling on display that it should be given an X rating. 我是越来越失望了，许多人把任何基于HTTP的接口叫做REST API，眼前的例子就是SocialSite REST API。那是RPC，实实在在的RPC。它与显示如此耦合，再差也莫过于此 What needs to be done to make the REST architectural style clear on the notion that hypertext is a constraint? In other words, if the engine of application state (and hence the API) is not being driven by hypertext, then it cannot be RESTful and cannot be a REST API. Period. Is there some broken manual somewhere that needs to be fixed? 基于超文本概念，如何才能确保清晰的REST架构风格呢？这样来说吧，如果应用程序状态引擎（即API）不是由超文本驱动的，那就不是RESTful也不是REST的API。就这么简单。某些REST方面的破手册是否该修正一下呢？ API designers, please note the following rules before calling your creation a REST API: API的设计者们，把你们的那些东西叫做REST API前请注意以下的规则： A REST API should not be dependent on any single communication protocol, though its successful mapping to a given protocol may be dependent on the availability of metadata, choice of methods, etc. In general, any protocol element that uses a URI for identification must allow any URI scheme to be used for the sake of that identification. [Failure here implies that identification is not separated from interaction.] REST API不应依赖于任何特定的通讯协议，在采用某个具体协议时可能受限于元数据的有效性、方法的选择等。通常，协议元素使用URI作标识时，对该标识必须允许运用任何URI方案。[ 不符合这一点意味着标识与交互没有分离 ] A REST API should not contain any changes to the communication protocols aside from filling-out or fixing the details of underspecified bits of standard protocols, such as HTTP’s PATCH method or Link header field. Workarounds for broken implementations (such as those browsers stupid enough to believe that HTML defines HTTP’s method set) should be defined separately, or at least in appendices, with an expectation that the workaround will eventually be obsolete. [Failure here implies that the resource interfaces are object-specific, not generic.] REST API不应修改通讯协议中预留出来作为补充或修正标准协议用途的资源，例如HTTP的PATCH方法和Link head域。违背了这一原则的方案应当单独定义，或者至少在附录中标注出来这样的方案最终会废弃掉。[ 不符合这一点意味着资源接口是对象相关的，不通用 ] A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state, or in defining extended relation names and/or hypertext-enabled mark-up for existing standard media types. Any effort spent describing what methods to use on what URIs of interest should be entirely defined within the scope of the processing rules for a media type (and, in most cases, already defined by existing media types). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] REST API应当将绝大部分精力放在媒体类型的定义上，或者是扩展关系名称的定义、已有超文本标记中的标准媒体类型等方面，以实现资源的表述、操作应用程序状态。任何类似于对某某URI应当使用什么样的方法等工作，都应当完全定义在特定媒体类型的处理规则范围中（绝大部分情况下已有媒体类型都已经定义好了这些规则）。[ 不符合这一点意味着交互是由其它信息驱动，而不是超文本 ] A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. [Failure here implies that clients are assuming a resource structure due to out-of band information, such as a domain-specific standard, which is the data-oriented equivalent to RPC’s functional coupling]. REST API决不能定义固定的资源名称或者层次关系（这是明显的客户端、服务器端耦合），服务器必须可以自由控制自己的名称空间。应当像HTML forms和URI模板一样，通过媒体类型和链接关系指示客户端如何构造正确的URI。[ 不符合这一点意味着客户端在通过其它信息（例如领域相关标准）猜测资源结构，这是数据导向，类似于RPC的函数耦合 ] A REST API should never have “typed” resources that are significant to the client. Specification authors may use resource types for describing server implementation behind the interface, but those types must be irrelevant and invisible to the client. The only types that are significant to a client are the current representation’s media type and standardized relation names. [ditto] REST API决不能使用对客户端有重要意义的类型化资源。规范的作者可能使用资源类型描述接口背后的服务器端实现，但这些类型必须与客户端无关，对客户端不可见。对客户端唯一有意义的类型是当前的表述性媒体类型和标准的关系名称。[ 同上 ] A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user’s manipulation of those representations. The transitions may be determined (or limited by) the client’s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] 使用REST API应该只需要知道初始URI（书签）和一系列针对目标用户的标准媒体类型（任何客户端都了解用来操作该媒体类型的API）。这样所有的应用程序状态转换都通过这样的方式进行：服务器在返回的表述性消息中提供选项，由客户端进行选择，或者是伴随着用户对表述性内容的操作而进行。状态转换由客户端对媒体类型的了解程度和资源通讯机制决定，或者受限于这些因素，这些问题都可以根据实际情况得以改善的（例如使用javascript这种code-on-demand技术）。[ 不符合这一点意味着交互是由其它信息驱动，而不是超文本 ] There are probably other rules that I am forgetting, but the above are the rules related to the hypertext constraint that are most often violated within so-called REST APIs. Please try to adhere to them or choose some other buzzword for your API. 也许还有其它一些规则我一时想不起来了，但在那些所谓的REST API中通常都违背了上面这些超文本约束相关的规则，请纠正这些错误或者改用其它称谓吧]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>REST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API命名实践]]></title>
    <url>%2F2018%2F05%2F18%2FRESTful-API%E5%91%BD%E5%90%8D%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[引言在互联网高度普及的今天，作为一名Web开发者，如果你还没听说过“REST”这个技术名词，出门都不好意思跟人打招呼。尽管如此，对于REST这个泊来品的理解，大多数人仍然停留在“盲人摸象”的阶段。 有人认为，在Web Controller层写的API就是REST API。而且，从开发角度对于URI的命名、HTTP Mehthod的选择没有建立起规范的意识。这样是不优雅的！（没有对错之分） 作为带着问题学习总结的我，未打算通过本篇文档全面的阐述清楚REST，而是尽量的总结一些理论和思考，一起探讨！ REST 的诞生Web 技术发展Web开发技术的发展可以粗略划分成以下几个阶段： 静态内容阶段：在这个最初的阶段，使用Web的主要是一些研究机构。Web由大量的静态HTML文档组成，其中大多是一些学术论文。Web服务器可以被看作是支持超文本的共享文件服务器。 可以想象下当时的HTTP请求只有“GET”，且MIME为“HTML或TEXT” CGI程序阶段：在这个阶段，Web服务器增加了一些编程API。通过这些API编写的应用程序，可以向客户端提供一些动态变化的内容。Web服务器与应用程序之间的通信，通过CGI（Common Gateway Interface）协议完成，应用程序被称作CGI程序。 脚本语言阶段：在这个阶段，服务器端出现了ASP、PHP、JSP、ColdFusion等支持session的脚本语言技术，浏览器端出现了Java Applet、JavaScript等技术。使用这些技术，可以提供更加丰富的动态内容。 瘦客户端应用阶段：在这个阶段，在服务器端出现了独立于Web服务器的应用服务器。同时出现了Web MVC开发模式，各种Web MVC开发框架逐渐流行，并且占据了统治地位。基于这些框架开发的Web应用，通常都是瘦客户端应用，因为它们是在服务器端生成全部的动态内容。 RIA应用阶段：在这个阶段，出现了多种RIA（Rich Internet Application）技术，大幅改善了Web应用的用户体验。应用最为广泛的RIA技术是DHTML+Ajax。Ajax技术支持在不刷新页面的情况下动态更新页面中的局部内容。同时诞生了大量的Web前端DHTML开发库，例如Prototype、Dojo、ExtJS、jQuery/jQuery UI等等，很多开发库都支持单页面应用（Single Page Application）的开发。其他的RIA技术还有Adobe公司的Flex、微软公司的Silverlight、Sun公司的JavaFX（现在为Oracle公司所有）等等。 移动Web应用阶段：在这个阶段，出现了大量面向移动设备的Web应用开发技术。除了Android、iOS、Windows Phone等操作系统平台原生的开发技术之外，基于HTML5的开发技术也变得非常流行。 REST 的诞生从上述Web开发技术的发展过程看，Web从最初其设计者所构思的主要支持静态文档的阶段，逐渐变得越来越动态化。Web应用的交互模式，变得越来越复杂：从静态文档发展到以内容为主的门户网站、电子商务网站、搜索引擎、社交网站，再到以娱乐为主的大型多人在线游戏、手机游戏。 Web发展到了1995年，在CGI、ASP等技术出现之后，沿用了多年、主要面向静态文档的HTTP/1.0协议已经无法满足Web应用的开发需求，因此需要设计新版本的HTTP协议。在HTTP/1.0协议专家组之中，有一位年轻人脱颖而出，显示出了不凡的洞察力，后来他成为了HTTP/1.1协议专家组的负责人。这位年轻人就是Apache HTTP服务器的核心开发者Roy Fielding，他还是Apache软件基金会的合作创始人。 所以，REST 并不是在互联网诞生之初就有的，它是在HTTP/1.1协议中才出现的，由Roy Thomas Fielding这位大神对Web技术做了深入的总结和分析，提出的一套网络软件的架构风格理论框架，当时Fielding为这种架构风格取了一个轻松愉快的名字：“REST” ———— Representational State Transfer（表述性状态转移） Roy Thomas Fielding 关于REST的论文 https://www.ics.uci.edu/~fielding/pubs/dissertation/fielding_dissertation.pdf REST 详解REST 架构风格问题：REST 究竟是什么？是一种新的技术、一种新的架构、还是一种新的规范？ 首先，REST是Web自身的架构风格，也是世界上最成功的分布式应用架构风格。它是为运行在互联网环境的分布式超媒体系统量身定制的。 REST是一种架构风格！ REST是一种架构风格！ REST是一种架构风格！ 所以，就会存在实际开发工作中即使没有正确的理解和应用REST，但也能顺利的完成开发工作。也正因为如此，给开发工作中推广正确实践和统一风格带来不小的困难。因为大多数程序员总是在寻找最快解决问题，最快完成需求的方式，怎么简单怎么来。 解读 RESTREST ———— Representational State Transfer (表现层状态转化) 从“Representational State Transfer”这个定义去理解REST架构风格原则。 1. 资源（Resources）REST 的名称“表现层状态转化”中，省略了主语。“表现层”其实指的是“资源（Resources）”的“表现层” 资源是一种看待服务器的方式，此处指的“资源”是一个抽象的概念，它不仅仅指服务器端真实存在的文件、数据库表，而是指任何可被名词表述的东西。所以在定义“资源”时可以要多抽象就多抽象。 对于客户端，可以将服务器端看作是由很多离散的资源组成。服务端可以用URI（统一资源定位符）指向资源，每种资源都对应一个特定的URI。要向获取这个资源，访问它的URI就可以了，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓“上网”，就是与互联网上一系列的“资源”互动，调用它的URI。 2. 表现层（Representation）“资源”是一种信息实体，它可以有多在的表现形式。我们把“资源”具体呈现出来的形式，叫做它的“表现层（Representation）” 比如，文本信息可以用txt格式表现，也可以用HTML格式 、XML格式、JSON格式表现，甚至可以用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的表现形式。资源的具体表现形式，应该在HTTP请求的的头部信息中用Accept和Content-Type字段指明，这两个字段才是对“表现层”的描述。 详见HTTP MIME明细 3. 状态转化（State Transfer）HTTP协议是一个无状态的协议，这意味着所有资源的状态都保存在服务器端。因为客户端想要操作服务器，必须通过某种手段，让服务器端资源发生“状态转化”。而这种转化是建立在表现层之上的，所以就是“表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体对应HTTP协议中的HTTP Method：GET、POST、PUT、PATCH、DELETE、HEAD、OPTIONS。每一种HTTP Method代表资源状态转化的一种约定的方式。 HTTP 动词 对于资源的具体操作类型，有HTTP动词表示。 常用的HTTP动词如下： 123456789- GET : 从服务器取出资源（一个或多个）- POST : 在服务器新建一个资源，并返回创建后的完整资源到客户端- PUT : 在服务器以覆盖形式，全量更新资源，并返回更新后的完整资源到客户端- PATCH : 在服务器端更新资源，但只更新指定的内容- DELETE : 在服务器端删除资源 其中，GET、PUT、PATCH、DELETE都应该是幂等的。 另外，HEAD、OPTIONS对于团队开发来说基本不用。 123- HEAD : 获取资源的元数据- OPTIONS : 获取信息，关于资源的哪些属性是客户端可以改变的 4. 综述综合上面的解读，总结一下什么是REST架构风格： (1) 服务器端的任何信息和数据都要被抽象资源化；(2) 资源用URI进行表述，每一个URI代表一种资源；(3) 客户端与服务器之间，基于某种表现层形式，互相传递资源；(4) 客户端与服务器之间，基于HTTP Method对服务器端资源的操作，实现“表现层状态转化”； REST 与 RESTful定义： 如果一个架构符合REST原则，就称它为RESTful架构 如果HTTP API的设计符合REST原则，那么可称它为RESTful API 所以，回到开篇讲的大多数人对于REST还是处于“盲人摸象”的阶段，回想下自己和身边的同事，在工作中经常交流到的REST API或RESTful API，其实只能算个HTTP API吧？ REST 风格优点架构风格不是非此即彼的是非题，在实际开发中可以自主的选择是否应用REST风格。那么，如果应用REST风格会带来哪些优势呢？ 从面向操作编程，转变为面向资源编程。更面向对象，架构更清晰、松耦合。 我们应该确定的认为系统由“资源+对资源的操作”组成，而不是由“操作”组成 面向操作编程会导致API膨胀，功能重复度高。 统一URI命名风格，URI具备很强的可读性，具备自解释的能力。服务器资源层次目录清晰。 状态无关。确保系统横向扩展的能力。 超文本驱动。确保系统演化的能力。 REST实践体会1. URI命名难度变大在没有要求URI必须用资源名词来组成URI时，URI的命名从来不是什么难事，常见的命名风格有： 动词+名词 /deposit/getUsers: 获取某个项目保证金用户列表 /orders/submitAudit: 订单提交审核 /cart/add: 商品加购物车 URI全局唯一即可 /finance/budget/getPurchaseplanNextAuditOrgList：我有点小无语… 为什么会这样： 我们平时搞系统是这样的： 有新建用户功能 新建用户需要一个URL 往这个URL发送的数据要定义好 开始写后端和前端 这是以操作为第一位的设计方法，首先确认了一个操作，然后围绕这个操作把周边需要的东西建设好，这种方式当然可以架构出一个系统，甚至是一个好系统，但是偶尔会有些问题： 操作之间是会有关联，你的设计容易变成“第2个操作要求第1个操作进行过”，这种关系多起来你的系统就乱了 你的URL设计会缺乏一致性 操作通常被认为是有副作用（Side Effect）的，所以很少有人基于操作去设计缓存之类的东西 该怎么应对？ 确实，REST是高度抽象的理论和风格，在实际开发中会面对各种复杂的功能和场景，导致很难完全的应用REST风格。当我们在争论REST风格到底如何设计才是正宗时，发现心中的困惑不仅没有降低，反而增加了。 我的想法：仍以真正的系统需求为出发点，使用REST风格让系统的架构更清晰，让系统的开发协作更高效。部分不适合REST的场景应该灵活变通。 回到URI的命名： 坚持URI仍以资源为导向，清晰的表述服务器端资源目录 保障URI资源层次清晰的情况下，只允许在URI最末一级添加动词，例如：/market/orders/1/audit 如果某些动作是HTTP动词表示不了的，考虑把动作抽象成一种资源 比如：网上汇款，从账户1向账户2汇款100元，错误的URI 1POST /accounts/1/transfer/500/to/2 正确的写法是把动词transfer改成名词transaction 1POST /transaction?from=1&amp;to=2&amp;amount=100 2. 用不用HTTP PATCHPATCH 作为HTTP的Method之一，其实它是2010年3月份才正式成为HTTP Method的，详见：RFC 5789 也正因为PATCH出现的晚, 所以并不是所有Web容器都支持，反而目前实现了PATCH方法的Web容器很少 几个常见Web容器实现PATCH方法的情况，供参考： Apache HttpComponents HttpClient version 4.2 or later 支持了 PATCH 目前 JDK7 的 HttpURLConnection 未实现 PATCH TOMCAT 7 也不行 PlayFramework 2 也不支持 Spring 3.2 开始支持 PATCH 方法，但要选对部署的容器 JBoss Netty 支持 PATCH，可见： http://docs.jboss.org/netty/3.2/api/org/jboss/netty/handler/codec/http/class-use/HttpMethod.html]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>RESTful</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引合并]]></title>
    <url>%2F2018%2F05%2F06%2FMySQL%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[历史背景MySQL 5.0版本之前，一个表一次只能选择并使用一个索引。 MySQL 5.1版本开始，引入了Index Merge Optimization技术，使得MySQL支持一个表一次查询同时使用多个索引。 官方文档：MySQL Index Merge Optimization Index Merge Optimization支持三种合并算法 The Index Merge Intersection Access Algorithm 对应SQL 中的 AND 场景 The Index Merge Union Access Algorithm 对应SQL中的 OR 场景（where条件是等值判断） The Index Merge Sort-Union Access Algorithm 对应SQL中的 OR 场景（where条件是范围查询） 注：索引合并(Index Merge)的使用取决于optimizer_switch系统变量的index_merge，index_merge_intersection，index_merge_union和index_merge_sort_union标志的值。默认情况下，所有这些标志都打开。 要仅启用特定算法，请将index_merge设置为关闭，并仅启用其他应允许的其他算法。 ##关于”Index Merge Intersection Access Algorithm”的疑问 针对 MySQL Index Merge Optimization Intersection Algorithm AND 场景的 index merge optimization为什么会比使用单个索引来的高效？ 设想： 使用单个索引的场景 选中选择性高的索引先获得一份数据 在再mysql服务器端用using where的方式，按第二条件进行过滤，得到最终满足所有条件的数据行。 同时使用表内多个索引的场景 按每个索引，在索引树里拿只满足本索引条件的行数据 将两份行数据，放一块进行交集运算。 从索引的次数、磁盘IO、内存交接运算来看，事情没变少、反而变多了。 自我初版解释合理的解释样例SQL1select * from table_sample where column_1 = A AND column_2 = B; 前提条件，SQL中不能有范围查询，如果存在范围查询，数据库优化器默认使用单索引方式，不用index merge optimization SQL的WHERE从句中的所有条件字段都有对应的索引，否则问题就来了，肯定会在内存中有次using_where的。 单表多Index并行检索时，拿到的是数据行地址，以上述SQL为例，即拿到了两份行数据地址：Index Column_1的行数据地址集，Index Column_2的行数据地址集 再在内存中完成两份行数据地址集的交集运算（只需要比地址） 此时，再决定是否回表拿更多的数据。 如果字段中有primary key，就不用回表啦！ 如上的执行步骤，就会比较合理。有效率上的优势。 【更进一步】 explain 显示type 为 index_merge时，到底要不要引起关注？【需要引起注意】 拿着SQL琢磨下，是否还有优化的空间，例如：采用组合索引；强制走单索引（需要对比测试看效果，还要看业务数据场景和增长趋势）； 注： 当索引本身信息可以覆盖select的字段时（或是select count(*)）,效率会很高，因为内存索引里已经能提供返回的数据了，不用回表。 当索引本身信息不能覆盖select的字段时，就要回表查行数据了，性能差别很大。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL子查询很慢的问题分析]]></title>
    <url>%2F2018%2F05%2F04%2FMySQL%E5%AD%90%E6%9F%A5%E8%AF%A2%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[慢查询案例1DELETE FROM settlement_invoice_attachment g1 WHERE demand_id in (SELECT id FROM settlement_invoice_demand g2 WHERE statement_id = 1802065000000074956) 乍眼一看，上述SQL如此简单，且demand_id和statement_id字段都是建了索引，即使是Review也会认为是OK没问题的。 然而，实际情况却是个慢查询，情况如下： explain明细 settlement_invoice_attachment是全表查 注：rows 2689 是因为用的测试环境，真线环境数据是几十万级别 子查询 原理分析(上述SQL子查询为什么这么慢)经验之谈 当看到SQL执行计划中select_type字段出现“DEPENDENT SUBQUERY”的时候，要打起精神了！着重分析下潜在风险！ 基础知识：Dependent SubQuery意味着什么？ 官方含义为： SUBQUERY: 子查询中的第一个SELECT； DEPENDENT SUBQUERY: 子查询中的第一个SELECT， 取决于外面的查询。 换句话说，就是子查询的g2查询执行方式依赖于外层g1的查询结果什么意思呢？它以为着两步走： 第一步：【先执行外部SQL查询】MySQL根据”DELETE FROM settlement_invoice_attachment g1 WHERE” 得到一个大结果集t1，其数据量就是全表所有行了，假设是85万行。 第二步：【后执行内部SQL子查询】第一步的大结果集t1中的每一条记录，都将与子查询SQL组成新的查询语句：SELECT id FROM settlement_invoice_demand g2 WHERE statement_id = 1802065000000074956 AND id = %t1.demand_id%。等于说，子查询要执行85万次……即使这两部查询都用到了索引，也是巨慢的。 优化策略 改写SQL为JOIN的方式 12DELETE ah FROM settlement_invoice_attachment ah INNER JOIN settlement_invoice_demand de ON ah.demand_id = de.id WHERE de.statement_id = 1802065000000074956; 拆成独立SQL多次执行 平时怎么识别？ 看子查询出现的位置 若子查询出现在WHERE从句中，而且是出现在IN（）中，则需要引起注意，用Explain瞧瞧（并不是子查询放IN（）里就一定是全表扫，本案例用，将DELETE改成SELECT就不是DEPENDENT SUBQUERY） 数据库原理 MySQL处理子查询时，会(优化)改写子查询，但优化的不是很友好，一直受业界批评比较多 有时候优化的挺糟糕的，特别是WHERE从句中的IN（）子查询 MySQL 子查询的弱点 mysql 在处理子查询时，会改写子查询。通常情况下，我们希望由内到外，先完成子查询的结果，然后再用子查询来驱动外查询的表，完成查询。 例如：select * from test where tid in(select fk_tid from sub_test where gid=10)通常我们会感性地认为该 sql 的执行顺序是： 1、sub_test 表中根据 gid 取得 fk_tid(2,3,4,5,6)记录。2、然后再到 test 中，带入 tid=2,3,4,5,6，取得查询数据。 但是实际mysql的处理方式为：select from test where exists (select from sub_test where gid=10 and sub_test.fk_tid=test.tid)mysql 将会扫描 test 中所有数据，每条数据都将会传到子查询中与 sub_test 关联，子查询不会先被执行，所以如果 test 表很大的话，那么性能上将会出现问题。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Provider暴露服务]]></title>
    <url>%2F2018%2F05%2F02%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Provider%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前言Dubbo Provider暴露服务的流程中，需要掌握几个核心抽象对象 过程中的重要类 ServiceConfig：记录了Dubbo Service所有相关的配置信息。ServiceConfig作用 DubboProtocol：以Dubbo协议的方式暴露服务，并以此为中心维护所有相关的动态服务数据。 RegisterProtocol: 内部会加载具体的注册中心Register,例如：ZookeeperRegister。完成服务向注册中心注册的动作。 ServiceConfig#loadRegistries：解析获得注册中心地址列表 过程中的重要对象 com.alibaba.dubbo.common.URL: 服务发布的地址 Invoker: 对原Service Interface进行了代理封装，屏蔽了具体Service Interface的差异，方便统一管理和调用。 Exporter： 一个ServiceBean每向一个注册中心Register注册一次，就会生成已各Exporter。Exporter用于连接暴露服务的Url与本地Invoker的对应关系。 ExporterMap: 记录着服务地址和Exporter的对应关系 来自Dubbo官方的几个架构设计图，先感觉下 ServiceBean核心流程 Spring容器启动，带动Dubbo Bean配置解析以及Bean实例化。 Dubbo启动 关键类： DubboNamespaceHandler ServiceBean ServiceConfig作用 ServiceBean 继承了ServiceConfig，所有的Provider服务的Dubbo配置都在ServiceConfig中。 Dubbo Service基本信息 Dubbo Service参数配置 注册中心地址信息。对应ServiceConfig中的loadRegistries(). ServiceBean 实现了InitializingBean, 实现了afterPropertiesSet()方法，在每个Dubbo Service Bean实例化后，在afterPropertiesSet()方法中进行所有Dubbo服务注册需要的操作。 afterPropertiesSet()中前置代码都是在做一些配置校验和默认值设置，最后会执行export()方法注册暴露服务。 afterPropertiesSet() export() doExport() doExportUrls() doExportUrlsFor1Protocol(DubboProtocol, regitsryURLs) DubboProtocol.export(wrapperInvoker) doExportUrlsFor1Protocol(ProtocolConfig protocolConfig, List registryURLs) 是真正执行export暴露服务的代码区 DubboProtocol#Export核心流程1234567891011121314151617181920212223242526272829public &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException &#123; URL url = invoker.getUrl(); // export service. String key = serviceKey(url); DubboExporter&lt;T&gt; exporter = new DubboExporter&lt;T&gt;(invoker, key, exporterMap); exporterMap.put(key, exporter); //export an stub service for dispatching event Boolean isStubSupportEvent = url.getParameter(Constants.STUB_EVENT_KEY, Constants.DEFAULT_STUB_EVENT); Boolean isCallbackservice = url.getParameter(Constants.IS_CALLBACK_SERVICE, false); if (isStubSupportEvent &amp;&amp; !isCallbackservice) &#123; String stubServiceMethods = url.getParameter(Constants.STUB_EVENT_METHODS_KEY); if (stubServiceMethods == null || stubServiceMethods.length() == 0) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(new IllegalStateException("consumer [" + url.getParameter(Constants.INTERFACE_KEY) + "], has set stubproxy support event ,but no stub methods founded.")); &#125; &#125; else &#123; stubServiceMethodsMap.put(url.getServiceKey(), stubServiceMethods); &#125; &#125; //获取一个服务端口，使用NettyServer绑定并监听，并设置Server监听事件处理回调为：DubboProtocol#requestHandler //Exchanger.bind的实际对象可配置，对应dubbo-remoting-api包 openServer(url); optimizeSerialization(url); return exporter;&#125; DubboProtocol核心数据123456789101112131415161718192021222324252627public class DubboProtocol extends AbstractProtocol &#123; ... //单例 private static DubboProtocol INSTANCE; //本地启动Server监听服务的Map private final Map&lt;String, ExchangeServer&gt; serverMap = new ConcurrentHashMap&lt;String, ExchangeServer&gt;(); // &lt;host:port,Exchanger&gt; //记录消费端的Exchanger private final Map&lt;String, ReferenceCountExchangeClient&gt; referenceClientMap = new ConcurrentHashMap&lt;String, ReferenceCountExchangeClient&gt;(); // &lt;host:port,Exchanger&gt; // private final ConcurrentMap&lt;String, LazyConnectExchangeClient&gt; ghostClientMap = new ConcurrentHashMap&lt;String, LazyConnectExchangeClient&gt;(); // private final Set&lt;String&gt; optimizers = new ConcurrentHashSet&lt;String&gt;(); //consumer side export a stub service for dispatching event //servicekey-stubmethods private final ConcurrentMap&lt;String, String&gt; stubServiceMethodsMap = new ConcurrentHashMap&lt;String, String&gt;(); private ExchangeHandler requestHandler = &#123;...&#125;; ...&#125; 12345public abstract class AbstractProtocol implements Protocol &#123; // ExporterMap protected final Map&lt;String, Exporter&lt;?&gt;&gt; exporterMap = new ConcurrentHashMap&lt;String, Exporter&lt;?&gt;&gt;();&#125; Dubbo Service是哪个时机注册到注册中心的？ 有关注到这个章节内容的小伙伴，说明你此时可能也还没想通吧，请听我道来。 这里会涉及到Dubbo的SPI机制，Dubbo 有好几个利用SPI+动态代理+Filter的处理责任链模式，ProtocolFilterWrapper.java算一个。 - Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); 在Dubbo源码中，Dubbo有自行注册几个protocol SPI （这里只列举Dubbo服务注册相关的） SPI机制用法详见 Dubbo Protocol SPI扩展详见 RegistryProtocol SPI: 注册位置：dubbo-registry-api包,resources下的com.alibaba.dubbo.rpc.Protocol 注册位置：dubbo-registry-zookeeper包,resources下的com.alibaba.dubbo.register.RegistryFactory 其实，在ServiceConfig中拿到的全局protocol并不直接是DubboProtocol，而是一串Protocol，DubboProtocol只是其中之一，这些Protocol会以责任链的方式逐一被调用 所以，在doExportUrlsFor1Protocol中protocol.export(…)时，会先执行DubboProtocol#export,再执行RegisterProtocol#export,各司其职。 RegisterProtocol中会根据Dubbo Service配置的register地址类型来决定加载哪个具体的RegisterFactory 123456789101112131415161718public void register(URL registryUrl, URL registedProviderUrl) &#123; //RegisterFactory根据注册中心类型，获取到注册实例，例如ZookeeperRegistry Registry registry = registryFactory.getRegistry(registryUrl); //执行注册，实际对应ZookeeperRegistry#register registry.register(registedProviderUrl); &#125; public &lt;T&gt; Exporter&lt;T&gt; export(final Invoker&lt;T&gt; originInvoker) throws RpcException &#123; ... if (register) &#123; register(registryUrl, registedProviderUrl); ProviderConsumerRegTable.getProviderWrapper(originInvoker).setReg(true); &#125; ... &#125; Netty Server当DubboProtocol.export.openServer()时，就是在本地启动Dubbo Service的Server服务并启动监听。 实现上是通过Exchanger拿到被配置的信息交换层的实现套件（一般是Netty）。 - 获取一个服务端口，使用NettyServer绑定并监听，并设置Server监听事件处理回调为：DubboProtocol#requestHandler - Exchanger.bind的实际对象可配置，对应dubbo-remoting-api包 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class NettyServer extends AbstractServer implements Server &#123; private static final Logger logger = LoggerFactory.getLogger(NettyServer.class); private Map&lt;String, Channel&gt; channels; // &lt;ip:port, channel&gt; private ServerBootstrap bootstrap; private org.jboss.netty.channel.Channel channel; public NettyServer(URL url, ChannelHandler handler); @Override protected void doOpen() throws Throwable &#123; NettyHelper.setNettyLoggerFactory(); ExecutorService boss = Executors.newCachedThreadPool(new NamedThreadFactory("NettyServerBoss", true)); ExecutorService worker = Executors.newCachedThreadPool(new NamedThreadFactory("NettyServerWorker", true)); ChannelFactory channelFactory = new NioServerSocketChannelFactory(boss, worker, getUrl().getPositiveParameter(Constants.IO_THREADS_KEY, Constants.DEFAULT_IO_THREADS)); bootstrap = new ServerBootstrap(channelFactory); final NettyHandler nettyHandler = new NettyHandler(getUrl(), this); channels = nettyHandler.getChannels(); // https://issues.jboss.org/browse/NETTY-365 // https://issues.jboss.org/browse/NETTY-379 // final Timer timer = new HashedWheelTimer(new NamedThreadFactory("NettyIdleTimer", true)); bootstrap.setPipelineFactory(new ChannelPipelineFactory() &#123; public ChannelPipeline getPipeline() &#123; NettyCodecAdapter adapter = new NettyCodecAdapter(getCodec(), getUrl(), NettyServer.this); ChannelPipeline pipeline = Channels.pipeline(); /*int idleTimeout = getIdleTimeout(); if (idleTimeout &gt; 10000) &#123; pipeline.addLast("timer", new IdleStateHandler(timer, idleTimeout / 1000, 0, 0)); &#125;*/ pipeline.addLast("decoder", adapter.getDecoder()); pipeline.addLast("encoder", adapter.getEncoder()); pipeline.addLast("handler", nettyHandler); return pipeline; &#125; &#125;); // bind channel = bootstrap.bind(getBindAddress()); &#125; @Override protected void doClose(); public Collection&lt;Channel&gt; getChannels(); public Channel getChannel(InetSocketAddress remoteAddress); public boolean isBound();&#125; ServiceConfig作用(见代码注释)12345678910111213141516171819202122232425262728293031323334public class ServiceConfig&lt;T&gt; extends AbstractServiceConfig &#123; ... // 采用的protocol远程调用层实现，用于封装RPC调用，默认是DubboProtocol，其余可选还有HttpProtocol,HessianProtocol,InjvmProtocol,RedisProtocol等 private static final Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); //对ServiceBean进行代理，包装成Dubbo内部通用的Invoker private static final ProxyFactory proxyFactory = ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension(); //ServiceBean作为Dubbo Provider启动时，会在本地起server服务，每个server服务都会绑定并监听端口。 private static final Map&lt;String, Integer&gt; RANDOM_PORT_MAP = new HashMap&lt;String, Integer&gt;(); //记录已暴露服务的服务地址 private final List&lt;URL&gt; urls = new ArrayList&lt;URL&gt;(); //一个ServiceBean每向一个注册中心Register注册一次，就会生成已各Exporter。Exporter用于连接暴露服务的Url与本地Invoker的对应关系。 private final List&lt;Exporter&lt;?&gt;&gt; exporters = new ArrayList&lt;Exporter&lt;?&gt;&gt;(); //关于本ServiceBean的Java Class信息 private String interfaceName; private Class&lt;?&gt; interfaceClass; // reference to interface impl private T ref; // service name private String path; // method configuration private List&lt;MethodConfig&gt; methods; private ProviderConfig provider; private transient volatile boolean exported; private transient volatile boolean unexported; private volatile String generic; Dubbo 启动Spring容器启动，带动Dubbo Bean配置实例化。Dubbo Bean配置来自于Dubbo Provider XML 文件。 1234567891011121314151617181920public class DubboNamespaceHandler extends NamespaceHandlerSupport &#123; static &#123; Version.checkDuplicate(DubboNamespaceHandler.class); &#125; public void init() &#123; registerBeanDefinitionParser("application", new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser("module", new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser("registry", new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser("monitor", new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser("provider", new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser("consumer", new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser("protocol", new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser("service", new DubboBeanDefinitionParser(ServiceBean.class, true)); //dubbo provider bean配置解析 registerBeanDefinitionParser("reference", new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser("annotation", new AnnotationBeanDefinitionParser()); &#125;&#125; 具体详见笔记：Dubbo源码解析-Spring Bean注册 ServiceBean实例化123public class ServiceBean&lt;T&gt; extends ServiceConfig&lt;T&gt; implements InitializingBean, DisposableBean, ApplicationContextAware, ApplicationListener&lt;ContextRefreshedEvent&gt;, BeanNameAware &#123; ......&#125; ServiceBean 继承了ServiceConfig，所有的Provider服务的Dubbo配置都在ServiceConfig中。 ServiceBean 实现了InitializingBean, 实现了afterPropertiesSet()方法，在每个Dubbo Service Bean实例化后，进行暴露服务的相关操作。 afterPropertiesSet()中前置代码都是在做一些配置校验和默认值设置，最后会执行export()方法注册暴露服务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145public void afterPropertiesSet() throws Exception &#123; //如果没有配置provider if (getProvider() == null) &#123; //获取IOC容器里的所有provider Map&lt;String, ProviderConfig&gt; providerConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProviderConfig.class, false, false); if (providerConfigMap != null &amp;&amp; providerConfigMap.size() &gt; 0) &#123; Map&lt;String, ProtocolConfig&gt; protocolConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProtocolConfig.class, false, false); if ((protocolConfigMap == null || protocolConfigMap.size() == 0) &amp;&amp; providerConfigMap.size() &gt; 1) &#123; // 兼容旧版本 List&lt;ProviderConfig&gt; providerConfigs = new ArrayList&lt;ProviderConfig&gt;(); for (ProviderConfig config : providerConfigMap.values()) &#123; if (config.isDefault() != null &amp;&amp; config.isDefault().booleanValue()) &#123; providerConfigs.add(config); &#125; &#125; //关联所有providers if (providerConfigs.size() &gt; 0) &#123; setProviders(providerConfigs); &#125; &#125; else &#123; ProviderConfig providerConfig = null; for (ProviderConfig config : providerConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (providerConfig != null) &#123; throw new IllegalStateException("Duplicate provider configs: " + providerConfig + " and " + config); &#125; providerConfig = config; &#125; &#125; if (providerConfig != null) &#123; setProvider(providerConfig); &#125; &#125; &#125; &#125; //如果没有配置application，且没有配置provider if (getApplication() == null &amp;&amp; (getProvider() == null || getProvider().getApplication() == null)) &#123; //获取所有applications Map&lt;String, ApplicationConfig&gt; applicationConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ApplicationConfig.class, false, false); if (applicationConfigMap != null &amp;&amp; applicationConfigMap.size() &gt; 0) &#123; ApplicationConfig applicationConfig = null; for (ApplicationConfig config : applicationConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (applicationConfig != null) &#123; throw new IllegalStateException("Duplicate application configs: " + applicationConfig + " and " + config); &#125; applicationConfig = config; &#125; &#125; //关联application if (applicationConfig != null) &#123; setApplication(applicationConfig); &#125; &#125; &#125; //如果没有配置module，且没有配置provider if (getModule() == null &amp;&amp; (getProvider() == null || getProvider().getModule() == null)) &#123; Map&lt;String, ModuleConfig&gt; moduleConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ModuleConfig.class, false, false); if (moduleConfigMap != null &amp;&amp; moduleConfigMap.size() &gt; 0) &#123; ModuleConfig moduleConfig = null; for (ModuleConfig config : moduleConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (moduleConfig != null) &#123; throw new IllegalStateException("Duplicate module configs: " + moduleConfig + " and " + config); &#125; moduleConfig = config; &#125; &#125; //关联module if (moduleConfig != null) &#123; setModule(moduleConfig); &#125; &#125; &#125; //如果没有配置registries，且没有配置provider if ((getRegistries() == null || getRegistries().size() == 0) &amp;&amp; (getProvider() == null || getProvider().getRegistries() == null || getProvider().getRegistries().size() == 0) &amp;&amp; (getApplication() == null || getApplication().getRegistries() == null || getApplication().getRegistries().size() == 0)) &#123; Map&lt;String, RegistryConfig&gt; registryConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, RegistryConfig.class, false, false); if (registryConfigMap != null &amp;&amp; registryConfigMap.size() &gt; 0) &#123; List&lt;RegistryConfig&gt; registryConfigs = new ArrayList&lt;RegistryConfig&gt;(); for (RegistryConfig config : registryConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; registryConfigs.add(config); &#125; &#125; //关联registries if (registryConfigs != null &amp;&amp; registryConfigs.size() &gt; 0) &#123; super.setRegistries(registryConfigs); &#125; &#125; &#125; //如果没有配置monitor，且没有配置provider if (getMonitor() == null &amp;&amp; (getProvider() == null || getProvider().getMonitor() == null) &amp;&amp; (getApplication() == null || getApplication().getMonitor() == null)) &#123; Map&lt;String, MonitorConfig&gt; monitorConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, MonitorConfig.class, false, false); if (monitorConfigMap != null &amp;&amp; monitorConfigMap.size() &gt; 0) &#123; MonitorConfig monitorConfig = null; for (MonitorConfig config : monitorConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (monitorConfig != null) &#123; throw new IllegalStateException("Duplicate monitor configs: " + monitorConfig + " and " + config); &#125; monitorConfig = config; &#125; &#125; //关联monitor if (monitorConfig != null) &#123; setMonitor(monitorConfig); &#125; &#125; &#125; //如果没有配置protocol，且没有配置provider if ((getProtocols() == null || getProtocols().size() == 0) &amp;&amp; (getProvider() == null || getProvider().getProtocols() == null || getProvider().getProtocols().size() == 0)) &#123; Map&lt;String, ProtocolConfig&gt; protocolConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProtocolConfig.class, false, false); if (protocolConfigMap != null &amp;&amp; protocolConfigMap.size() &gt; 0) &#123; List&lt;ProtocolConfig&gt; protocolConfigs = new ArrayList&lt;ProtocolConfig&gt;(); for (ProtocolConfig config : protocolConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; protocolConfigs.add(config); &#125; &#125; //关联protocol if (protocolConfigs != null &amp;&amp; protocolConfigs.size() &gt; 0) &#123; super.setProtocols(protocolConfigs); &#125; &#125; &#125; //如果没有配置path if (getPath() == null || getPath().length() == 0) &#123; if (beanName != null &amp;&amp; beanName.length() &gt; 0 &amp;&amp; getInterface() != null &amp;&amp; getInterface().length() &gt; 0 &amp;&amp; beanName.startsWith(getInterface())) &#123; setPath(beanName); &#125; &#125; //暴露provider,重点！！！ if (! isDelay()) &#123; export(); &#125; &#125; Export暴露服务 export()方法会完成后续服务注册的所有流程 12345678910111213141516171819202122232425262728293031323334public synchronized void export() &#123; //如果provider没有配置 if (provider != null) &#123; //如果exporter没有配置使用provider所关联的exporter if (export == null) &#123; export = provider.getExport(); &#125; //如果delay（延迟暴露）没有配置，获取provider的delay if (delay == null) &#123; delay = provider.getDelay(); &#125; &#125; //如果不需要暴露接口则直接返回 if (export != null &amp;&amp; ! export.booleanValue()) &#123; return; &#125; //如果延迟暴露的时间（毫秒级）是存在的，开启线程并等待delay毫秒后开始暴露接口，否则直接执行暴露接口过程 if (delay != null &amp;&amp; delay &gt; 0) &#123; Thread thread = new Thread(new Runnable() &#123; public void run() &#123; try &#123; Thread.sleep(delay); &#125; catch (Throwable e) &#123; &#125; doExport(); &#125; &#125;); thread.setDaemon(true); thread.setName("DelayExportServiceThread"); thread.start(); &#125; else &#123; doExport(); &#125; &#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务端业务处理不成功，应该返回HTTP 200 还是 HTTP 4XXX系列？]]></title>
    <url>%2F2018%2F04%2F23%2F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E4%B8%9A%E5%8A%A1%E5%A4%84%E7%90%86%E4%B8%8D%E6%88%90%E5%8A%9F%EF%BC%8C%E5%BA%94%E8%AF%A5%E8%BF%94%E5%9B%9EHTTP-200-%E8%BF%98%E6%98%AF-HTTP-4XXX%E7%B3%BB%E5%88%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[场景其实，纠结只出现在例如保存表单的场景，如果服务端因各种业务上的原因（校验不通过，状态不满足等）导致保存未成功，并要返回对应的提示信息，此时服务端回应此HTTP 请求时，是用 “200 + json” 还是用“400 + 错误信息”？ 在公司内不同项目间，两种风格都有，且小伙伴们各执己见。 我这么看首先，我先表达我赞同“200 + json”的方式。 更具体些，服务端所有的Controller Method对返回值做统一的Response包装 1234567891011121314样例1：&#123; "code": 200, "data": &#123;...&#125;, "message":"操作成功"&#125;样例2：&#123; "success": false "data": &#123;...&#125;, "code": 100409, "message":"数据已存在"&#125; 我的观点1. 协议分层对于RPC请求，存在两个层面的操作结果 [1] HTTP请求本身的结果 ———— 业务无关性，与网络、框架层面相关[2] 业务处理的结果 ———— 强业务逻辑相关性，与网络、框架层面无关 为什么我们会有本贴讨论的话题与分歧，或者说为什么大部分人觉得http code不够试用，是因为实际开发应用场景中，尝试着只用http code 去表达上述两个层面的结果。 分层表示的优点： RPC请求， 是可以基于不同的底层协议的， 比如我们用的HTTP协议，很容易替换成ZeroMQ, RabbitMQ, UDP， 基于TCP的自定义协议…… 只要能实现一问一答模型的协议，都是可以用的。这个时候， HTTP协议只是一种底层协议， 底层协议的错误号，并不应该被上层协议使用。 2. HTTP Code 表达能力局限性虽然HTTP协议非常友好的定义了诸多的HTTP Code码，但在实际开发应用中，对于繁多的应用场景，HTTP Code的表达能力显得力不从心，部分场景仍旧不能避免的辅以Response Body信息。加之这些HTTP Code并不是应用开发中的绝对标准。 3. HTTP Code 语义表达的不统一性[1] 同样是HTTP 4XX系列，不同系统的解释也是不一样的[2] 同样是”参数校验不通过”的业务问题，不同系统使用的HTTP码也是不一样的 4. Http Code数量有限，表达能力有限这个应该很好理解，大家应该也有体会。 5. 系统集成友好性如果我们把HTTP协议当作一种传输层协议看待，200 可以很好表达， 整个底层传输都是没有问题， 包括负载均衡系统， nginx， 反向代理， fast cgi守护程序都是工作正常的。 而返回各种HTTP Status Code经常会让外部使用者非常的困惑，特别是他们对HTTP Status Code有一定了解，却对你的系统不甚了解的情况下。 所以，除了考虑ajax请求的处理，还要考虑整个调用的中间链路以及框架集成方面的因素 返回200能避免CDN等中间商替换或缓存 国内的通信运营商画蛇添足根据HTTP状态码给替换成导航页或广告推广页面 对于系统审计程序不友好，例如 HTTP Response Code = 4XX的请求算请求成功？请求失败？请求异常？————无法区分！ 6. 扩展性 返回200OK，扩展性更强，修改的时候只需要修改字段而不需要特别处理Status Code 易于与真正的400错误区分，方便审计和分析。而实际上，当服务器能够正常返回，证明服务器已经正确的理解并得出相应的结果（并且这个结果也是预定义的，并非未知），这显然与400的定义不符。 返回200更优。保不准哪天某种状态是HTTP协议不支持的，保不准哪个需要新增的字段是HTTP协议没有的]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>WEB</tag>
        <tag>REST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Spring Bean注册]]></title>
    <url>%2F2018%2F04%2F22%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Spring-Bean%E6%B3%A8%E5%86%8C%2F</url>
    <content type="text"><![CDATA[相信大家对于Dubbo Provider/Consumer的配置非常熟练，但这背后的实现原理清楚吗？如果有不太清楚的朋友，可以再往下阅读下。 知识点 自定义 Spring XML Bean机制 背景据我们所知 Spring 注解方式声明Bean的方式是在Class上打上@Component注解（@Component的扩展注解也可），当Spring容器启动时，Spring会自动扫面所有带有@Component注解的Class，自动注册到Bean容器中。 例如： 1234@Componentpublic class Student &#123; //do something&#125; 也可以通过XML文件配置的方式声明Bean 例如： 1&lt;bean id="Student" class="com.test.spring.beans.Student"&gt;&lt;/bean&gt; 但，回过头来看Dubbo，我们并没有通过上述的方式去声明Dubbo配置中的Bean，却也能像使用Spring Bean一样用@Autowire去注入Dubbo服务的Bean，这其中的原理是什么呢？让我们从源码中找答案。 原理启动Spring容器在SpringBoot+Dubbo的搭配中，Java应用的启动入口main方法一般会这么写。通过此步骤去启动Java程序并将Dubbo Bean注册Spring容器。 12345678910111213141516package com.sample;@ComponentScan(basePackages = &#123; "com.sample.myapp"&#125;)@SpringBootApplication@EnableSchedulingpublic class MyApplication &#123; public static void main(String[] args) &#123; //启动Spring容器 SpringApplication application = new SpringApplication(MyApplication.class, "classpath:/spring/dubbo-config.xml"); //指定Dubbo配置文件 application.run(args); &#125;&#125; Spring如何识别Dubbo 自定义Bean标签Spring为了支持用户自定义类加载到Spring容器，提供了org.springframework.beans.factory.xml.NamespaceHandler接口和org.springframework.beans.factory.xml.NamespaceHandlerSupport抽象类，NamespaceHandler#init方法会在对象的构造函数调用之后、属性初始化之前被DefaultNamespaceHandlerResolver调用。dubbo的DubboNamespaceHandler类正是继承了NamespaceHandlerSupport，其代码实现如下： 1234567891011121314151617181920public class DubboNamespaceHandler extends NamespaceHandlerSupport &#123; static &#123; Version.checkDuplicate(DubboNamespaceHandler.class); &#125; public void init() &#123; registerBeanDefinitionParser("application", new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser("module", new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser("registry", new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser("monitor", new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser("provider", new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser("consumer", new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser("protocol", new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser("service", new DubboBeanDefinitionParser(ServiceBean.class, true)); registerBeanDefinitionParser("reference", new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser("annotation", new AnnotationBeanDefinitionParser()); &#125;&#125; registerBeanDefinitionParser方法使用的是父抽象类NamespaceHandlerSupport的默认实现，第一个参数是elementName，即元素名称，即告诉Spring你要解析哪个标签，第二个参数是BeanDefinitionParser的实现类，BeanDefinitionParser是Spring用来将xml元素转换成BeanDefinition对象的接口。dubbo的DubboBeanDefinitionParser类就实现了这个接口，负责将标签转换成bean定义对象BeanDefinition。 所以，以后想要了解Dubbo Bean初始化相关细节，可以查看DubboBeanDefinitionParser#parse的代码实现。 例如： Dubbo Bean 会有哪些默认设置 dubbo服务提供者使用dubbo:service标签时，如果既不设置id，也不设置name，则dubbo给ServiceBean在Spring容器中定义的ID是什么？ Dubbo xml文件中的配置是怎么作用到Dubbo Bean中去的 关于NamespaceHandlerSupport spring.handlers # 指定xml namespace的解析handler类 spring.schemas # 指定xml xsd文件位置 dubbo.xsd # 设计你要的xml配置格式 DubboNamespaceHandler # 自定义NamespaceHandler,完成从xml中读取配置内容，并转换成Spring Bean进行注册 Spring容器会默认加载classpath/META-INF下的spring.handlers和spring.schemas两个文件，来加载xsd和对应的NamespaceHandler,所以dubbo-config-spring包下的META-INF目录下也有这两个文件 练习DEMO1. 设计配置属性和JavaBean 设计好配置项，并通过JavaBean来建模，本例中需要配置People实体，配置属性name和age（id是默认需要的） 12345public class People &#123; private String id; private String name; private Integer age; &#125; 2. 编写XSD文件 为上一步设计好的配置项编写XSD文件，XSD是schema的定义文件，配置的输入和解析输出都是以XSD为契约，本例中XSD如下 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;xsd:schema xmlns="http://veryjj/cutesource/schema/people" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:beans="http://www.springframework.org/schema/beans" targetNamespace="http://veryjj/cutesource/schema/people" elementFormDefault="qualified" attributeFormDefault="unqualified"&gt; &lt;xsd:import namespace="http://www.springframework.org/schema/beans" /&gt; &lt;xsd:element name="people"&gt; &lt;xsd:complexType&gt; &lt;xsd:complexContent&gt; &lt;xsd:extension base="beans:identifiedType"&gt; &lt;xsd:attribute name="name" type="xsd:string" /&gt; &lt;xsd:attribute name="age" type="xsd:int" /&gt; &lt;/xsd:extension&gt; &lt;/xsd:complexContent&gt; &lt;/xsd:complexType&gt; &lt;/xsd:element&gt; &lt;/xsd:schema&gt; 关于xsd:schema的各个属性具体含义就不作过多解释，可以参见http://www.w3school.com.cn/schema/schema_schema.asp &lt;xsd:element name=”people”&gt;对应着配置项节点的名称，因此在应用中会用people作为节点名来引用这个配置 &lt;xsd:attribute name=”name” type=”xsd:string” /&gt;和&lt;xsd:attribute name=”age” type=”xsd:int” /&gt;对应着配置项people的两个属性名，因此在应用中可以配置name和age两个属性，分别是string和int类型 完成后需把xsd存放在classpath下，一般都放在META-INF目录下（本例就放在这个目录下） 3. 编写NamespaceHandler和BeanDefinitionParser完成解析工作 1234567891011121314151617181920212223242526 public class MyNamespaceHandler extends NamespaceHandlerSupport &#123; public void init() &#123; registerBeanDefinitionParser("people", new PeopleBeanDefinitionParser()); &#125; &#125; public class PeopleBeanDefinitionParser extends AbstractSingleBeanDefinitionParser &#123; protected Class getBeanClass(Element element) &#123; return People.class; &#125; protected void doParse(Element element, BeanDefinitionBuilder bean) &#123; String name = element.getAttribute("name"); String age = element.getAttribute("age"); String id = element.getAttribute("id"); if (StringUtils.hasText(id)) &#123; bean.addPropertyValue("id", id); &#125; if (StringUtils.hasText(name)) &#123; bean.addPropertyValue("name", name); &#125; if (StringUtils.hasText(age)) &#123; bean.addPropertyValue("age", Integer.valueOf(age)); &#125; &#125; &#125; 4. 编写spring.handlers和spring.schemas串联起所有部件 spring提供了 spring.handlers和spring.schemas这两个配置文件来完成这项工作，这两个文件需要我们自己编写并放入META-INF文件夹 中，这两个文件的地址必须是META-INF/spring.handlers和META-INF/spring.schemas，spring会默认去 载入它们，本例中spring.handlers如下所示： spring.handlers 1http\://veryjj/cutesource/schema/people=study.schemaExt.MyNamespaceHandler spring.schemas 1http\://veryjj/cutesource/schema/people.xsd=META-INF/people.xsd 以上就是载入xsd文件 5. 使用自定义schema定义Spring Bean 到此为止一个简单的自定义配置以完成，可以在具体应用中使用了。使用方法很简单，和配置一个普通的spring bean类似，只不过需要基于我们自定义schema，本例中引用方式如下所示： 12345678&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:cutesource="http://veryjj/cutesource/schema/people" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://veryjj/cutesource/schema/people http://veryjj/cutesource/schema/people.xsd"&gt; &lt;cutesource:people id="cutesource" name="黄老师" age="27"/&gt; &lt;/beans&gt; 其中xmlns:cutesource=”http://veryjj/cutesource/schema/people&quot; 是用来指定自定义schema，xsi:schemaLocation用来指定xsd文件。&lt;cutesource:people id=”cutesource” name=”黄老师” age=”27”/&gt;是一个具体的自定义配置使用实例。 6. 注入自定义schema定义的Spring Bean 跟Spring Bean的注入方式完全一样，按你喜欢的方式来。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Dubbo可以这么学]]></title>
    <url>%2F2018%2F04%2F22%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Dubbo%E5%8F%AF%E4%BB%A5%E8%BF%99%E4%B9%88%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[最近面试工作颇多，Dubbo作为的微服务主流技术架构，也是分布式系统中面试的高频考题之一。但从面试的过程中得到的反馈，大家对于Dubbo的关注以及掌握程度基本都处于会基本使用的程度，基本没遇到有对Dubbo框架做学习研究的求职者。 求职者一般只会聊下面两个话题： Dubbo 是什么东西？ 答：RPC框架/微服务框架，在实际工作中用Dubbo做业务功能服务化。 Dubbo的工作原理是什么样的？ 答：Provider端将服务注册到Zookeeper中，Consumer端从Zookeeper获取Provider，然后就可以调用API了。 一般情况下关于Dubbo的基本都聊到此结束了，虽然说没回答错，但也忒简洁了吧，连Dubbo架构图中（下图）的内容都没说完整，而这并不是面试官想得到的讯息。 Dubbo作为主流的微服务技术框架，必然有其优秀的一面，也是学习RPC框架思想很好的素材 Dubbo 应该掌握哪些内容？（个人思路） 阅读Dubbo的用户手册以及开发手册。Dubbo.io 知晓Dubbo支持的功能 知晓Dubbo的各种扩展点 知晓Dubbo的设计思想（这里不得不说Dubbo.io的文档说明写的非常详细、到位，甚至一度让我觉得没有写Blog的必要） Dubbo 核心流程源码实现 Dubbo Bean的集成 Provider 注册、暴露服务 Consumer 注册、订阅服务 Consumer 调用实现 Provider 处理请求 Dubbo SPI机制 Dubbo Filter机制 思考些高级的 Dubbo各可配机制主流选择的优缺点 register remoting rpc Dubbo Cluster Dubbo 怎么做服务治理 策略路由 降级 熔断 Dubbo 性能基线&amp;性能调优 框架扩展 服务监控 流量分析 那么，逐步的去落实吧！如果开始、请务必坚持！]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty 核心对象梳理-1]]></title>
    <url>%2F2018%2F04%2F08%2FNetty-%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1%E6%A2%B3%E7%90%86-1%2F</url>
    <content type="text"><![CDATA[《Netty In Action》阅读笔记摘要 What is NettyNetty是一款用于快速开发高性能的网络应用程序的Java框架 它封装了网络编程的复杂性 Key words: 是一款Java语言的开发框架 封装、提供程序快速网络编程的能力 高性能 Netty是完全异步和事件驱动的 Netty 核心组件ChannelChannel 是Java NIO的一个基本构造 可以将Channel 看作是连接的载体。因此，它可以被打开、被关闭、连接、断开连接。 事件Channel连接上发生的事件。可以等价的理解为 epoll 中关于每个Socket事件，例如：EPOLLIN, EPOLLOUT, EPOLLHUP的回调 回调类似于常见的回调，Netty内部用回调来处理事件； 可理解为 epoll 中关于每个Socket事件的回调，例如：EPOLLIN, EPOLLOUT, EPOLLHUP的回调。 应用程序可以自定义回调，感知Netty网络通信的事件。 ChannelFutureFuture提供了另一种在操作完成时通知应用程序的方式。 JDK中的java.util.concurrent.Future 在使用上是阻塞调用的，不优雅。Netty 提供了另一种实现：ChannelFuture，用于在执行异步操作的时候使用。 Netty的每个出站I/O都将返回一个ChannelFuture。 使用ChannelFuture时，可以配合使用ChannelFutureListener。只要实现operationComplete() 回调即可，非常方便。且支持多ChannelFuture。 1234567891011121314151617Channel channel = ...;//Does not block ChannelFuture future = channel.connect(new InetSocketAddress("192.168.0.1", 25));​future.addListener(new ChannelFuturelistener()&#123; @Override public void operationComplete(ChannelFuture future)&#123; if (future.isSuccess())&#123; ByteBuf buffer = Unpooled.copiedBuffer("Hello", Charset.defaultCharset()); ChannelFuture wf = future.channel().writeAndFlush(buffer); ... &#125; else &#123; Throwable cause = future.cause(); cause.printStackTrace(); &#125; &#125;&#125;) ChannelHandler可以初步理解为每个ChannelHandler实例都类似于一种为了响应特定事件而被执行的回调。 PSNetty 的内部实现细节跟Linux epoll的用法很相似，熟悉Linux Epoll以及编程模型的朋友来说可以对比着来学习，寻找类同点、差异点以及差异的原因。 Netty 同时支持OIO, NIO, EPOLL等多路复用模式，我是以熟悉的epoll作为切入熟悉的内部流程原理。 Netty的组件和设计 Channel ———— Socket； EventLoop ———— 控制流、多线程处理、并发； ChannelFuture ———— 异步通知； Channel 接口Netty的Channel接口所提供的API，大大地降低了使用Socket类的复杂性。 EventLoopGroup 接口主要作用 用于注册Channel 执行部分Runnable任务 这里重点讲下“注册Channel”，在实际编程或应用时，每个Channel都是向EventLoopGroup注册的，由EventLoopGroup按照指定的策略方法，将Channel注册到EventLoopGroup下某个具体的EventLoop当中去。 12345678910111213141516171819202122232425262728293031323334public interface EventLoopGroup extends EventExecutorGroup &#123; ... /** * Register a &#123;@link Channel&#125; with this &#123;@link EventLoop&#125;. The returned &#123;@link ChannelFuture&#125; * will get notified once the registration was complete. */ ChannelFuture register(Channel channel); ...&#125;public abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup &#123; ... @Override public ChannelFuture register(Channel channel) &#123; return next().register(channel); &#125; ...&#125;public abstract class MultithreadEventExecutorGroup extends AbstractEventExecutorGroup &#123; ... @Override public EventExecutor next() &#123; return chooser.next(); &#125; ... EventLoop 接口EventLoop定义了Netty的核心抽象，用于处理连接的生命周期中所发生的事件。 一个EventLoopGroup包含一个或者多个EventLoop； 一个EventLoop在它的生命周期内只和一个Thread绑定； 所有由EventLoop处理的I/O事件都将在它专有的Thread上被处理； 一个Channel在它的生命周期内只注册于一个EventLoop； 一个EventLoop可能会被分配给一个或多个Channel； 在这种设计中，一个给定的Channel的I/O操作都是由相同的Thread执行的，实际上消除了对于同步的需要。 ChannelFuture 接口Netty中所有的I/O操作都是异步的。所有我们需要一种用于在之后的某个时间点确定其结果的方法。 为此，Netty提供了ChannelFuture接口，其addListener()方法注册了一个ChannelFutureListener，以便在某个操作完成时得到通知。 ChannelHandler 接口顾名思义，Channel的Handler，它充当了所有处理入站和出站数据的应用程序逻辑的容器。ChannelHandler的方法是由网络事件触发的。 ChannelPipeline 接口ChannelPipeline为ChannelHandler链提供了容器，并定义了用于在该链上传播入站和出站事件流的API。当Channel被创建时，它会被自动的分配到它专属的ChannelPipeline。 ChannelPipleline中的ChannelHandler的执行顺序是由它们被添加的顺序所决定的。 编码器和解码器Netty用于网络通信，天然需要编码和解码。也是用ChannelPipeline + ChannelHandler的机制实现的。 所有由Netty提供的编码器/解码器适配器类都实现了ChannelOutboundHandler或者ChannelInboundHandler接口。 引导（Bootstrap）Netty的引导类为应用程序的网络层配置提供了容器。 类别 Bootstrap ServerBootstrap 网络编程中的作用 连接到远程主机和端口 绑定到一个本地端口 EventLoopGroup的数目 1 2 使用Netty的ChannelOption和属性 在每个Channel创建时都手动配置它可能会变得相当乏味。幸运的是，你不必这样做。相反，你可以使用option()方法来将ChannelOption应用到Bootstrap上。你所提供的值将会被自动应用到Bootstrap所创建的所有Channel。 引导DatagramChannel Bootstrap除了引导基于TCP协议的SocketChannel，也可以用于引导无连接的协议。Netty提供了各种DatagramChannel的实现。与面向连接的TCP相比，唯一区别是不再调用connect()方法，而是只调用bind()方法 123456789101112131415161718192021222324//使用Bootstrap和DatagramChannelBootstrap bootstrap = new Bootstrap();bootstap.group(new OioEventLoopGroup()) .channel(OioDatagramChannel.class) .handler(new SimpleChannelInboundHandler&lt;DatagramPacket&gt;()&#123; @Override public void channelRead0(ChannelHandlerContext ctx, DatagramPacket msg) throws Exception &#123; //Do something with the packet &#125; &#125;);ChannelFuture future = bootstrap.bind(new InetSocketAddress(0));future.addListener(new ChannelFutureListener()&#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception&#123; if (channelFuture.isSuccess())&#123; System.out.println("Channel bound"); &#125; else &#123; System.out.println("Bind attempt failed"); channelFuture.cause().printStackTrace(); &#125; &#125;&#125;) 我的理解 引导的根对象是 EventLoopGroup，间接的负责监听、处理所有Channel的网络事件。 EventLoop是EventLoopGroup内的成员，每个EventLoop与具体的线程绑定。也可以理解一个线程，一个EventLoop。 EventLoop直接负责处理其下所有Channel的网络事件。 ChannelHadler是Channel网络事件逻辑处理的容器，应用逻辑开发的重点就在此。 当一个Channel上来一个网络事件时，对应的EventLoop首先进行响应，并找到Channel所属的ChannelPipeline，Channel作为输入驱动一次ChannelPipeline。 ChannelPipeline 遍历其下ChannelHandler，逐个处理Channel的网络事件。 ChannelFuture可以同步等结果，也可以异步通知结果，都支持，自己选！ ByteBuf网络数据的基本单位是字节。Java NIO提供了ByteBuffer作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。 Netty的ByteBuffer替代品是ByteBuf，一个强大的实现，既解决了JDK API的局限性，又为网络应用程序的开发者提供了更好的API。 ByteBuf优点： 对于同一个数据buffer，维护readIndex, writeIndex两份索引 ByteBuf模式 堆缓冲区模式： 将数据存储在JVM的堆空间中，应用代码可直接访问缓冲区中的数据。 直接缓冲区模式： JDK 1.4引入的ByteBuffer类允许JVM实现直接使用操作系统的本地内容，这就避免了JAVA 应用在每次调用本地I/O操作前/后 需要将缓冲区的内容复制到一个与操作系统结合的中间缓冲区中。 缺点：因为数据不是在堆上，所以业务代码处理时不得不经过一次复制。 复合缓冲区： 为多个ByteBuf提供一个统一的聚合视图，可以根据需要向复合缓冲区中添加或者删除ByteBuf实例。 字节级操作 可以以字节的操作方式使用ByteBuf 随机访问索引 顺序访问索引 可丢弃字节 可读字节 可写字节 索引管理 indexOf / ByteBufProcessor 派生缓冲区 读/写操作 ByteBuf池化分配 为了降低分配和释放内存的开销，Netty通过interface ByteBufAllocator实现了ByteBuf的池化。 Unpooled 缓冲区 如果未能获取到一个ByteBufAllocator的引用，Netty提供一个简单的Unpooled工具类，它提供创建未池化的ByteBuf实例。 关于 ChannelFuture 和 ChannelPromise ChannelFuture read-only 没有返回值的异步通知、调用 DefaultFutureListeners -&gt; listeners[N] ChannelPromise writeable 可写异步执行结果的通知、调用 notifyListenerNow -&gt; 回到Listeners -&gt; 取出对应的Channel进行回调操作]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅聊分布式事务]]></title>
    <url>%2F2018%2F03%2F30%2F%E6%B5%85%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前言 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近很久没有写博客了，一方面是因为公司事情最近比较忙，另外一方面是因为在进行 CAP 的下一阶段的开发工作，不过目前已经告一段落了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来还是开始我们今天的话题，说说分布式事务，或者说是我眼中的分布式事务，因为每个人可能对其的理解都不一样。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式事务是企业集成中的一个技术难点，也是每一个分布式系统架构中都会涉及到的一个东西，特别是在微服务架构中，几乎可以说是无法避免，本文就分布式事务来简单聊一下。 数据库事务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等… &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接着，我们就说一下分布式事务。 分布式理论 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？ CAP定理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。 这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个时候有同学可能会把数据库的2PC（两阶段提交）搬出来说话了。OK，我们就来看一下数据库的两阶段提交。 对数据库分布式事务有了解的同学一定知道数据库支持的2PC，又叫做 XA Transactions。 MySQL从5.5版本开始支持，SQL Server 2005 开始支持，Oracle 7 开始支持。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，XA 是一个两阶段提交协议，该协议分为以下两个阶段： 第一阶段：事务协调器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交. 第二阶段：事务协调器要求每个数据库提交数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，如果有任何一个数据库否决此次提交，那么所有数据库都会被要求回滚它们在此事务中的那部分信息。这样做的缺陷是什么呢? 咋看之下我们可以在数据库分区之间获得一致性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果CAP 定理是对的，那么它一定会影响到可用性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果说系统的可用性代表的是执行某项操作相关所有组件的可用性的和。那么在两阶段提交的过程中，可用性就代表了涉及到的每一个数据库中可用性的和。我们假设两阶段提交的过程中每一个数据库都具有99.9%的可用性，那么如果两阶段提交涉及到两个数据库，这个结果就是99.8%。根据系统可用性计算公式，假设每个月43200分钟，99.9%的可用性就是43157分钟, 99.8%的可用性就是43114分钟，相当于每个月的宕机时间增加了43分钟。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上，可以验证出来，CAP定理从理论上来讲是正确的，CAP我们先看到这里，等会再接着说。 BASE理论&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是BASE理论，它是用来对CAP定理进行进一步扩充的。BASE理论指的是： Basically Available（基本可用） Soft state（软状态） Eventually consistent（最终一致性） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有了以上理论之后，我们来看一下分布式事务的问题。 分布式事务&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，要实现分布式事务，无外乎那几种解决方案。 一、两阶段提交（2PC）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和上一节中提到的数据库XA事务一样，两阶段提交就是使用XA协议的原理，我们可以从下面这个图的流程来很容易的看出中间的一些比如commit和abort的细节。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;两阶段提交这种解决方案属于牺牲了一部分可用性来换取的一致性。在实现方面，在 .NET 中，可以借助 TransactionScop 提供的 API 来编程实现分布式系统中的两阶段提交，比如WCF中就有实现这部分功能。不过在多服务器之间，需要依赖于DTC来完成事务一致性，Windows下微软搞的有MSDTC服务，Linux下就比较悲剧了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外说一句，TransactionScop 默认不能用于异步方法之间事务一致，因为事务上下文是存储于当前线程中的，所以如果是在异步方法，需要显式的传递事务上下文。 优点： 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能100%保证强一致） 缺点： 实现复杂，牺牲了可用性，对性能影响较大，不适合高并发高性能场景，如果分布式系统跨接口调用，目前 .NET 界还没有实现方案。 二、补偿事务（TCC）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段： Try 阶段主要是对业务系统做检测及资源预留 Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 举个例子，假入 Bob 要向 Smith 转账，思路大概是：我们有一个本地方法，里面依次调用1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些 缺点： 缺点还是比较明显的，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。 三、本地消息表（异步确保）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于ebay。我们可以从下面的流程图中看出其中的一些细节： 基本思路就是： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。 优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。在 .NET中 有现成的解决方案。 缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。 四、MQ 事务消息&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有一些第三方的MQ是支持事务消息的，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。 以阿里的 RocketMQ 中间件为例，其思路大致为： 第一阶段Prepared消息，会拿到消息的地址。 第二阶段执行本地事务，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。 也就是说在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了RocketMQ会定期扫描消息集群中的事务消息，这时候发现了Prepared消息，它会向消息发送者确认，所以生产方需要实现一个check接口，RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。 优点： 实现了最终一致性，不需要依赖本地数据库事务。 缺点： 实现难度大，主流MQ不支持，没有.NET客户端，RocketMQ事务消息部分代码也未开源。 五、Sagas 事务模型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Saga事务模型又叫做长时间运行的事务（Long-running-transaction）, 它是由普林斯顿大学的H.Garcia-Molina等人提出，它描述的是另外一种在没有两阶段提交的的情况下解决分布式系统中复杂的业务事务问题。你可以在这里看到 Sagas 相关论文。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们这里说的是一种基于 Sagas 机制的工作流事务模型，这个模型的相关理论目前来说还是比较新的，以至于百度上几乎没有什么相关资料。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该模型其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，那么Sagas工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如我们一次关于购买旅游套餐业务操作涉及到三个操作，他们分别是预定车辆，预定宾馆，预定机票，他们分别属于三个不同的远程接口。可能从我们程序的角度来说他们不属于一个事务，但是从业务角度来说是属于同一个事务的。 他们的执行顺序如上图所示，所以当发生失败时，会依次进行取消的补偿操作。 因为长事务被拆分了很多个业务流，所以 Sagas 事务模型最重要的一个部件就是工作流或者你也可以叫流程管理器（Process Manager），工作流引擎和Process Manager虽然不是同一个东西，但是在这里，他们的职责是相同的。在选择工作流引擎之后，最终的代码也许看起来是这样的 12345678910111213SagaBuilder saga = SagaBuilder.newSaga(&quot;trip&quot;) .activity(&quot;Reserve car&quot;, ReserveCarAdapter.class) .compensationActivity(&quot;Cancel car&quot;, CancelCarAdapter.class) .activity(&quot;Book hotel&quot;, BookHotelAdapter.class) .compensationActivity(&quot;Cancel hotel&quot;, CancelHotelAdapter.class) .activity(&quot;Book flight&quot;, BookFlightAdapter.class) .compensationActivity(&quot;Cancel flight&quot;, CancelFlightAdapter.class) .end() .triggerCompensationOnAnyError();camunda.getRepositoryService().createDeployment() .addModelInstance(saga.getModel()) .deploy(); 这里有一个 C# 相关示例，有兴趣的同学可以看一下。 优缺点 这里我们就不说了，因为这个理论比较新，目前市面上还没有什么解决方案，即使是 Java 领域，我也没有搜索的太多有用的信息。 转自：https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识鱼骨图]]></title>
    <url>%2F2018%2F03%2F29%2F%E8%AE%A4%E8%AF%86%E9%B1%BC%E9%AA%A8%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[今天，偶然的机会在同事电脑上漂见“鱼骨图”。晚上吃完饭就在思考这个问题：“为什么我从来没用过鱼骨图分析问题？鱼骨图适用于什么类型的分析？” 查了些资料追求普及，介绍的内容大同小异 概念摘要鱼骨分析法，又名 因果分析法，是一种发现问题”根本原因”的分析方法。 鱼骨图可以进一步被划分为： 整理问题型鱼骨图（各要素与特性值间不存在原因关系，而是结构构成关系） 原因型鱼骨图（鱼头在右，特性值通常以“为什么……”来写） 对策型鱼骨图（鱼头在左，特性值通常以“如何提高/改善……”来写） 怎么作图分析结构 A、针对问题点，选择层别方法（如人机料法环等）； B、按头脑风暴分别对各层别类别找出所有可能原因（因素）； C、将找出的各要素进行归类、整理，明确其从属关系； D、分析选取重要因素； E、检查各要素的描述方法，确保语法简明、意思明确; 分析要点 A、确定大要因（大骨）时，现场作业一般从“人机料法环”着手,管理类问题一般从“人事时地物”层别，应视具体情况决定； B、大要因必须用中性词描述（不说明好坏），中、小要因必须使用价值判断（如…不良）； C、头脑风暴时，应尽可能多而全地找出所有可能原因，而不仅限于自己能完全掌控或正在执行的内容。对人的原因，宜从行动而非思想态度面着手分析； D、中要因跟特性值、小要因跟中要因间有直接的原因-问题关系，小要因应分析至可以直接下对策； E、如果某种原因可同时归属于两种或两种以上因素，请以关联性最强者为准（必要时考虑三现主义：即现时到现场看现物，通过相对条件的比较，找出相关性最强的要因归类。）； F、选取重要原因时，不要超过7项，且应标识在最未端原因。 绘图过程 A、填写鱼头（按为什么不好的方式描述），画出主骨； B、画出大骨，填写大要因； C、画出中骨、小骨，填写中小要因； D、用特殊符号标识重要因素； 使用步骤(1) 查找要解决的问题； (2) 把问题写在鱼骨的头上； (3) 召集同事共同讨论问题出现的可能原因，尽可能多地找出问题； (4) 把相同的问题分组，在鱼骨上标出； (5) 根据不同问题征求大家的意见，总结出正确的原因； (6) 拿出任何一个问题，研究为什么会产生这样的问题； (7) 针对问题的答案再问为什么？这样至少深入五个层次（连续问五个问题）； (8) 当深入到第五个层次后，认为无法继续进行时，列出这些问题的原因，而后列出至少20个解决方法。 看了鱼骨图的概念介绍后，在脑海中立马出现了新的问题：”鱼骨图和思维导图有什么区别？” 鱼骨图和思维导图有什么区别？类同点 都是基于主题逐步分解、细化的过程 都是类树形结构 我的理解理解-1首先、思维导图和鱼骨图都是图形思维，而图形思维最大的特点就是将我们的思维结构化，并由此实现图形化。 而所谓的结构化一般就是构建逻辑思维，任何的逻辑思维都是基于这两个出发点而来的：1、分类；2、顺序；所以只要你的分类与顺序是一样的，那么你的逻辑思维是一样的，由此产生的图形思维也是一样的。而图形、分列、表格……只是图形思维具体的呈现方式。换句话说，在图形思维一样的前提下，不同的人选择了自己认为好的呈现方式让图形思维更可视化、更清晰化、更感性化。 另外，还有不同之处在于思维导图不仅仅能用于构建逻辑思维，还能扩展发散思维，以及还能强化思维记忆。这是和鱼骨图的区别之一！ 理解-2 【思维导图】在实际应用中，思维导图常用于个人，侧重于个人思维的发散，以求思维的全面性，思维导图的层次感是逻辑思维自然的产出。应用场景广泛。 【鱼骨图】常用在多人同时参与的头脑风暴场景，侧重于发挥团队智力逐层解剖问题，找到问题原因。往往会以”问题原因分析 + 跟进Action”作为结果产出。 鱼骨图作图工具 Xmind]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>分析图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写作技巧]]></title>
    <url>%2F2018%2F03%2F17%2FHexo%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[Hexo 写作的技巧与备忘，不喜勿喷。 链接： Hexo命令 文章插入图片原生Markdown语法原生Markdown语法插入图片有三种方式 1. 插入本地图片只需要在基础语法的括号中填入图片的位置路径即可，支持绝对路径和相对路径。例如： 1![image](/home/picture/1.png) 评价：不灵活不好分享，本地图片的路径更改或丢失都会造成markdown文件调不出图。 2. 插入网络图片只需要在基础语法的括号中填入图片的网络链接即可，现在已经有很多免费/收费图床和方便传图的小工具可选。例如： 1![image](http://baidu.com/pic/doge.png) 评价：将图片存在网络服务器上，非常依赖网络和网络图片存储 3. 把图片存入markdown文件用base64转码工具把图片转成一段字符串，然后把字符串填到基础格式中链接的那个位置。基础用法： 1![avatar](data:image/png;base64,iVBORw0......) 这个时候会发现插入的这一长串字符串会把整个文章分割开，非常影响编写文章时的体验。如果能够把大段的base64字符串放在文章末尾，然后在文章中通过一个id来调用，文章就不会被分割的这么乱了。比如： 12![avatar][doge] [doge]:data:image/png;base64,iVBORw0...... 评价：麻烦，费劲。 Hexo方式安装插件与配置 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save，这是下载安装一个可以上传本地图片的插件 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 使用方式在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片 1![你想输入的替代文字](xxxx/图片名.jpg) 注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo装修记录]]></title>
    <url>%2F2018%2F03%2F17%2FHexo%E8%A3%85%E4%BF%AE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前篇《个人博客搭建-结缘Hexo》 从时间投入上来说，搭建Hexo可谓是分分钟的事情，装修可花了我近一天的时间。 Hexo 是一个开放、扩展性强的框架，样式风格、功能都可以通过主题包、插件来实现。完全可以根据个人口味来装修你的博客。 选主题关于Hexo的主题，你可以问度娘”Hexo Themes”网友好评度高的主题，也可以去官网自行挑选https://hexo.io/themes/ 需要提前说明的是：不是选上对应主题，所有功能就有了，这也是我为什么会单独写这篇备忘的原因。 经过各种试用和对比，我最终选择了：NexT这个主题 Go Github，理由如下： （Most Important）界面符合我口味。NexT 中的Pisces主题，样例：IIssNan’s Notes NexT主题集成的插件多，极大的方便了初级的小白用户，详见：NexT主题的_config.xml配置项 博客评论 Baidu Analytics / Google Analytics 文章阅读数量 baidu push algolia_search / local_search 背景画布特效 highlight_theme 等等 支持手机端 安装主题Hexo theme 统一放在Hexo根目录的themes目录下，每个主题一个子目录。 如果主题是Github上的，推荐使用如下命令下载 1git clone https://github.com/iissnan/hexo-theme-next.git themes/next 然后修改hexo根目录下的_config.xml，切换主题 OK, hexo g -&gt; hexo s 看下效果吧 主题配置找到 hexo/themes/next/_config.xml，一项项阅读熟悉吧，注释写的很详细。 基础配置基础配置项： 菜单配置：主页、关于、标签、分类、归档等 头像 打赏 社交主页 侧边栏 主题的_config.xml会引用hexo的_config.xml中基础配置，所以请同时配置hexo的_config.xml，例如：博客抬头、语种、相关数据目录等 背景特效效果图示 配置next主题的_config.xml 支持搜索 文章阅读计数NexT提供两种插件方式：1、leancloud_visitors（国内的） 和 2、firestore(谷歌的)我选用的是leancloud_visitors，配置相对简单一些 配置LeanCloud 注册：https://leancloud.cn打开LeanCloud官网，进入注册页面注册。完成邮箱激活后，点击头像，进入控制台页面创建新应用，如下： 创建名称为Counter的Class 修改NexT的_config.xml配置文件 123456# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: "你的App Id" app_key: "你的App Key" PV/UV Google Analytics12# Google Analyticsgoogle_analytics: '你的Google Analytics Code' 问题解决标签和分类页面不显示问题当时切换NexT主题后，侧边栏的标签、分类点击时，是无法正常显示标签和分类的，属：Cannot Get /tags/ 若出现此问题，请按下方式解决在hexo 目录下执行 步骤一： 1hexo new page 'tags' 步骤二： 编辑刚新建的页面，将页面的类型设置为tags，主题会自动为这个页面显示标签云。 123456---title: TagClouddate: 2018-03-17 15:31:21type: "tags"comments: false #注意：如果有启动多说或Disqus评论，需要关闭评论，添加comments字段并设置为false--- “分类”同理~ 其他推荐 hexo的next主题个性化配置教程]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客搭建-结缘Hexo]]></title>
    <url>%2F2018%2F03%2F17%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA-%E7%BB%93%E7%BC%98Hexo%2F</url>
    <content type="text"><![CDATA[Github Pages + Hexo 搭建个人博客偶然的机会，看到”***.github.io”域名的个人博客，瞬间就来了兴趣，莫非Github能方便的创建个人博客？ 咨询了下度娘，知道了用GitHub Pages + Hexo 搭建个人博客的方式，于是说干就干！ PS: 我用的是Macbook + Shell，某些操作Windows的朋友可能要转化翻译成Windows上的命令。 1. 创建Github仓库首先，个人Github账号应该有吧？如果没有，先去注册一个。 然后，在Github上新建一个仓库，如下图： 确保新建的仓库以Github Pages方式发布 OK，此步骤完成了！ 2. 安装Hexo正式使用Hexo前，请先安装Node.js 和Git 安装Node.js去官网下载并安装：Link 安装Git 作为玩Github的程序员，默认你已经安装了Git 安装Hexo 新建一个目录作为Hexo的根目录（PS: 后续Hexo相关的功能以及写博客都基于此目录） 进入新建的目录 12345npm install hexo-cli -ghexo init #初始化网站npm installhexo g #hexo generate的简写，意思生成博客站点hexo s #hexo server的简写，即启动运行hexo的站点，这一步之后就可以通过http://localhost:4000 查看了 常用Hexo命令 1234hexo c : hexo clean 清除hexo已生成的publichexo g : hexo generate 重新生成hexo站点hexo s : hexo server 运行hexo站点。注：本地运行时，在hexo上做的修改保存后即生效的，不用重新hexo ghexo d : hexo deploy 将hexo发布到github上去。 3. Hexo deploy 到Github 编辑根目录下_config.yml文件 1234deploy: type: git repo: https://github.com/VeryJJ/VeryJJ.github.io.git #这里的网址填你自己的 branch: master 安装hexo deploy插件： npm install hexo-deployer-git –save 在Hexo目录下执行hexo d hexo d 成功后，就大工告成拉！你可以在浏览器输入***.github.io(你新建的github.io仓库)，就能看到你的个人博客拉！ 以后写博客的步骤为： 在电脑本地hexo new ‘文章名’ 丰富你的文章 hexo g hexo d 发布 博客搭建好了，但相信你会觉得它好丑，没关系，请继续阅读下一篇Hexo的装修总结 参考链接 我是如何利用Github Pages搭建起我的博客，细数一路的坑]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 场景化下的命令备忘]]></title>
    <url>%2F2018%2F03%2F15%2FGit%20%E5%9C%BA%E6%99%AF%E5%8C%96%E4%B8%8B%E7%9A%84%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[查看分支1234git branchgit branch -a 查看远端所有分支git branch -v 查看本地分支，以及分支上最新的commit提交信息git branch -vv 在-v基础上，多现实本地分支和远程分支的关联关系 查看某次commit的修改1git show commit号 查看某个文件的历史修改记录12345678910111213141516171819202122232425262728293031323334git log 文件名git log -p 文件名git log --author=提交人 只查看提交人的提交记录git log --pretty=oneline 单行显示提交记录git log --name-only 显示每次commit修改的文件列表git log --name-status 查看commit记录里的文件修改状态git log --grep=&apos;abc&apos; 显示commit描述匹配abc的commit记录git log -S &quot;代码内容&quot; 按代码内容搜索commit记录，如果代码内容部分想用正则表达式，则将-S换成-Ggit log --pretty=&apos;%H %Cblue%cd %C(yellow)%cn %Cred%s&apos; 按commit号+提交日期+提交人+commit标题 显示pretty格式%H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字 %_ae 作者的电子邮件地址 （由于新浪博客显示问题，请去除 %_ae 中的 _ ）%_ad 作者修订日期（可以用 -date= 选项定制格式）（由于新浪博客显示问题，请去除 % ad 中的 _ ）%ar 作者修订日期，按多久以前的方式显示 %cn 提交者(committer)的名字 %_ce 提交者的电子邮件地址（由于新浪博客显示问题，请去除 %_ce 中的 _ ）%_cd 提交日期 （由于新浪博客显示问题，请去除 %_cd 中的 _ ）%cr 提交日期，按多久以前的方式显示 %d: ref名称%s: 提交的信息标题%b: 提交的信息内容%Cred: 切换到红色 %Cgreen: 切换到绿色 %_Cblue: 切换到蓝色 （由于新浪博客显示问题，请去除 %_Cblue 中的 _）%Creset: 重设颜色 %C(...): 制定颜色, as described in color.branch.* config option %n: 换行 查看未commit的本地修改12git diff git diff 文件名 git diff 比较commit之间的差异123git diff commit 比较HEAD与commit之间的差异git diff commit_1 commit_2 比较两个commit之间的差异git diff commit_1..commit_2 与git diff commit_1 commit_2 一样效果 从服务器拉代码git pull --rebase (推荐)会把本地未push得commit放到缓冲区，然后把远程最新版本拉过来，再应用本地commit，这样不会造成本地有新commit时，merge的效果。 git pull 直接更新，若本地和远端都有新commit，都执行自动merge。 拉分支git checkout -b branchName 创建本地新分支 git checkout -b branchName remotes/origin/branchName 以远端分支创建本地新分支 git push origin $newBranch:$newBranch 将本地分支提交到远端进行创建 删除远程分支123$ git push origin :master# 等同于$ git push origin --delete master 提交修改git add . 将修改加到stage状态区 git commint -m &quot;注释&quot; git push 推送所有分支 git push origin develop 只推送develop分支 添加文件git add -A 删除文件git rm 文件名 git rm -r 目录名 Pushgit push push所有分支 git push origin master 将本地主分支推到远程主分支 git push –u origin master 将本地主分支推到远程（如无远程主分支则创建，用于初始化远程仓库） git push origin &lt;local_branch&gt; 创建远程分支，origin是远程仓库名。 git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; 创建远程分支 强制push如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做git pull合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用–force选项。 git push --force origin 合并分支mergegit merge remotes/origin/mc-s-3 将远端mc-s-3分支merge到本地 rebasegit rebase develop git rebase remotes/origin/develop 配置mergetoolgit config –global merge.tool bc3 git config –global mergetool.bc3.path 软件执行文件地址 merge策略1234567891011Git merge 策略的总结:1、使用 -s 指定策略，使用 -X 指定策略的选项2、默认策略是recursive3、策略有 ours，但是没有theirs (Git老版本好像有)4、策略ours直接 忽略 合并分支的任何内容，只做简单的合并，保留分支改动的存在5、默认策略recursive有选项ours 和 theirs6、-s recursive -X ours 和 -s ours 不同，后者如第3点提到直接忽略内容，但是前者会做合并，遇到冲突时以自己的改动为主7、-s recursive -X theirs的对立面是 -s recursive -X ours`注：-s recursive -X ours 合并分支，冲突时以本地为主` 回退未commit的修改git checkout [path] 将指定路径的修改还原到最新版本 回退已commit，未push的修改git reset HEAD &lt;file&gt; --mixed 选项：默认的 --soft 选项：改动会回退到stage状态 --hard 选项：改动会直接丢失。 git rebase -i 想要删除的commit的前一个commit号。 出来的界面里，将想要删除的commit描述改为drop，保存即可。 回退已push的修改git revert 指定的commit号。跳出来的界面，选择要回退的commit内容（取消前面的#） 可以随便选某个commit删除 若revert一个merge的commit，则要指定parent 号 git revert commit 号 -m 1。 这样就选parent 1，那么parent 1又是哪一个呢？一般来说，如果你在master上mergezhc_branch,那么parent 1就是master，parent 2就是zhc_branch. 重排commit顺序git rebase -i commit号 出来的界面中，将列出来的commit行重新排序再保存，就等于修改commit顺序了。 修改commit的描述未push方法一： git rebase -i commit号 对应commit号前改为edit，保存。出来后git commit --amend。将commit描述修改掉，保存。 出来后再git rebase --continue即可。 方法二： git commit --amend 修改最近的一次commit 代码仓库迁移git clone --bare robbin_site robbin_site.git git remote remove origin git remote add origin git@120.27.160.167:ZCY/doc-round-1.git git push –-all -–progress origin 导出指定版本的代码版本 git archive -o ../updated.zip HEAD $(git diff --name-only HEAD^) 例如：git archive -o ./version.zip 指定commit号 或者 git archive --format zip -output &quot;./archive.zip&quot; HEAD tag功能创建taggit tag -a v1.0.0 -m &apos;备注&apos; 查看taggit tag 切换taggit checkout tag名 删除taggit tag -d v1.0.0 指定commit打taggit tag -a v1.0.0 commit号 发布标签git push origin v1.0.0 将本地v1.0.0标签推送到git服务器 git push origin -tags 将本地所有tag一次性推送到git服务器 创建补丁当前分支所有超前master的提交：git format-patch -M master 某次提交以后的所有patch:git format-patch 4e16 --4e16指的是commit名 从根到指定提交的所有patch:git format-patch --root 4e16 某两次提交之间的所有patch:git format-patch 365a..4e16 -o &lt;patch_dir&gt; --365a和4e16分别对应两次提交的名称 某次提交（含）之前的几次提交：git format-patch –n 07fe --n指patch数，07fe对应提交的名称 故，单次提交即为： git format-patch -1 07fe 应用补丁方法一（推荐）12345678910111、在同一个仓库下找到对应的commit号2、切换到对应分支下，git cherry-pick commit 号3、如果冲突，git mergetool 解决冲突。4、git status根据提示commit代码，并pushcherry-pick 一个commit区间git cherry-pick &lt;start-commit-id&gt;^..&lt;end-commit-id&gt; start-commit-id是版本树里较早的commitcherry-pick一个merge commitgit cherry-pick &lt;commit-id&gt; -m parent-number -m代表 --mainline实际例子：git cherry-pick 32b234 -m 1 1，2分别代表什么 查看未push到远程仓库的commit1、查看到未传送到远程代码库的提交次数12345 git status //只能看次数显示结果类似于这样：# On branch master# Your branch is ahead of &apos;origin/master&apos; by 2 commits. 2、查看到未传送到远程代码库的提交描述/说明12345git cherry -v显示结果类似于这样：+ b6568326134dc7d55073b289b07c4b3d64eff2e7 add default charset for table items_has_images+ 4cba858e87752363bd1ee8309c0048beef076c60 move Savant3 class into www/includes/class/ 3、查看到未传送到远程代码库的提交详情1234567891011121314git log master ^origin/master这是一个git log命令的过滤，^origin/master可改成其它分支。显示结果类似于这样：commit 4cba858e87752363bd1ee8309c0048beef076c60Author: Zam &lt;zam@iaixue.com&gt;Date: Fri Aug 9 16:14:30 2013 +0800 move Savant3 class into www/includes/class/commit b6568326134dc7d55073b289b07c4b3d64eff2e7Author: Zam &lt;zam@iaixue.com&gt;Date: Fri Aug 9 16:02:09 2013 +0800 add default charset for table items_has_images 查看两个分支的差异查看dev中有，而master中没有的1234git log dev ^master反之：查看master中有，dev中没有的git log master ^dev 查看dev中比master多了哪些提交（A比B多了哪些，就把A放..右边）1git log master..dev 不在乎谁多谁少，只想看差异的提交1git log --left-right dev...master #--left-right 会帮助显示差异的commit属于哪个分支 整个目录比较差异详情1git difftool develop..pre-online --dir Git stash 暂存1234567891011121314151617git stash 将当前工作区里未commit的修改放到暂存区，将代码恢复到最近的一次修改git stash list 查看暂存区的列表git show stash@&#123;0&#125; see the last stash git stash pop apply lastest stash and remove it from th list git stash clear 清空暂存栈git stash apply stash@&#123;1&#125; 指定暂存区里的某一次stash，应用到本地 删除本地git branch -a 能看到，而远程已经删掉的分支记录1git fetch -p 更改时间显示方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647--date=(relative|local|default|iso|rfc|short|raw) Only takes effect for dates shown in human-readable format, such as when using &quot;--pretty&quot;. log.date config variable sets a default value for log command’s --date option.--date=relative shows dates relative to the current time, e.g. &quot;2 hours ago&quot;.--date=local shows timestamps in user’s local timezone.--date=iso (or --date=iso8601) shows timestamps in ISO 8601 format.--date=rfc (or --date=rfc2822) shows timestamps in RFC 2822 format, often found in E-mail messages.--date=short shows only date but not time, in YYYY-MM-DD format.--date=raw shows the date in the internal raw git format %s %z format.--date=default shows timestamps in the original timezone (either committer’s or author’s).####格式化显示例子：--date=format:&apos;%Y-%m-%d %H:%M:%S&apos;参数：%a Abbreviated weekday name%A Full weekday name%b Abbreviated month name%B Full month name%c Date and time representation appropriate for locale%d Day of month as decimal number (01 – 31)%H Hour in 24-hour format (00 – 23)%I Hour in 12-hour format (01 – 12)%j Day of year as decimal number (001 – 366)%m Month as decimal number (01 – 12)%M Minute as decimal number (00 – 59)%p Current locale&apos;s A.M./P.M. indicator for 12-hour clock%S Second as decimal number (00 – 59)%U Week of year as decimal number, with Sunday as first day of week (00 – 53)%w Weekday as decimal number (0 – 6; Sunday is 0)%W Week of year as decimal number, with Monday as first day of week (00 – 53)%x Date representation for current locale%X Time representation for current locale%y Year without century, as decimal number (00 – 99)%Y Year with century, as decimal number%z, %Z Either the time-zone name or time zone abbreviation, depending on registry settings; no characters if time zone is unknown%% Percent sign 全局更改方式1git config --global log.date relative 代码量统计当天提交的代码量1git log --author=&quot;$(git config --get user.name)&quot; --no-merges --since=1am --stat 统计报告-gitstats 用GitStatX图形化工具查看 统计报告-gitinspector1gitinspector --format=html --since=2018-01-01 --until=2018-12-30 --timeline --localize-output -w ./ &gt; ~/tmp/gitinspector/zcy-payment-center-201801.html gitinspector命令说明123456789101112131415161718192021222324252627282930313233343536➜ car-manage git:(master) gitinspector --help用法：/usr/local/bin/gitinspector [选项]... [目录] 在目录列出有关库的信息,如果没有指定目录，那么将使用现目录。如果有多个目录，将采用指定的最后一个目录长选项的强制性参数对短选项也适用布尔参数只能给予长选项 -f, --file-types=EXTENSIONS 一串逗号分隔的文件类型 这些文件将会被用于计算统计数据. 默认文件类型: java,c,cc,cpp,h,hh,hpp,py,glsl,rb,js,sql -F, --format=FORMAT 指定生成的输出文件的格式； 默认格式是&apos;text&apos; 和 可选格式: html,htmlembedded,text,xml --grading[=BOOL] 按照学生成评判项目的格式， 显示统计数据和信息； 等同于 -HlmrTw 选项 -H, --hard[=BOOL] 记录行数并且寻找重复的内容; 如果数据库较大，这个可能会需要一些时间 -l, --list-file-types[=BOOL] 列出所有现在的数据库分支的文件格式 -L, --localize-output[=BOOL] 在翻译版本存在的前提下，将输出结果翻译到系统语言 -m --metrics[=BOOL] 在分析提交时，检查特定指标 -r --responsibilities[=BOOL] 显示每位作者主要职责 --since=DATE 只显示从特定时间起的结果 -T, --timeline[=BOOL] 显示提交时间轴, 包括作者名称 --until=DATE 只显示特定时间前的结果 -w, --weeks[=BOOL] 按周来显示统计数据，而非月 -x, --exclude=PATTERN 按特定格式排除不应该被统计 的文件，作者名字或邮箱;可以按文件名，作者名， 作者邮箱。可以重复 -h, --help 显示这个帮助信息并退出 --version 显示版本信息并退出gitinspector 会过滤信息并且仅统计那些修改，增加或减少，指定文件类型的提交，如需详细信息，请参考 -f 或 --file-types 选项]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
</search>
