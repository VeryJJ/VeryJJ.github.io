<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一文就让你精通JVM]]></title>
    <url>%2F2020%2F10%2F04%2F%E4%B8%80%E6%96%87%E5%B0%B1%E8%AE%A9%E4%BD%A0%E7%B2%BE%E9%80%9AJVM%2F</url>
    <content type="text"><![CDATA[《一文就让你精通JVM》网上有关JVM的知识贴多如牛毛，其中有纷杂的零碎知识贴，也有整理优秀的长贴。信息量非常充分。 但作为复习或整理JVM知识的而言，还可以有更好的学习用户体验和高效的方式。因此，就想尝试写一篇有关JVM知识点的“秘籍”，让初学者仅读此文就能快速精通JVM的知识脉络以及关键知识，也能让复习着快速反查知识和经验之谈。 黄老师 1、背景知识铺垫1.1、JRE、JVM、JDK先讲讲JVM、JRE、JDK是什么 JVM (Java Virtual Machine) JVM有自己的规范，所有的JVM版本都必须按此规范实现。 JVM是一个抽象、虚拟、不物理存在的”机器”。是通过在真实的计算机上仿真模拟各种计算机功能来实现的。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。 JVM负责执行Java代码 JRE (Java Runtime Environment) JRE包含JVM JRE是一个运行环境容器，提供诸多Java运行需要的libs。包括：标准、非标准的Java组件；Java规范要求、Java规范未要求的组件。 JRE为JVM服务 rt.jar(contains: lang, util, awt, swing, math, runtime libraries) JDK (Java Development Kit) 从Kit单词上可预知JDK会包含很多东西，的确，JDK包含：JVM、JRE，以及Java语言和工具包。 JDK比JRE多的部分：Development, debugging tools Oracle 官方Java概念图 1.2、HotSpot Client/Server模式为什么要铺垫下 HotSpot Client/Server模式？因为后续GC章节默认收集器在不同模式下不同。 HotSpot JVM具有两种模式：Client模式和Server模式。可以理解为针对不同的硬件环境和软件场景做的JVM优化版本。 Java HotSpot Client VM(-client)：轻量级。为在客户端环境中减少启动时间而优化，使用的策略和功能都是较简单版本。 Java HotSpot Server VM(-server)：重量级。为在服务器环境中最大化程序执行速度而设计，使用的策略和功能是为了最大化的发挥硬件优势，提升吞吐量。《《《 作为服务端Java开发，大多数情况下默认Server模式。 HotSpot的安装的模式，32位的hotspot都是client模式；64位的都是server模式的。 可通过java -version查看 若想要修改模式，则需变更JVM的配置文件。32位的虚拟机在 “%JAVA_HOME%/jre/lib/i386/jvm.cfg”；64位的虚拟机在“%JAVA_HOME%/jre/lib/amd64/jvm.cfg”； 2、JVM体系结构 JVM体系结构图 操作系统视角 2.1、Class文件 Class 文件是一种特定的二进制文件格式的文件。格式紧凑，包含了JVM指令集和符号表以及若干其他辅助信息，其编码结构风格被称为“字节码”。 在JVM体系里，不同的硬件(主要是CPU)和操作系统环境下的JVM是不同版本的实现，而Java语言的平台无关性，主要体现在“Class文件”上。Java代码一次编译成Class文件，可以在不同体系的JVM版本下运行。 其他语言只要其代码能被编译成符合Class文件规范的Class文件，就能在JVM上运行。 2.2、内存区 之 虚拟机栈区 见上图《JVM体系结构图》 在JVM里有一块专门为Java线程分配栈空间的内存区，叫”虚拟机栈区”。 每个Java线程创建时都会分配一个“虚拟机栈”，此虚拟机栈的生命周期与其所绑定的线程生命周期一致。 而当线程执行Java代码时，JVM会为每个Java方法创建一个固定结构的内存模型：栈帧。 此“栈帧”结构是线程虚拟机栈入栈/出栈操作的基本单位（入栈/出栈的时机对应Java方法的调用和返回） 栈帧是用于支持JVM进行方法调用和方法执行的数据结构 2.2.1、栈帧(Stack Frame)结构 局部变量表： 编译期确定局部变量表大小。一组变量存储空间， 容量以slot为最小单位，而slot的大小随硬件体系而定，以此来适应硬件体系的差异。 虚拟机规范中未明确指明一个Slot应占用的内存空间大小，只是导向性的说到每个Slot都应该存放一个boolean、byte、char、char、short、int、float、reference或returnAddress类型的数据，这8种数据类型都可以使用32位或更小的物理内存来存放，Slot的长度可以随着处理器、操作系统或虚拟机的不同而发生变化。 操作栈：编译期确定操作数栈最大深度。是一个后入先出栈（LIFO）。操作数栈可类比CPU的寄存器，协助JVM完成Java方法内的调用传值，计算操作。 例如：整数加法的字节码指令iadd再运行的时候操作数栈中最接近栈顶的两个元素已经存入了两个int类型的数值，当执行这个指令时，会将这两个int值出栈并相加，然后将相加的结果入栈。 动态连接： 指向运行时常量池中该栈帧所属Java方法的引用，这样才能找到真正&amp;完整的代码片段。 Class文件中关于方法调用，存的是符号引用。字节码在运行期，需要将符号引用转化为带具体内存地址的代码片段的直接引用。 方法返回地址：方法退出的过程实际上等同于把当前栈帧出栈，并恢复上层方法的局部变量表和操作数栈，把返回值（如果有）压入调用者栈帧的操作数栈中，调整PC计数器的值以指向方法调用指令后面的一条指令等。 方法返回地址里一般存的是调用者当时的PC计数器的值。当方法正常返回时，就返回这个地址。而当方法异常退出时，返回地址要通过异常处理器表来确定。 额外附加信息：预留的扩展区域，由JVM实现方按需使用。 2.3、内存区 之 本地方法栈区本地方法栈是JVM为Native方法提供的内存空间。 本地方法栈里的基本元素结构取决于本地方法接口的具体实现机制。若其使用C连接模型,那么本地方法栈就是C栈。 Java方法栈和本地方法栈之间可以灵活交叉切换。 有些虚拟机的实现直接把本地方法栈和虚拟机栈合二为一，比如典型的Sun HotSpot虚拟机。 2.4、内存区 之 堆内存区 Java堆，是JVM管理的最大的一块内存，也是GC的主战场。里面存放的是几乎所有的对象实例和数组数据。 JIT编译器有栈上分配、标量替换等优化技术的实现导致部分对象实例数据不存在Java堆，而是栈内存。 从内存回收角度：Java堆为分代回收机制，分为年轻代和老年代。这样划分的好处是为了更快的回收内存（每次回收的内存范围相对小） 从内存分配角度：Java堆可以划分出线程私有的分配缓冲区(Thread Local Allocation Buffer,TLAB)；这样划分的好处是为了更快的分配内存； 关联知识点： 1、标量替换：允许将对象打散分配在栈上，比如若一个对象拥有两个字段，会将这两个 字段视作局部变量进行分配。 2.4.1、各内存区默认配比 年轻代：老年代 默认 1：2（将堆空间分为3份） Eden ：from ：to 默认 8：1：1 相关JVM参数 -Xms：初始堆大小。默认物理内存的1/64。 -Xmx：最大堆大小。默认物理内存的1/4。 -Xmn：年轻代大小 -XX:NewRatio：年轻代与老年代的比值 -XX:SurvivorRatio：Eden区域Survivor区的大小比值。默认8:1:1。 2.4.2、TLAB全称 Thread Local Allocation Buffer，线程本地分配缓冲区。 默认是开始的。JVM命令 -XX:+UseTLAB。 由于堆是全局共享的，因此存在同一时间会有多个线程在堆上申请空间的并发情况。为保证堆内存分配操作的原子性，JVM采用“CAS+失败重试”的方式，但这种方式在并发竞争激烈的情况下效率会进一步下降。 因此，JVM额外设计了TLAB 来避免多线程分配对象内存时的冲突处理。 大致原理： JVM在内存年轻代Eden Space中开辟了一小块线程私有的区域，称作TLAB。默认设定为占用Eden Space的1%。 Java中每个线程都会有自己的缓冲区称作TLAB。 由于TLAB是线程私有的，所以内存分配没有锁开销，效率高。 在Java程序中很多对象都是小对象且用过即丢，它们不存在线程共享也适合被快速GC，所以对于小对象通常JVM会优先分配在TLAB上。 故，对象仍旧是在堆上被分配的，只不过分配的方式变了而已。 2.4.3、逃逸分析 关联知识点：逃逸分析只在JVM运行在server模式时才能启用。 逃逸分析 是一种可以有效减少Java 中堆内存分配压力的分析算法。JVM默认开启。 通过逃逸分析，Java Hotspot编译器能够分析出一个即将新创建对象的引用的使用范围，从而决定通过哪种方式分配对象内存（栈上标量替换；TLAB分配；堆分配；）。 2.4.4、对象内存分配的两种方法为对象分配内存空间的任务等同于把一块确定大小的内存从Java堆中划分出来。 指针碰撞 (Serial、ParNew等带Compact过程的收集器)假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”（Bump the Pointer）。 空闲列表 (CMS这种基于Mark-Sweep算法的收集器)如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空闲列表”（Free List）。 2.4.5、对象实例的具体结构 对于填充数据不是一定存在的，仅仅是为了字节对齐。 2.5、内存区 之 方法区当JVM使用类装载器装载某个类时，它首先要定位对应的class文件，然后读入这个class文件，最后，JVM提取该文件的内容信息，并将这些信息存储到方法区，最后返回一个class实例。 注：方法区 是 JVM规范的一部分，并不是实际的实现。实际实现有JDK7以前的永久代，以及JDK8的元空间。 方法区的特点： 1、方法区是线程安全的。假如同时有两个线程都企图访问方法区中的同一个类，而这个类还没有被装入JVM，那么只允许一个线程去装载它，而其它线程必须等待。 2、方法区的大小不是固定的，JVM可根据应用运行期的需要动态调整。其内存也不一定是连续的。 3、方法区也可被垃圾收集（GC），当某个类不在被使用(不可触及)时，JVM将卸载这个类，进行垃圾收集。 方法区主要存放的是已被虚拟机加载的类信息、常量、静态变量、编译器编译后的代码等数据。 常量池分解 2.5.1、静态常量池： 即class文件中的常量池，是文件级、静态级的。class文件中的常量池不仅仅包含字符串(数字)字面量，还包含类、方法的信息，占用class文件绝大部分空间。 2.5.2、运行时常量池： 运行时常量池是方法区的一部分，可以理解为一块专门存放常量的内存。包括装载class文件而放入的常量，也包括代码运行时动态生成的常量（String类的intern()方法）。 静态常量池和运行时常量池的差别在于，一个是存在文件中，是“静”的，一个是存在内存中，是“动”的。 而我们常说的常量池，一般是指在方法区中的动态常量池。 2.6、内存区 之 堆外内存区讲堆外内存前，首先带大家从操作系统视角看堆外内存所处的位置。相信看了直观的图示后能立马有清晰的记忆和理解。 助读说明： 1、先着眼操作系统进程视角，每个进程都拥有自己独占的进程级内存空间。每个进程看到的机器内存大小是一样的，且内存范围也是一样的。为什么？因为用户态进程看到的是逻辑内存地址，并不是真实的物理内存地址。 2、用户态进程的内存地址都是逻辑地址，用malloc分配，故物理上不连续。 3、堆内存在JVM内。受JVM管理，参与GC。 4、堆外内存处于进程内存空间，但属于JVM堆内存之外。 5、堆外内存不受JVM管理，不参与GC。编码时需要主动释放。 2.6.1、直接内存（Direct Buffer） Direct Buffer是堆外内存的一种具体类型，常出现于NIO模型中。 NIO模型中为什么要用Direct Buffer？ 答：NIO需要进行socket连接的收包和发包，这个操作最终要从操作系统用户态进入到内核态进行操作。而内核态调用要求内存地址必须可靠（即在一次完整调用周期内内存地址是不会变的）。但Java堆里的内存会受GC影响而移动整理，地址会变。故NIO不能用堆里的内存。 即使代码层面非要用Java堆内存区做NIO操作，JVM仍会自动将堆内存对象转换为直接内存对象，然后再进行内核态操作。 故Direct Buffer对于JVM来说有如下优点： 1、对于NIO操作,直接用Direct Buffer比用Java堆内存,免去一次内存拷贝，相比之下效率高了。 2、GC压力更小。因Direct Buffer是堆外内存，是代码者自行管理，不用JVM额外操心,GC的范围就缩小了。 其他须知： 1、Direct Buffer创建是比较耗时的，高性能或高频场景下，建议池化。 3、JVM垃圾收集器JVM垃圾回收是继掌握内存模型后，必须进阶更进一步掌握的JVM内存知识。 全世界的程序员都知道：“Java的内存是会自动回收的”。这背后会有哪些知识点呢，请看下文。 3.1、垃圾回收原理3.1.1、垃圾分析策略 引用计数 策略 比较古老的回收算法。原理是额外维护对象上的引用计数。当发生GC时，回收引用计数为0的对象的内存。 此算法的缺点：无法处理循环引用的问题。 ​ 那怎么破解呢？用Java里的弱引用（Weak Reference）破开强引用关系。 可达性分析 策略（主流JVM用的都是这种） 从GC ROOT开始，遍历引用关系节点，当所有的引用节点遍历完毕之后，剩余未遍历到的节点则被认为是没有被引用到的节点，即可被回收的对象节点。 GC Root 示例 需要知晓的是： 1、会有一些列的“GC Roots”且同时工作，在Java中可作为GC Root的对象包括： ​ a）虚拟机栈中引用的对象（栈帧中的本地变量表） ​ b）方法区中类静态属性引用的对象 ​ c）方法区中常量引用的对象 ​ d）本地方法栈中JNI（Native）引用的对象 2、基于GC Root做可达性分析时，需要STW（Stop The World），必须记住此知识点，对于了解后续垃圾回收算法有帮助。 3.1.2、垃圾回收算法 一、引用计数 算法 略 二、标记-清除 算法（Mark-Sweep） 最基础的垃圾回收算法，思想简单且容易实现。 工作上分为两个阶段： 一、标记阶段：从GC Roots开始标记被引用对象； 二、清除阶段：遍历整个堆，把未标记的对象清除； 缺点：会有内存碎片。导致下一次GC的提前到来。 三、复制 算法（Copying） 为了解决Mark-Sweep算法的缺陷，Copying算法就被提了出来。 它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把此内存区域空间一次清空，这样一来就不容易出现内存碎片的问题。 优点：不会出现内存碎片。 缺点：需要两倍的内存空间。 适合对象生命周期短的内存区域，一次能回收绝大部分内存。例如：年轻代 四、标记-整理 算法（Mark-Compact） 此算法结合了“标记-清除”和“复制”两个算法的优点。 分为三个阶段：（我更喜欢划分为三个阶段） 第一阶段：从GC Root开始标记所有被引用对象 第二阶段：遍历整个堆，将存活对象往集中的一片内存区域移动。 第三阶段：基于内存整理后的指针边界，清除整理区域以外的内存空间。 再强调下特殊点：时序上是“标记-&gt;整理-&gt;清理”。 3.1.3、小结一下 堆内存区 垃圾分析策略 常用垃圾回收算法 年轻代 GC Root可达性分析 1、复制算法 老年代 GC Root可达性分析 1、标记-清除算法2、标记-整理算法 3.2、分代收集策略分代收集策略的目的本质是为了更高效效率的进行内存回收。 核心思想是根据对象存活生命周期的特点，将JVM内存划分为若干个区域，不同内存区域按其对象生命周期的特点采用不同的回收频率和回收算法。常见的分代方式：年轻代、老年代、永久代。 反过来试想，假设JVM在一次垃圾回收窗口时，要对整个JVM内存区对垃圾回收分析和清除，Stop The World会持续很长时间。 与垃圾回收策略的匹配 堆内存区 内存区特点 垃圾分析策略 常用垃圾回收算法 年轻代 对象生命周期短，每次回收量大 GC Root可达性分析 1、复制算法 老年代 对象生命周期长，每次回收量小 GC Root可达性分析 1、标记-清除算法2、标记-整理算法 3.3、JVM触发GC的时机3.2.1、JVM自动触发GC的时机 触发YoungGC： 新建对象时，发现eden space满了（或者因内存碎片找不到足够连续的内存），JVM会触发一次YoungGC 触发FullGC： 升到老年代的对象空间大于老年代剩余空间，JVM会触发一次FullGC。或者被HandlePromotionFailure参数强制FullGC。 3.2.2、手动触发GC 代码System.gc()。但只是像JVM提交了一次GC请求，至于何时会真正执行GC，未知。 12345678910111213141516171819/** * Indicates to the VM that it would be a good time to run the * garbage collector. Note that this is a hint only. There is no guarantee * that the garbage collector will actually be run. */ public static void gc() &#123; boolean shouldRunGC; synchronized(lock) &#123; shouldRunGC = justRanFinalization; if (shouldRunGC) &#123; justRanFinalization = false; &#125; else &#123; runGC = true; &#125; &#125; if (shouldRunGC) &#123; Runtime.getRuntime().gc(); &#125;&#125; 只有在justRanFinalization=true的时候才会执行。 强制执行GC的方式 123System.gc();runtime.runFinalizationSync();System.gc(); 3.3、垃圾收集器3.3.1、垃圾收集器类型HotSpot JVM里有7种垃圾收集器，你是怎么记忆它们的呢？ 上图解读： （A）图中展示了7种收集器：Serial、ParNew、Parallel Scavenge、Serial Old、Parallel Old、CMS、G1； （B）图中它们所处区域，表明其是属于年轻代收集器还是老年代收集器： 年轻代收集器**：Serial、ParNew、Parallel Scavenge； 老年代收集器：Serial Old、Parallel Old、CMS； 整堆收集器：G1； （C）两个收集器间的连线，表明它们可以搭配使用： - Serial 配 ：Serial Old 或 CMS； - ParNew 配 ：Serial Old 或 CMS； - Parallel Scavenge 配 ：Serial Old 或 Parallel Old； （D）CMS与Serial Old的连线表示Serial Old是作为CMS出现“Concurrent Mode Failure”失败的后备预案。 3.3.2、常见垃圾收集器对比 GC时STW的停顿时长的排行：G1 &lt; CMS &lt; Parallel收集器 &lt; Serial收集器 JVM默认收集器 怎么查看？ 怎么切换？ 例子：-XX:+UseConcMarkSweepGC 参数 描述 UseSerialGC JVM运行下Client模式下的默认收集器，打开此开关后，使用Serial+Serial Old的收集器组合。 UseParNewGC 使用ParNew+Serial Old的收集器组合。 UseConcMarkSweepGC 使用ParNew+CMS的收集器组合。 UseParallelGC 虚拟机运行在Server模式下的默认收集器，打开此开关后：- JDK 7u4版本前：使用Parallel Scavenge + Serial Old的收集器组合。也是Java书上的说法。- JDK 7u4版本后：使用Parallel Scavenge + Parallel Old的收集器组合，此时的Parallel已经很成熟了。求证过程：Link UseParallelOldGC 使用Parallel Scavenge + Parallel Old的收集器组合 3.3.3、常见垃圾收集器及原理1、（年轻代）Serial ​ Serial（串行）垃圾收集器是最基本、发展历史最悠久的收集器；JDK1.3.1前是HotSpot年轻代收集的唯一选择； ​ 特点： ​ 1、针对年轻代； ​ 2、采用复制算法；单线程收集；全程STW； ​ 3、HotSpot Client模式下的默认年轻代收集器； Serial/Serial Old组合收集器运行示意图 ​ 适用场景：小内存（小于200M），单CPU的环境。 2、 （年轻代）ParNew ​ ParNew垃圾收集器是Serial收集器的多线程版本。 ​ 特点： ​ 1、除了多线程外，其余的行为、特点和Serial收集器一样（两者还共用了不少代码）。 ​ 2、也是针对年轻代，也是复制算法。 ​ 3、”-XX:+UseConcMarkSweepGC”：指定使用CMS后，会默认使用ParNew作为年轻代收集器； ​ 4、”-XX:ParallelGCThreads”：指定垃圾收集的线程数量，ParNew默认开启的收集线程与CPU的数量相同； ParNew/Serial Old组合收集器运行示意图 ​ 适用场景： ​ 在HotSpot Server模式下，ParNew收集器是一个非常重要的收集器，因为除了Serial外，只有它能与CMS收集器配合工作。 3、 （年轻代）Parallel Scavenge ​ （Java8 默认的收集器）Parallel Scavenge又称为吞吐量优先收集器，特点是多线程回收，以吞吐量优先，高效率的利用CPU时间。 ​ 特点： ​ 1、Java8 默认的收集器； ​ 2、年轻代收集器；复制算法；多线程收集； ​ 3、Parallel Scavenge收集器的目标是以高吞吐量为目标，达一个可控制的吞吐量。 吞吐量： 吞吐量 = 运行用户代码时间 /（运行用户代码时间+垃圾收集时间） 高吞吐量即减少垃圾收集时间，让用户代码获得更长的运行时间 ​ 适用场景： ​ Parallel Scavenge收集器的高吞吐量可以最高效率的利用CPU时间，尽快的完成程序的运算任务等，主要适合在后台运算而不是太多交互的任务（其不适合需要与用户交互的程序，良好的响应速度能提升用户的体验，此种场景CMS效果更好）。 4、 （老年代）Serial Old ​ Serial Old是 Serial收集器的老年代版本。是JDK 7u4版本前的老年代默认收集器。 ​ 特点： ​ 1、采用标记整理法； ​ 2、单线程； Serial/Serial Old组合收集器运行示意图 ​ 适用场景： ​ 1、主要用于HotSpot的Client模式。 5、 （老年代）Parallel Old ​ Parallel Old垃圾收集器是Parallel Scavenge收集器的老年代版本；目前已经很成熟了，是JDK 7u4版本后的老年代默认收集器（JDK8的老年代默认收集器）。 ​ 特点： ​ 1、标记-整理 算法； ​ 2、多线程收集； Parallel Scavenge/Parallel Old收集器运行示意图 6、 （老年代）CMS ​ CMS收集器也称为并发低停顿收集器（或低延迟垃圾收集器），以获取最短回收停顿时间为目标。 ​ CMS并不是等内存不足了才进行FullGC，而是基于设定的GC阈值，当超过阈值时主动进行CMS GC，因为CMS GC是与用户线程并发的，故用户线程对于CMS GC的停顿感知是很少的。 CMS的GC阈值： 1、设定了CMSInitiatingOccupancyFraction时，以此为阈值，区间：0~100。 2、CMSInitiatingOccupancyFraction默认为-1，则按“((100 - MinHeapFreeRatio) + (double)( CMSTriggerRatio * MinHeapFreeRatio) / 100.0) / 100.0 ”的值决定，其中MinHeapFreeRatio默认值40，CMSTriggerRatio默认值80，那么阈值为92%。 为什么要通过阈值留有一部分空闲内存时进行GC？因为CMS GC时可能会遇到“浮动垃圾”，见下文。 ^敲黑板： 需要注意：CMS GC不是FullGC！！！ HotSpot VM里对concurrent collection和full collection有明确的区分。所有带有“FullCollection”字样的VM参数都是跟真正的full GC相关，而跟CMS并发GC无关的，cms收集算法只是清理老年代。 CMS收集器运作过程 第一步：初始标记（CMS initial mark）： 仅标记一下GC Roots能直接关联到的对象。需要STW，但速度很快。 第二步：并发标记（CMS concurrent mark）：进行GC Roots Tracing的过程，标出存活对象。因与用户线程并发运行，不能保证标记出所有存活对象。 第三步：重新标记（CMS remark）：多线程。修正并发标记期的标记结果。需要STW，相对也不长。 第四步：并发清除（CMS concurrent sweep）：回收所有垃圾对象。 整个过程中耗时最长的并发标记和并发清除都可以与用户线程一起工作；所以总体上说，CMS收集器的内存回收过程与用户线程一起并发执行； CMS收集器运行示意图 ​ 特点： ​ 1、并发收集（与用户线程并发执行），低停顿。 ​ 2、标记-清除算法，会产生内存碎片。 ​ 缺点： ​ 1、对CPU资源非常敏感 CMS的默认收集线程数量 = (ParallelGCThreads + 3) / 4 ParallelGCThreads = （ncpus &lt;= 8）? ncpus : (3 + （(ncpus * 5) / 8))。CPU数量小于8时，ParallelGCThreads为CPU数量。 并发收集线程占用一部分CPU资源，当CPU数量多于4个，收集线程占用的CPU资源多于25%，对用户程序影响可能较大；不足4个时，影响更大，可能无法接受。 ​ ​ 2、无法处理浮动垃圾，可能出现“Concurrent Mode Failure” ​ a）浮动垃圾（Floating Garbage） 在并发清除时，用户线程新产生的垃圾，称为浮动垃圾； 这使得并发清除时需要预留一定的内存空间，CMS所需要的空间比其他垃圾收集器大； “-XX:CMSInitiatingOccupancyFraction”：用于设置CMS预留内存空间比例； ​ b）”Concurrent Mode Failure”失败 如果CMS预留内存空间无法满足程序需要，就会出现一次”Concurrent Mode Failure”失败； 这时JVM启用后备预案：临时启用Serail Old收集器，而导致另一次FullGC的产生； 这样的代价是很大的，所以CMSInitiatingOccupancyFraction不能设置得太大。 ​ 3、产生内存碎片 解决方法： （1）”-XX:+UseCMSCompactAtFullCollection + -XX:+CMSFullGCsBeforeCompaction” 设置CMS执行N次FullGC后，进行一次带整理的FullGC。默认未开启。 （2）降低-XX:CMSInitiatingOccupancyFraction参数，以提早执行CMS GC动作，虽然CMS GC不会进行内存碎片的压缩整理，但它会合并老年代中相邻的free空间。这样就可以容纳更多的年轻代晋升行为。 适用场景： ​ 1、与用户交互多的场景，注重服务的响应速度，常见于WEB、B/S系统的服务器上的应用。系统停顿时间最短，给用户带来较好的体验。 7、 （整堆）G1 G1收集器运作过程： 第一步、初始标记（Initial Marking）：仅标记一下GC Roots能直接关联到的对象；且修改TAMS（Next Top at Mark Start）,让下一阶段并发运行时，用户程序能在正确可用的Region中创建新对象；需要”Stop The World”，但速度很快； 第二步、并发标记（Concurrent Marking）： 进行GC Roots Tracing的过程； 刚才产生的集合中标记出存活对象；耗时较长，但应用程序也在运行；并不能保证可以标记出所有的存活对象； 第三步、最终标记（Final Marking）： 为了修正并发标记期间因用户程序继续运作而导致标记变动的那一部分对象的标记记录； 上一阶段对象的变化记录在线程的Remembered Set Log；这里把Remembered Set Log合并到Remembered Set中； 需要”Stop The World”，且停顿时间比初始标记稍长，但远比并发标记短；采用多线程并行执行来提升效率。 第四步、筛选回收（Live Data Counting and Evacuation）： 首先排序各个Region的回收价值和成本； 然后根据用户期望的GC停顿时间来制定回收计划； 最后按计划回收一些价值高的Region中垃圾对象； 回收时采用”复制”算法，从一个或多个Region复制存活对象到堆上的另一个空的Region，并且在此过程中压缩和释放内存； 可以并发进行，降低停顿时间，并增加吞吐量； ​ 特点： ​ 1、充分利用多CPU的硬件优势，通过并行缩短STW时间，通过并发让用户线程同时进程。 ​ 2、能独立管理整个堆（年轻代和老年代） ​ 3、结合多种垃圾收集算法，整体上基于标记-整理算法，局部Region间基于复制算法。不会产生内存碎片。 ​ 4、可预测的停顿，低停顿，高吞吐。 ​ ​ 适用场景： ​ 针对具有大内存、多处理器机器的服务端应用。可提供低GC停顿的能力。 4、GC调优4.1、GC调优的目标内存区 一般都针对年轻代、老年代调优，尤其是老年代。 方法区可以进行GC，但一般不操心方法区的GC，因为GC的性价比太低，主要回收“废弃常量和无用的类”。如果内存不够就扩大吧。JDK8方法区的实现为元空间，元空间使用的是本地内存（非堆内存），默认情况下元空间的大小是无限的。 4.2、GC调优的策略 常见的招数，一招招使 注：一定是先找1台机器进行试验，对比，然后再做出选择，发布生产环境。 4.3.1、大多数的Java应用不需要GC调优 大部分需要GC调优的，不是JVM参数问题，是代码问题。 在实际情况中，基于GC情况优化代码比优化GC参数要多得多。 4.2.2、（选择合适的GC收集器）互联网Web应用考虑CMS收集器，提升交互响应性能 GC低停顿：CMS收集器是与用户线程并发的内存垃圾收集器； 大幅减少FullGC：CMS GC是concurrent GC，是周期性主动的回收内存。大幅减少FullGC的发生。 4.2.3、（选择合适的堆大小） 去设置堆大小，别不设置用默认的 拍脑袋定初始参数 -&gt; 运行JVM -&gt; 查监控 -&gt; 调整JVM -&gt; 查监控 -&gt; 反复试验 -&gt; 确认JVM参数 4.2.4、（选择合适的年轻代比重）年轻代尽量大 年轻代 尽量大，那么可以减少YoungGC。进而减少了对象进入老年代的频率,进而减少FullGC的频率。 4.2.5、-XX:MaxTenuringThreshold 合理年轻代对象晋升进入老年代的年龄 -XX:MaxTenuringThreshold 设置年轻代对象进入老年代的年龄大小，减少老年代的内存占用，降低 FullGC 发生的频率 4.3.6、（小心大对象）避免大对象直接进入老年代 怎么应对/避免？ 一：-XX:+PretenureSizeThreshold 控制 设置-XX:+PretenureSizeThreshold 参数：代表超过这个值的时候，对象直接在old区分配内存。默认值是0，代表不管新对象多大都是先在eden中分配内存。 注意：PretenureSizeThreshold参数只对Serial和ParNew两款收集器有效，Parallel Scavenge收集器不认识这个参数。如果遇到必须使用此参数的场合，可以考虑ParNew加CMS的收集器组合。 二：更大的年轻代，含：eden区、survivor区 eden区能申请的到，就不会去old区申请了。同时可适当调高-XX:MaxTenuringThreshold（Linux 64下，默认15），让大对象在年轻代生，在年轻代亡。 同时，可考虑搭配最小堆大小和最大堆大小，并设置MinHeapFreeRatio或MaxHeapFreeRatio来掌控堆大小的按需扩大与收缩。 4.3、GC调优分析工具发现JVM内存问题以及查看对应GC情况是简单的 （原始一点的）登录到服务器上敲命令：jstat、jmap、jstack。 （常见的）登录公司运维平台查看指标情况：Grafana、容器管理平台等 （辅助工具型的）APM工具，如：pinpoint。 pinpoint： Grafana： 一般我们能从统计图表里快速的看到问题现象，例如： FullGC频繁； 年轻代占用比例不高，但老年代会有规律性FullGC； …… 然后，进一步定位GC现象发生的原因和具体代码位置，是需要些技术分析能力和不断试验的。 这里我们需要用工具来帮我们高效的定位问题。 常用工具： 使用顺序： 1、dump JVM堆 2、用工具load dump后的文件，通过工具查看内存占用Top的对象，通过对象Class声明或Reference找到代码位置。 1、JProfiler（推荐）（功能很强大；需付费） 软件下载地址：https://www.ej-technologies.com/products/jprofiler/overview.html IntelliJ IDEA有插件版本 例图： 2、JVisualVM（免费） 软件下载地址：http://visualvm.github.io/index.html IntelliJ IDEA有插件版本 使用指导贴：https://www.cnblogs.com/happy-rabbit/p/6232581.html 例图： 4.4、GC调优的问题举例5、监控指标怎么看5.1、Heap &amp; Non-Heap 以JDK8及以后为背景 例图来自Grafana 例图来自JProfiler 5.1.1、Heap Memory = 堆内存 ​ 看完之前章节，应熟悉堆内存包含什么。这里不再赘述。 5.1.2、Non-Heap Memory = 非堆内存 如果non-heap使用上升趋势，我们应该关注什么呢？ non-heap（非堆内存）指Java进程内存中，JVM 堆内存范围以外的内存。不清楚的同学，复习上文第2章节：《JVM体系结构》 那么，non-heap（非堆内存）主要包含哪些内容呢？ 1、【栈区】：虚拟机栈；本地方法栈；&lt;&lt;&lt;&lt; 这块基本不用操心。也不会持续增大。 2、【方法区】； &lt;&lt;&lt;&lt; 存在持续增大的可能，见本文《怎么构造方法区OOM》 3、【Native Memory】； &lt;&lt;&lt;&lt; 一般是JNI用到的内存，可能是频繁NIO导致大量Direct Buffer；也可能是内存未管理好，持续泄露内存了。 4、【Code Cache】：&lt;&lt;&lt;&lt; 用于编译和保存本地代码的内存。JVM内部处理或优化。一般不操心。 6、JVM类加载器很快就添上此章节 7、其他独立知识点7.1、内存逃逸分析 见本文《2.4.3、堆内存区 - 逃逸分析》章节 7.2、直接内存 见本文《2.4.6、堆外内存区》章节 7.3、Java内存屏障7.3.1、什么是内存屏障（Memory Barrier）？ 内存屏障（memory barrier）是一个CPU指令。 ​ 通过在代码中插入这样一条指令可达到几种目的效果： 作用一：干预编译器的指令重排行为，确保内存屏障前后的代码指令有严格的先后执行顺序。 作用二：内存屏障的另一个作用是强制更新一次不同CPU的缓存。以此来主动影响一些数据的可见性。 例如，一个写屏障会把这个屏障前写入的数据刷新到缓存，这样任何试图读取该数据的线程将得到最新值，而不用考虑到底是被哪个cpu核心或者哪颗CPU执行的。 7.3.2、为什么需要内存屏障 在多CPU（核）场景下，为了充分利用CPU，会通过流水线将指令并行进行。为了能并行执行，又需要将指令进行重排序以便进行并行执行，那么问题来了，那些指令不是在所有场景下都能进行重排，除了本身的一些规则（如Happens Before 规则）之外，我们还需要确保多CPU的高速缓存中的数据与内存保持一致性, 不能确保内存与CPU缓存数据一致性的指令也不能重排，内存屏障正是通过阻止屏障两边的指令重排序来避免编译器和硬件的不正确优化而提出的一种解决办法。 7.3.3、Java中内存屏障的主要类型Java内存屏障主要有Load和Store两类。 对Load Barrier来说，在读指令前插入读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据 。 对Store Barrier来说，在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存。 对于Load和Store，在实际使用中，又分为以下四种： 1、LoadLoad 屏障：序列：Load1，Loadload，Load2 。 确保Load1所要读入的数据能够在被Load2和后续的load指令访问前读入。通常能执行预加载指令或/和支持乱序处理的处理器中需要显式声明Loadload屏障。 2、StoreStore 屏障：序列：Store1，StoreStore，Store2 。 确保Store1的数据在Store2以及后续Store指令操作相关数据之前对其它处理器可见（例如向主存刷新数据）。 3、LoadStore 屏障：序列： Load1，LoadStore， Store2 。 确保Load1的数据在Store2和后续Store指令被刷新之前读取。在等待Store指令可以越过loads指令的乱序处理器上需要使用LoadStore屏障。 4、StoreLoad 屏障：序列: Store1，StoreLoad， Load2 。 确保Store1的数据在被Load2和后续的Load指令读取之前对其他处理器可见。 7.3.4、Java中内存屏障的使用 1、Synchronized 通过 Synchronized关键字包住的代码区域，当线程进入到该区域读取变量信息时，JVM保证读到的是最新的值。这是因为在同步区内对变量的写入操作，在离开同步区时就将当前线程内的数据刷新到内存中，而对数据的读取也不能从缓存读取，只能从内存中读取，保证了数据的读有效性。这就是插入了StoreStore屏障 2、volatile 知识点：如果一个字段被声明成volatile，java线程内存模型确保所有线程看到这个变量的值是一致的。 使用了volatile修饰变量，则对变量的写操作，会插入StoreLoad屏障。 大致过程： 1、在volatile变量的作用域插入内存屏障，防止指令重排序 2、通过内存屏障，强制将本CPU的修改操作立即写入主存。此时会利用缓存一致性机制，组织多个CPU同时刷主存的此数据区域。 3、本CPU回写volatile字段数据到主存后，其他CPU的嗅探技术会发现此字段已被回写过主存了，其他CPU的嗅探技术会将它的字段缓存设置为无效，其他CPU下次访问此字段时没，会强制从主存读最新值。 3、Unsafe类 UNSAFE.putOrderedObject类似这样的方法,会插入StoreStore内存屏障 Unsafe.putVolatiObject 则是插入了StoreLoad屏障 7.4、Java的平台无关性 Java的平台无关性(一次编译、到处运行)得益于： 1、统一的Class字节码文件格式； 2、统一的JVM 规范 (指令集，内存模型，操作数栈架构)； JVM将平台相关性的活给干了，屏蔽了差异。 JVM不止有”平台无关性”，还有”语言无关性”，只要最终能编译成符合Class字节码文件格式规范要求的，都能在JVM上运行。如：Coljure, Groovy, JRuby, Scala等 7.5、Java对象引用类型对象引用类型分为强引用、软引用、弱引用、虚引用。 强引用：就是我们一般声明对象时是虚拟机生成的引用，强引用环境下，垃圾回收时需要严格判断当前对象是否被强引用，如果被强引用，则不会被垃圾回收。对应到我们日常写的代码。 软引用：软引用一般被作为缓存来使用。与强引用的区别是，软引用在垃圾回收时，虚拟机会根据当前系统的剩余内存来决定是否对软引用进行回收。如果剩余内存紧张，则虚拟机会回收软引用所引用的空间。 弱引用：弱引用与软引用类似，都是作为缓存来使用。但与软引用不同，弱引用在进行垃圾回收时，是一定会被回收掉的，因此其生命周期只存在于一个垃圾回收周期内。 “软引用”和“弱引用”比较少见。 他们一般被作为缓存使用，而且一般是在内存大小比较受限的情况下做为缓存。因为如果内存足够大的话，可以直接使用强引用作为缓存即可，同时可控性更高。 虚引用：使用虚引用的目的就是为了得知对象被GC的时机，可以利用虚引用来进行销毁前的一些操作，比如说资源释放等。虚引用一个很重要的用途就是用来做堆外内存的释放，DirectByteBuffer就是通过虚引用来实现堆外内存的释放的。 7.6 、容器化环境中的JVM内存设置须知在JDK10前，JVM是无法感知容器环境存在的，JVM获取到的有关系统硬件的指标都是实际物理机的CPU和内存指标。这其实对于JVM运行环境来说是不合理的。 Java在JDK10以后，开始了对容器资源限制的支持（支持向linux cgroup获取容器内的硬件资源指标），可以使用-XX:+UseContainerSupport参数来指定JVM使用容器的内存指标，注：此参数是JVM内默认开启的。（其他类似的JVM还有：-XX:InitialRAMPercentage； -XX:MaxRAMPercentage等） 值得庆幸的是，其中一些功能已被移植到JDK-8u131及以后的版本。在JDK-8u131+及java9，需要加上”-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap”才能使得Xmx感知docker的memory limit。 我们知道JVM对于CPU和内存都有默认取值逻辑，且这个对于JVM的运行性能影响非常大。这个值设置的不对会严重影响线上应用的可用性和性能。 推荐： 建议大家部署Java应用时，必须设置明确的”Xms,Xmx”。减少外部依赖或假设，以减少未知风险的发生概率。 7.7、怎么构造方法区OOM？先牢记知识点方法区存的是哪些东西：“已被虚拟机加载的类信息、常量、静态变量、编译器编译后的代码等数据” 观察这些存放的信息里，哪几个在JVM运行期是动态的？ 运行期产生大量的动态类。 持续高频使用String.intern()方法，产生大量常量。 怎么优化？ 通过工具查看方法区大量的动态类的来源代码。 一般方法区的溢出是由于大量的动态类，而动态类往往来自于框架或三发SDK。基本没的干预，所以一般是调大方法区大小。 &gt;&gt;&gt;&gt; 更多内容，很快到来 &lt;&lt;&lt;&lt;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务API设计之——API命名规范]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%8D%E5%8A%A1API%E8%AE%BE%E8%AE%A1%E4%B9%8B%E2%80%94%E2%80%94API%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[API命名规范命名风格 面向资源 同RESTful命名风格 在大型系统中，常以”业务领域”视角进行模块划分，以达到业务”高内聚低耦合”的效果。 “业务领域”必有”数据对象”沉淀，从宏观抽象的角度看，&quot;数据对象&quot;可统称为&quot;资源&quot;，”业务领域”就是业务相近的”资源”的集合。 &quot;资源&quot;一定是业务抽象后的对象： 可以是具体的数据对象： 商品 订单 合同 发票 采购计划 etc 可以是抽象的对象概念： 租户 用户 支付 文件 需求 etc &quot;业务领域&quot;与&quot;业务领域&quot;之间的依赖，可理解为是对&quot;资源&quot;操作(读、写、通知)的依赖。 所以，API作为&quot;业务领域&quot;间沟通的手段，其应该(Should)以面向资源角度进行命名。 注：子资源，需要逐级索引命名，例如：修改-订单-商品：updateOrderItem。 单一视角 参见单一视角原则 动宾风格API应该(Should)以&quot;动宾短语&quot;风格命名。 例如： 12345xxx.xxx.xxx.OrderService // 上下文已涵盖Order语义Response&lt;T&gt; save(...) Response&lt;T&gt; updateItem(Long orderId, List&lt;T&gt; items) 1234567xxx.xxx.xxx.WCService // 上下文未涵盖Order语义Response&lt;T&gt; saveOrder(...) Response&lt;Boolean&gt; removeOrder(Long orderId) Response&lt;T&gt; updateOrderItem(Long orderId, List&lt;T&gt; items) // 逐级索引子资源 统一术语API命名统一”动词”术语、”名词”术语。优点是能风格一致，经验复用。 详见政采云API术语参考 注：统一术语的节奏，参考研发级术语规范逐步执行：业务内统一、业务领域内统一、平台统一。 错误实践-1：”商品”命名不统一1234业务1：商品 -&gt; item ✔️业务2：商品 -&gt; items业务3: 商品 -&gt; product业务4：商品 -&gt; goods 错误实践-2：”特性”命名不统一123业务1：特性 -&gt; feature ✔️业务2: 特性 -&gt; character业务3：特性 -&gt; rule 错误实践-3：”金额”命名不统一123业务1：金额 -&gt; amount ✔️业务2: 金额 -&gt; money业务3：金额 -&gt; sum 错误实践-4：”校验”命名不统一123业务1：校验 -&gt; verify业务2: 校验 -&gt; check ✔️业务3：校验 -&gt; test 错误实践-5：”分页”命名不统一123业务1：分页 -&gt; page业务2: 分页 -&gt; paging✔️业务3：分页 -&gt; list 错误实践-6：”创建”命名不统一123业务1：创建 -&gt; save✔️业务2: 创建 -&gt; create业务3：创建 -&gt; insert 错误实践-7：”删除”命名不统一1234业务1：删除 -&gt; delete业务2: 删除 -&gt; remove✔️业务3：删除 -&gt; disable 业务3：删除 -&gt; cancel 错误实践-8：”检索”命名不统一123业务1：搜索 -&gt; query✔️业务2: 搜索 -&gt; search业务3：搜索 -&gt; list 常见API命名参考 假设：未按资源划分Service(上下文未界定资源域)的情况 “XXX”指某一种资源，”xxx”指”XXX”下的子资源 分页查询 正确实践 1Response&lt;Page&lt;T&gt;&gt; pagingXXX(QueryDTO q) //用对象包装查询条件 错误实践 1Response&lt;Page&lt;T&gt;&gt; pagingXXX(String name, String code, Long orgId, Long creatorId, Integer pageNo, Integer PageSize) 以上错误实践缺点：1、对于调用方来说，无论以什么条件查询，都需要逐个条件传参2、API对扩展不友好，一旦想增加查询条件，API就不兼容。 列表查询 正确实践 1Response&lt;List&lt;T&gt;&gt; listXXX(...) 获取单个详情 正确实践 12345Response&lt;T&gt; getXXX(Long id) 类同条件，用重载Response&lt;T&gt; getXXX(String code) 错误实践 123Response&lt;T&gt; getXXXById(Long id) Response&lt;T&gt; getXXXByCode(String code) 说明： API契约应该由”API名 + 入参”共同组成，而不是只靠”API名”说明一切。 API方法支持获取单个详情的方式，可以通过入参字段名自解释。无需再用”By***”来额外标注。 不带”By***”声明的方法语义上更具有扩展性。 创建 正确实践 1Response&lt;T&gt; saveXXX(...) //参照《阿里巴巴Java编码规范》 删除 正确实践 1Response&lt;T&gt; removeXXX(...) //参照《阿里巴巴Java编码规范》 更新 正确实践 123Response&lt;T&gt; updateXXX(...) //参照《阿里巴巴Java编码规范》Response&lt;T&gt; updateXXXxxx(...) //更新主资源下的子资源 提审 正确实践 1Response&lt;T&gt; submitXXX(...) 审核 正确实践 1Response&lt;T&gt; auditXXX(...) 退回（退回到流程中的某一步） 正确实践 1Response&lt;T&gt; returnXXX(...) 撤销（退回到流程的第一步） 正确实践 1Response&lt;T&gt; cancelXXX(...)]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>API命名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务API设计之——API错误返回规范]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%8D%E5%8A%A1API%E8%AE%BE%E8%AE%A1%E4%B9%8B%E2%80%94%E2%80%94API%E9%94%99%E8%AF%AF%E8%BF%94%E5%9B%9E%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[API错误返回规范禁止通过抛异常形式返回API业务错误API禁止抛Checked异常，即业务处理上的参数错误、逻辑错误、业务错误等禁止通过抛异常形式返回，应用Response#code, message表达业务错误。 注：不要逼调用方到处写try{}catch()。 正例： 1Response&lt;T&gt; saveDesposit(...); 反例： 1T saveDesposit(...) throws ServiceException, IllegalArgumentException, ValidationException; 禁止通过抛异常形式返回API业务错误API禁止抛Checked异常，即业务处理上的参数错误、逻辑错误、业务错误等禁止通过抛异常形式返回，应用Response#code, message表达业务错误。 注：不要逼调用方到处写try{}catch()。 正例： 1Response&lt;T&gt; saveDesposit(...); 反例： 1T saveDesposit(...) throws ServiceException, IllegalArgumentException, ValidationException; 需要调用方做错误细分处理的，API提供方务必一并提供判断工具类 正例： 1234567891011public void saveXXX()&#123; Response&lt;T&gt; result = xxxWriteService(...) if (!result.isSuccess())&#123; if (xxxUtils.isBankUnSupport(result.getCode))&#123; &lt;&lt;&lt;API提供方提供工具类解析code含义，且code含义可持续迭代更新，调用方无感知。 //银行渠道未开通，需要特殊提示 ... &#125;else&#123; ... &#125; &#125;&#125; 反例： 1234567891011public void saveXXX()&#123; Response&lt;T&gt; result = xxxWriteService(...) if (!result.isSuccess())&#123; if (&quot;10101&quot;.equals(result.getCode))&#123; &lt;&lt;&lt;调用方按API提供方的错误码值做硬编码，代码耦合。 //银行渠道未开通，需要特殊提示 ... &#125;else&#123; ... &#125; &#125;&#125; 【推荐】API返回可直接显示给用户的中文提示信息API失败时，只有API实现方最清楚是什么原因，该怎么提示。那么，请提供对应的提示信息。 我们系统中存在一些用国际化风格的error message，而当前的国际化实现方式真如你想的那么好用吗？ error message国际化原理： 代码中的提示信息国际化配置文件 国际化提示原理 1) 提示信息国际化的行为发生在Web层，Web层启动时会加载Web层的resources/messages提示信息文件 2)当REST API需要返回提示信息时，Web会根据HTTP 请求中的Locale值（例如：zh_CN、zh_TW、en_US、es_ES_Traditional_WIN等）来决定返回哪一种语言的提示信息。将errorMessage以此种语言方式返回给浏览器进行提示。 问题： 1）在分布式系统中，各个应用按领域自治，其resources/messages只维护了自身业务需要的errorMessage。 2）当图中C Service 将errorMessage = template.status.not.match 返回给 XX Service，XX Service直接透传给XX Web的情况下，XX Web的resources/messages是不包括template.status.not.match的，所以此errorMessage将无法正确的展示其本应该提示的信息。 所以，推荐API返回可直接显示给用户的中文提示信息。 正例： 123456789101112public Response&lt;Boolean&gt; saveTemplate(...) &#123; try&#123; ... &#125;catch(StateMachineException e)&#123; log.warn(&quot;...&quot;); ... return Response.fail(&quot;模板配置正在审核中，请在审核完成后再更新&quot;); &#125;catch(Exception e)&#123; ... &#125;&#125; 反例： 123456789101112public Response&lt;Boolean&gt; saveTemplate(...) &#123; try&#123; ... &#125;catch(StateMachineException e)&#123; log.warn(&quot;...&quot;); ... return Response.fail(&quot;模板管理状态机异常&quot;); &#125;catch(Exception e)&#123; ... &#125;&#125; 【推荐】返回具备可读性，引导性的错误提示信息 正例： 123456789101112public Response&lt;Boolean&gt; saveTemplate(...) &#123; try&#123; ... &#125;catch(StateMachineException e)&#123; log.warn(&quot;...&quot;); ... return Response.fail(&quot;模板配置正在审核中，请在审核完成后再更新&quot;); &#125;catch(Exception e)&#123; ... &#125;&#125; 反例： 例1 123456789101112public Response&lt;Boolean&gt; saveTemplate(...) &#123; try&#123; ... &#125;catch(StateMachineException e)&#123; log.warn(&quot;...&quot;); ... return Response.fail(&quot;模板管理状态机异常&quot;); &lt;&lt;&lt;&lt; 你作为用户，是不是吓一跳？ &#125;catch(Exception e)&#123; ... &#125;&#125; 例2 123456789101112public Response&lt;Boolean&gt; saveTemplate(...) &#123; try&#123; ... &#125;catch(StateMachineException e)&#123; log.warn(&quot;...&quot;); ... return Response.fail(e.getMessage()); &lt;&lt;&lt;&lt; message谁都看不懂，没有任何意义 &#125;catch(Exception e)&#123; ... &#125;&#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>错误码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务API设计之——API版本规范]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%8D%E5%8A%A1API%E8%AE%BE%E8%AE%A1%E4%B9%8B%E2%80%94%E2%80%94API%E7%89%88%E6%9C%AC%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[API版本规范发布RELEASE版本正式发布的api包必须是RELEASE版本 eg. 12345&lt;dependency&gt; &lt;groupId&gt;cn.gov.zcy.paas.template&lt;/groupId&gt; &lt;artifactId&gt;template-api&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt;&lt;/dependency&gt; 版本号风格使用 《Semantic Versioning》风格 概述Version号由 “MAJOR.MINOR.PATCH” 三段组合构成，version号增加含义： MAJOR version：【主版本号】代表API发生了不兼容的变更，即使是微小的不兼容。 MINOR version：【次版本号】代表以兼容的方式新增了功能、特性 PATCH version：【补丁版本号】代表以兼容的方式做了bugfix 用法 / FAQ版本号以0开始 X.Y.Z 三个版本号都是以0开始。 【特别注意】当版本号是 “1.0.9.RELEASE”时，它的下一个补丁版本号是”1.0.10.RELEASE” ！！！ 而不是”1.1.0.RELEASE”，这里不存在满十进位之说。 初始 MAJOR version 初始MAJOR version以0开始，代表业务的初始开发阶段，这过程中功能上任何改变都可能发生，此时的API是不稳定的。 初始版本一旦发布生产环境，即将MAJOR version变更为1，即 1.0.0.RELEASE。是第一个基线版本。 预发布版本 可以通过在补丁版本之后紧跟附加连字符和一系列点分隔标识符来表示预发布版本。标识符必须仅包含ASCII字母数字和连字符[0-9A-Za-z-]。标识符不能为空。数字标识符不得包含前导零。 预发布版本的优先级低于关联的普通版本。 预发布版本表示版本不稳定，可能无法满足其关联的正常版本所表示的预期兼容性要求。示例：1.0.0-alpha，1.0.0-alpha.1,1.0.0-0.3.7,1.0.0-x.7.z.92]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>API版本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务API设计之——API参数规范]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%8D%E5%8A%A1API%E8%AE%BE%E8%AE%A1%E4%B9%8B%E2%80%94%E2%80%94API%E5%8F%82%E6%95%B0%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[【强制】字段名称用小驼峰风格【强制】Service API返回值必须使用Response包装 Service API返回值强制要求进行通用包装，例如：Response。 Response的作用： 统一方法表示API调用是否成功 API调用失败时，统一格式反馈错误Code，错误Message 统一的Response易于调用方经验复用，框架集成 作为API调用方，其编码诉求很简单： API调用是否成功； 调用不成功时，提示文案是什么； 调用方几不想： 不想关心API内部有多牛逼 不想关心API可能会抛的各种Exception，以及因此不得不做各种异常处理 关于当前不统一的Response 【新业务】【强制】使用架构组定义的统一Response：ZCY Response 目前业务方有自定义Result/Response，风格和作用大同小异。有更好的设计可以自荐给架构组集成，杜绝各自开辟重复的新定义。 【强制】杜绝完全不规范的缩写，避免望文不知义。（国际通用缩写除外） 错误实践 AbstractClass“缩写”命名成 AbsClass; condition“缩写”命名成 condi； 此类随意缩写严重降低了代码的可阅读性。 【强制】禁止使用 Map 作为参数类型Map&lt;K,V&gt;机制非常灵活，但这样的灵活却是负作用巨大。 Map的数据说明是晦涩的，调用方、实现方之间需要具有隐式的契约解释支持哪些Key，每个Key的Value是什么类型。增加了双方的使用复杂度。 Map&lt;K,V&gt;不可被校验。加之第1条的使用复杂度，导致使用上非常容易出错。 用Map类型字段做预留扩展性的设计都是不优雅的设计。 注：参数中的调用方自定义数据部分允许使用Map。API提供方不关系、不解析、只透传。 【强制】业务对象/查询条件用DTO封装，禁止以入参方式平铺字段。 正确实践 分页查询，将查询条件以DTO方式包装。 Dubbo序列化特点： Dubbo API的POJO类中，UID不一致：没关系。 Dubbo API的POJO类中，字段数量不一致：没关系，只要字段名和类型一致，数据能反序列化成功。 发送方比接收方的字段多：没关系。 发送方比接收方的字段少：没关系。 1Response&lt;Page&lt;T&gt;&gt; pagingXXX(QueryDTO q) 错误实践 1Response&lt;Page&lt;T&gt;&gt; pagingXXX(String name, String code, Long orgId, Long creatorId, Integer pageNo, Integer PageSize) 以上错误实践缺点：1、对于调用方来说，无论以什么条件查询，都需要逐个条件传参。2、API对扩展不友好，一旦想增加查询条件，API就不兼容。 【推荐】DTO字段设置JSR303 Annotation进行基础校验 正确实践 123public interface ZcyPayFacade &#123; Result&lt;Boolean&gt; validTradePay(@NotNull @Valid TradePayPO tradePayPO);&#125; 1234567891011121314151617181920212223242526272829303132333435public class TradePayPO implements Serializable &#123; @NotBlank @Length(max = 15) /** 业务交易编号(订单编号) */ private String businessTradeNo; /** * 业务渠道：1-订阅，2-CA * @see BusinessTypeEnum * * */ @NotNull @Range(min = 1, max = 2) private Integer businessType; ...... /** 商户名称(商家) */ @NotBlank @Length(max = 50) private String merchantName; /** 订单标题（即商品名称），粗略描述用户的支付目的。如“喜士多（浦东店）消费”*/ @NotBlank @Length(max = 256) private String orderSubject; /** 订单描述（即商品描述），可以对交易或商品进行一个详细地描述，比如填写&quot;购买商品2件共15.00元&quot;*/ @NotBlank @Length(max = 128) private String orderBody; ......&#125; 【推荐】在客户端完成基础字段校验 方式1：【推荐】自定义Dubbo Filter实现通用拦截、校验。 方式2：【推荐】通过Builder模式构建入参对象。 方式3：【不推荐】Dubbo 客户端参数校验，要求consumer方设置validation=”true”，Dubbo 客户端参数校验。缺点：以抛异常方式处理校验失败，需要业务方额外处理Exception。而且，IDE并不会提示consumer方需要处理ConstraintViolationException。 方式4：Dubbo方式，local-stub特性。实现较复杂，校验代码通用性低。Dubbo local-stub 注：此规范与《阿里巴巴Java编码规范》互补，同时有效。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>API规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务API设计之——API设计原则]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%8D%E5%8A%A1API%E8%AE%BE%E8%AE%A1%E4%B9%8B%E2%80%94%E2%80%94API%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[你是否也感同身受？ 对接XX业务时，XX业务具备的功能和API全靠跑业务负责人那反复逐个询问、确认。用哪个API；怎么用；有没有限制；等等 各个业务间，甚至同一业务内，API风格不统一。 API命名：按自然语义全翻译的；按属性角度定义的；按操作角度定义的；动宾、非动宾的；复数、非复数的；等等 API入参：带Map的；相同语义字段名称不一样； API出参：有包装Resoponse的；直接返回结果数据的；相同数据，返回格式和字段名称有差别的； 错误信息：直接返回中文提示的；返回提示信息编码的；返回异常类型的；等等 XX业务API性能方面未知。 随着业务的演进，开放的API持续在增加，但类同的很多 API编码规范迫在眉睫 优秀API的特质 自解释 从API本身一眼就能看懂API是干什么的，支持的用法，适用的场景，异常的处理等 易学习 有完善的文档，以及提供尽可能多的示例和可copy－paste的代码。 易使用 功能强大，但使用简单。不增加调用方的使用成本（例如要求业务方用API时需要额外的配置和依赖），不暴露复杂的细节、冗长的使用流程给调用方感知。调用方只做最小的感知和最少的传参。 难误用 优秀的API可以使有经验的开发直接使用API而不需要阅读文档。 充分的静态检查、动态校验、显式的异常说明、有效的错误提示。 ZCY API 设计原则1. 充分原则不是随便一个功能就要有个接口，也不是随便一个需求就要加个接口。 每新建一个接口，要有充分的理由和考虑，即这个接口的存在是十分有意义和价值的。无意义的接口不仅增加了维护的难度，更重要是对于程序的可控性的大大降低，接口也会十分臃肿。 2. 单一视角原则设计接口时，分析的角度要统一。否则会造成接口结构的混乱。例如：不要一会以角色的角度设计，一会儿就要以功能的角度设计。 推荐：以”属性对象 + 行为”的视角定义API 3. 单一功能原则每个API接口应该只专注一件事，并做好。产品概念简单、关系清楚。功能模棱两可，诸多特殊逻辑的API肯定不是个优雅的API，且会造成功能类似重复的API。 注：如果API它很难命名，那么这或许是个不好的征兆，好的名称可以驱动开发、并且只需拆分与合并模块即可。 功能大而全的API在灵活性、简单性方面肯定捉襟见肘。定义API的粒度之前，建议先将业务分领域、划边界，以此来提取业务对象，然后再根据业务对象用例来设计单一功能的API。 比如：查询会员，可能除了查询会员表外还要获取该会员的其他必要信息，但不要在查询会员的同时还有修改权限等类似的其他业务功能，应该分成两个接口执行。 4. 简单原则接口设计简单、清晰。API执行的功能可以很丰富、很强大，但API声明和用法一定要尽量的简单，不能将功能的丰富通过复杂的用法来实现，这会导致API功能不单一，演进不可控。 最终的评审要看API的简单易用程度。 你写的例子，能不能让你的代码看起来更简单？ 你是不是强迫调用方关注/提供他们不在乎的选项/配置？ 有没有毫无价值的额外步骤？ 编写的代码一定要易于读、易于理解，这样别人才会欣赏，也能够给你提出合理化的建议。相反，若是繁杂难解的程序，其他人总是会避而远之的。 5. 抽象原则API的入参、出参所述的对象、属性，一定是按业务特性进行抽象后的实体。误将底层数据模型概念如实的反应到API上。抽象API、抽象对象实体更宏观，具有更好的适用性、兼容性、扩展性。 6. 兼容扩展原则对扩展开放，对修改关闭。保证API的向后兼容。 扩展参数应当是便利的，保证后续类似的需求，可以在已有的API上通过兼容扩展的方式实现。 7. 最小惊讶原则代码应该尽可能减少让读者惊喜。业务API只需根据需求来设计即可，不需要刻意去设计一下复杂无用、华而不实的API，以免弄巧成拙。 8. 低耦合原则API应该减少对其他业务代码的依赖关系。低耦合往往是完美结构系统和优秀设计的标志。 耦合的种类： 代码实现业务逆向调用。 条件逻辑依赖耦合。例如：此API在处理国税网超订单类型时，需要额外发送结算支付凭证上传的事件MQ出来。 耦合API无关的业务行为。例如：采购计划链路日志API被调用时，若是项目采购委托单的情况，需要额外调用公告的API拉取链路信息，新建成为一条此委托单的一条链路日志。 9. 正交原则正交性是指改变某个特性而不会影响到其他的特性。 API之间的功能应该成正交性，无功能重合。API之间应该是互相补充的关系。 10. 易测试原则对于API调用者而言，API应该是可被测试且易于被测试的。测试API不需要依赖额外的环境、容器、配置、公共服务等。 对可测试友好的API也是可被有效集成测试的前提。 11. 统一原则API要具备统一的命名、统一的入/出参规范、统一的异常规范、统一的错误码规范、统一的版本规范等。 统一规范的API优点： 易于被框架集成、处理 有助于API调用方、API提供方开发经验复用 避免犯错，避免误用]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>API设计原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JStorm-分享资料]]></title>
    <url>%2F2018%2F09%2F16%2FJStorm-%E5%88%86%E4%BA%AB%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[JStorm分享-课件资料]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>实时计算</tag>
        <tag>JStorm</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring数据库原理-事务管理]]></title>
    <url>%2F2018%2F08%2F11%2FSpring%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86-%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[知识点分解核心类对象 对象 一句话介绍 PlatformTransactionManager Spring事务的核心底层interface，定义了事务核心方法 AbstractPlatformTransactionManager Spring标准事务处理流程的抽象基类，定义了Spring事务处理框架 TransactionDefinition 事务属性相关。事务隔离级别；超时；传播行为；等 TransactionStatus 事务实例状态对象，可供查询，用于回滚、SavePoint等场景 DataSourceTransactionManager Spring框架TransactionManager的典型实现 TransactionTemplate 将编程式上下文多个步骤合并成一个核心的execute方法，方便事务编程 PlatformTransactionManager jar包 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt;&lt;/dependency&gt; Spring事务的核心底层interface，定义了事务核心方法：getTransaction, commit, rollback。 AbstractPlatformTransactionManager jar包 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt;&lt;/dependency&gt; Spring标准事务处理流程的抽象基类，定义了Spring事务处理框架。 AbstractPlatformTransactionManager 利用模板方式定义了Spring标准事务的处理流程，并提供了必须的默认实现，且将doBengin, doSuspend, doResume, doCommit, doRollback等方法开放给继承类实现。 AbstractPlatformTransactionManager 提供了如下事务流程功能 确定是否已存在事务 处理事务传播行为 控制事务的暂停和恢复 检查commit上的rollback-only标记 在回滚时进行必要的处理 触发已注册的事务同步回调。trigger[Before/After][Begin/Commit/Rollback/…]系列，参见：TransactionSynchronizationUtils。 DataSourceTransactionManager jar包 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt;&lt;/dependency&gt; Spring TransactionManager的典型实现类。 继承了AbstractPlatformTransactionManager类，并做了完整的实现。可供编程式事务开发。也可作为TransactionManager的具体实现注入到如TransactionTemplate， Mybatis SqlSessionFactory中去。 TransactionTemplate jar包 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt;&lt;/dependency&gt; TransactionTemplate 提供的是便捷的编程式事务的方法，将编程式上下文多个步骤合并成一个核心的execute方法。 其本身不具备事务管理的机制，需要通过注入PlatformTransactionManager的Bean实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class TransactionTemplate extends DefaultTransactionDefinition implements TransactionOperations, InitializingBean &#123; public TransactionTemplate(PlatformTransactionManager transactionManager) &#123; this.transactionManager = transactionManager; &#125; // 核心方法 @Override public &lt;T&gt; T execute(TransactionCallback&lt;T&gt; action) throws TransactionException &#123; if (this.transactionManager instanceof CallbackPreferringPlatformTransactionManager) &#123; return ((CallbackPreferringPlatformTransactionManager) this.transactionManager).execute(this, action); &#125; else &#123; TransactionStatus status = this.transactionManager.getTransaction(this); T result; try &#123; result = action.doInTransaction(status); &#125; catch (RuntimeException ex) &#123; // Transactional code threw application exception -&gt; rollback rollbackOnException(status, ex); throw ex; &#125; catch (Error err) &#123; // Transactional code threw error -&gt; rollback rollbackOnException(status, err); throw err; &#125; catch (Exception ex) &#123; // Transactional code threw unexpected exception -&gt; rollback rollbackOnException(status, ex); throw new UndeclaredThrowableException(ex, "TransactionCallback threw undeclared checked exception"); &#125; this.transactionManager.commit(status); return result; &#125; &#125; private void rollbackOnException(TransactionStatus status, Throwable ex) throws TransactionException &#123; logger.debug("Initiating transaction rollback on application exception", ex); try &#123; this.transactionManager.rollback(status); &#125; catch (TransactionSystemException ex2) &#123; logger.error("Application exception overridden by rollback exception", ex); ex2.initApplicationException(ex); throw ex2; &#125; catch (RuntimeException ex2) &#123; logger.error("Application exception overridden by rollback exception", ex); throw ex2; &#125; catch (Error err) &#123; logger.error("Application exception overridden by rollback error", ex); throw err; &#125; &#125;&#125; @Transactional 声明式事务理解Spring事务的核心对象和配合关系后，再来看Spring框架的声明式事务机制@Transactional就很简单了。 sprint-tx包中的ProxyTransactionManagementConfiguration会去配置关于@Transactional注解的处理机制。其中核心的是注册了TransactionInterceptor作为切面事务处理类 当执行声明式事务的代码块之前，会优先被TransactionInterceptor拦截，先执行TransactionInterceptor#invoke进行事务包围。 TransactionAspectSupport#invokeWithinTransaction 12345678910111213141516171819public class TransactionInterceptor extends TransactionAspectSupport implements MethodInterceptor, Serializable &#123; @Override public Object invoke(final MethodInvocation invocation) throws Throwable &#123; // Work out the target class: may be &#123;@code null&#125;. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. Class&lt;?&gt; targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // Adapt to TransactionAspectSupport's invokeWithinTransaction... return invokeWithinTransaction(invocation.getMethod(), targetClass, new InvocationCallback() &#123; @Override public Object proceedWithInvocation() throws Throwable &#123; return invocation.proceed(); &#125; &#125;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public abstract class TransactionAspectSupport implements BeanFactoryAware, InitializingBean &#123; //内部其他方法详见TransactionAspectSupport源码 protected Object invokeWithinTransaction(Method method, Class&lt;?&gt; targetClass, final InvocationCallback invocation) throws Throwable &#123; // If the transaction attribute is null, the method is non-transactional. final TransactionAttribute txAttr = getTransactionAttributeSource().getTransactionAttribute(method, targetClass); final PlatformTransactionManager tm = determineTransactionManager(txAttr); final String joinpointIdentification = methodIdentification(method, targetClass); if (txAttr == null || !(tm instanceof CallbackPreferringPlatformTransactionManager)) &#123; // Standard transaction demarcation with getTransaction and commit/rollback calls. TransactionInfo txInfo = createTransactionIfNecessary(tm, txAttr, joinpointIdentification); Object retVal = null; try &#123; // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. retVal = invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; // target invocation exception completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; commitTransactionAfterReturning(txInfo); return retVal; &#125; else &#123; // It's a CallbackPreferringPlatformTransactionManager: pass a TransactionCallback in. try &#123; Object result = ((CallbackPreferringPlatformTransactionManager) tm).execute(txAttr, new TransactionCallback&lt;Object&gt;() &#123; @Override public Object doInTransaction(TransactionStatus status) &#123; TransactionInfo txInfo = prepareTransactionInfo(tm, txAttr, joinpointIdentification, status); try &#123; return invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; if (txAttr.rollbackOn(ex)) &#123; // A RuntimeException: will lead to a rollback. if (ex instanceof RuntimeException) &#123; throw (RuntimeException) ex; &#125; else &#123; throw new ThrowableHolderException(ex); &#125; &#125; else &#123; // A normal return value: will lead to a commit. return new ThrowableHolder(ex); &#125; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; &#125; &#125;); // Check result: It might indicate a Throwable to rethrow. if (result instanceof ThrowableHolder) &#123; throw ((ThrowableHolder) result).getThrowable(); &#125; else &#123; return result; &#125; &#125; catch (ThrowableHolderException ex) &#123; throw ex.getCause(); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Spring</tag>
        <tag>数据库事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring数据库原理-DataSource]]></title>
    <url>%2F2018%2F08%2F11%2FSpring%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86-DataSource%2F</url>
    <content type="text"><![CDATA[DataSource引言用Spring进行Web应用开发时，我们经常会做datasource的配置。而且datasource的配法风格各异。那么他们到底有哪些异同点呢？ DataSource作用DataSource是javax.sql包中的类，是Java原生rt.jar包中的类。 1234567public interface DataSource extends CommonDataSource, Wrapper &#123; Connection getConnection() throws SQLException; Connection getConnection(String username, String password) throws SQLException;&#125; javax.sql.DataSource定义的是抽象方法，通过Java JNDI的方式将具体实现开放给各个厂商、组织自己、个人自己实现。 在Spring框架中，通过DataSource + 配置的方式，来定义具体的数据库源。并向Spring框架提供数据源的Connection服务。 在Spring中若想实现多数据源，那么就需要在DataSource下手。 javax.sql.DataSource源码注释说明1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * &lt;p&gt;A factory for connections to the physical data source that this * &#123;@code DataSource&#125; object represents. An alternative to the * &#123;@code DriverManager&#125; facility, a &#123;@code DataSource&#125; object * is the preferred means of getting a connection. An object that implements * the &#123;@code DataSource&#125; interface will typically be * registered with a naming service based on the * Java&amp;trade; Naming and Directory (JNDI) API. * &lt;P&gt; * The &#123;@code DataSource&#125; interface is implemented by a driver vendor. * There are three types of implementations: * &lt;OL&gt; * &lt;LI&gt;Basic implementation -- produces a standard &#123;@code Connection&#125; * object * &lt;LI&gt;Connection pooling implementation -- produces a &#123;@code Connection&#125; * object that will automatically participate in connection pooling. This * implementation works with a middle-tier connection pooling manager. * &lt;LI&gt;Distributed transaction implementation -- produces a * &#123;@code Connection&#125; object that may be used for distributed * transactions and almost always participates in connection pooling. * This implementation works with a middle-tier * transaction manager and almost always with a connection * pooling manager. * &lt;/OL&gt; * &lt;P&gt; * A &#123;@code DataSource&#125; object has properties that can be modified * when necessary. For example, if the data source is moved to a different * server, the property for the server can be changed. The benefit is that * because the data source&apos;s properties can be changed, any code accessing * that data source does not need to be changed. * &lt;P&gt; * A driver that is accessed via a &#123;@code DataSource&#125; object does not * register itself with the &#123;@code DriverManager&#125;. Rather, a * &#123;@code DataSource&#125; object is retrieved though a lookup operation * and then used to create a &#123;@code Connection&#125; object. With a basic * implementation, the connection obtained through a &#123;@code DataSource&#125; * object is identical to a connection obtained through the * &#123;@code DriverManager&#125; facility. * &lt;p&gt; * An implementation of &#123;@code DataSource&#125; must include a public no-arg * constructor. * * @since 1.4 */ 概要翻译 Part - 1: DataSource是获取物理数据源连接的工厂类。 作为DriverManager工具的替代方案，DataSource对象是获取连接的首选方法. DataSource的实现类一般都通过JNDI的方式注册到框架中进行使用。 Part - 2: DataSource一般由数据库厂商提供对应的实现类，DataSource有三种实现方式 基本实现，生成标准连接对象。 连接池实现，适用于中间层连接池管理器。 分布式事务实现。此实现适用于中间层事务管理器，并且几乎总是使用连接池管理器。 Part - 3: DataSource向Spring框架屏蔽了具体数据源的差异，即当物理数据源切换时，只需要更新相关的DataSource配置值即可，不需要应用层修改代码。 Part - 4: 数据库Driver都是通过DataSource对象被注册到DriverManager中，而不是由Driver直接向DriverManager注册。 但是对于获取Connection，先通过检索先获得DataSource，再根据DataSource对象进行getConnection，而不是直接从DriverManager获取Connection。 Spring-JDBC的DataSource实现案例 在 Spring-jdbc 下，DataSource 最顶级的类是 AbstractDataSource ，对 DataSource 的所有父接口方法做了实现。但保留 getConnection() 方法由子类实现。 在 AbstractDriverBasedDataSource 中，定义了大量的参数，诸如 url, username 等，这些都被用来定位并定义与数据库实例的连接。 1234567891011121314151617181920212223242526272829303132333435363738394041424344package org.springframework.jdbc.datasource;import java.sql.Connection;import java.sql.SQLException;import java.util.Properties;import org.springframework.lang.UsesJava7;import org.springframework.util.Assert;public abstract class AbstractDriverBasedDataSource extends AbstractDataSource &#123; private String url; private String username; private String password; private String catalog; private String schema; private Properties connectionProperties; public AbstractDriverBasedDataSource() &#123; &#125; ......略 public Connection getConnection() throws SQLException &#123; return this.getConnectionFromDriver(this.getUsername(), this.getPassword()); &#125; public Connection getConnection(String username, String password) throws SQLException &#123; return this.getConnectionFromDriver(username, password); &#125; @UsesJava7 protected Connection getConnectionFromDriver(String username, String password) throws SQLException &#123; Properties mergedProps = new Properties(); Properties connProps = this.getConnectionProperties(); if(connProps != null) &#123; mergedProps.putAll(connProps); &#125; ......略 return con; &#125; protected abstract Connection getConnectionFromDriver(Properties var1) throws SQLException;&#125; 整合方案为将除 url 外的所有参数整合在同一个 Properties 对象中 (其中，Properties 可以被认为是一个线程安全的 Hash Map) 。最终调用 Connection getConnectionFromDriver(Properties props) 获取连接。 AbstractDriverBasedDataSource 抽象类的两个子类 DriverManagerDataSource 和 SimpleDriverDataSource 都以不同方式获得了连接(Connection)，但总结而言，获取连接(Connection) 的任务被委托给了 Driver 来实现。 1234567891011121314151617181920212223242526272829303132333435// ----------------------------// SimpleDriverDataSource 的实现// ----------------------------@Overrideprotected Connection getConnectionFromDriver(Properties props) throws SQLException &#123; Driver driver = getDriver(); String url = getUrl(); Assert.notNull(driver, "Driver must not be null"); if (logger.isDebugEnabled()) &#123; logger.debug("Creating new JDBC Driver Connection to [" + url + "]"); &#125; return driver.connect(url, props);&#125;// -----------------------------// DriverManagerDataSource 的实现// -----------------------------@Overrideprotected Connection getConnectionFromDriver(Properties props) throws SQLException &#123; String url = getUrl(); Assert.state(url != null, "'url' not set"); if (logger.isDebugEnabled()) &#123; logger.debug("Creating new JDBC DriverManager Connection to [" + url + "]"); &#125; // 调了个内部函数 return getConnectionFromDriverManager(url, props);&#125;protected Connection getConnectionFromDriverManager(String url, Properties props) throws SQLException &#123; // 委托给 DriverManager 类来获取连接 // DriverManager 的主要操作是遍历在该管理类中注册的 Driver // 每个 Driver 实例都去尝试一下，能不能获得一个连接 // 第一次在某个 Driver 中拿到一个连接即返回连接 (Connection) return DriverManager.getConnection(url, props);&#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Spring</tag>
        <tag>DataSource</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Consumer启动]]></title>
    <url>%2F2018%2F06%2F02%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Consumer%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[Dubbo Consumer的启动过程和Provider一样，以DubboNamespaceHandler为起点，去解析代码配置中的ReferenceBean。 1public class ReferenceBean&lt;T&gt; extends ReferenceConfig&lt;T&gt; implements FactoryBean, ApplicationContextAware, InitializingBean, DisposableBean &#123; ReferenceBean同样既继承了ReferenceConfig，又实现了InitializingBean。也是在afterProperitesSet()中去执行服务引用 ReferenceBean afterPropertiesSet ReferenceConfig init() 完成service interface的class， methods解析 获取Service 注册中心registeries配置信息，用于向注册中西订阅service 检测是否配置有Dubbo Mock， Dubbo Stub createProxy()完成ReferenceConfig + Registeries ——》 Dubbo Service Invoker的转化。createProxy()返回时，返回的是被Proxy后的Invoker，即外层加了Dubbo Filter Chain。 DubboProtocol.refer(…) DubboProtocolDubboProtocol.class 作为Dubbo RPC层的具体实现协议，尤其完成Consumer中向注册中心真正订阅的动作。 1234567891011121314151617181920212223242526272829303132public class DubboProtocol extends AbstractProtocol &#123; ... public &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; serviceType, URL url) throws RpcException &#123; optimizeSerialization(url); // create rpc invoker with url. DubboInvoker&lt;T&gt; invoker = new DubboInvoker&lt;T&gt;(serviceType, url, getClients(url), invokers); invokers.add(invoker); return invoker; &#125; private ExchangeClient[] getClients(URL url) &#123; // whether to share connection boolean service_share_connect = false; int connections = url.getParameter(Constants.CONNECTIONS_KEY, 0); // if not configured, connection is shared, otherwise, one connection for one service if (connections == 0) &#123; service_share_connect = true; connections = 1; &#125; ExchangeClient[] clients = new ExchangeClient[connections]; for (int i = 0; i &lt; clients.length; i++) &#123; if (service_share_connect) &#123; clients[i] = getSharedClient(url); &#125; else &#123; clients[i] = initClient(url); &#125; &#125; return clients; &#125; ...&#125; Invoker12345678910111213141516171819public interface Invoker&lt;T&gt; extends Node &#123; /** * get service interface. * * @return service interface. */ Class&lt;T&gt; getInterface(); /** * invoke. * * @param invocation * @return result * @throws RpcException */ Result invoke(Invocation invocation) throws RpcException;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class DubboInvoker&lt;T&gt; extends AbstractInvoker&lt;T&gt; &#123; //与Service Provider端的连接 private final ExchangeClient[] clients; //同一service的invokers集合，集群时用到。 private final Set&lt;Invoker&lt;?&gt;&gt; invokers; ... public DubboInvoker(Class&lt;T&gt; serviceType, URL url, ExchangeClient[] clients, Set&lt;Invoker&lt;?&gt;&gt; invokers) &#123; super(serviceType, url, new String[]&#123;Constants.INTERFACE_KEY, Constants.GROUP_KEY, Constants.TOKEN_KEY, Constants.TIMEOUT_KEY&#125;); this.clients = clients; // get version. this.version = url.getParameter(Constants.VERSION_KEY, "0.0.0"); this.invokers = invokers; &#125; @Override protected Result doInvoke(final Invocation invocation) throws Throwable &#123; RpcInvocation inv = (RpcInvocation) invocation; final String methodName = RpcUtils.getMethodName(invocation); inv.setAttachment(Constants.PATH_KEY, getUrl().getPath()); inv.setAttachment(Constants.VERSION_KEY, version); ExchangeClient currentClient; if (clients.length == 1) &#123; currentClient = clients[0]; &#125; else &#123; currentClient = clients[index.getAndIncrement() % clients.length]; &#125; try &#123; boolean isAsync = RpcUtils.isAsync(getUrl(), invocation); boolean isOneway = RpcUtils.isOneway(getUrl(), invocation); int timeout = getUrl().getMethodParameter(methodName, Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT); if (isOneway) &#123; boolean isSent = getUrl().getMethodParameter(methodName, Constants.SENT_KEY, false); currentClient.send(inv, isSent); RpcContext.getContext().setFuture(null); return new RpcResult(); &#125; else if (isAsync) &#123; ResponseFuture future = currentClient.request(inv, timeout); RpcContext.getContext().setFuture(new FutureAdapter&lt;Object&gt;(future)); return new RpcResult(); &#125; else &#123; RpcContext.getContext().setFuture(null); return (Result) currentClient.request(inv, timeout).get(); &#125; &#125; catch (TimeoutException e) &#123; throw new RpcException(RpcException.TIMEOUT_EXCEPTION, "Invoke remote method timeout. method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125; catch (RemotingException e) &#123; throw new RpcException(RpcException.NETWORK_EXCEPTION, "Failed to invoke remote method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125; &#125; ...&#125; ReferenceConfig 核心数据12345678910111213141516171819202122232425262728public class ReferenceConfig&lt;T&gt; extends AbstractReferenceConfig &#123; //核心是DubboProtocol private static final Protocol refprotocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); //集群模式下使用，此处不解释 private static final Cluster cluster = ExtensionLoader.getExtensionLoader(Cluster.class).getAdaptiveExtension(); //注册中心地址 private final List&lt;URL&gt; urls = new ArrayList&lt;URL&gt;(); // interface name private String interfaceName; private Class&lt;?&gt; interfaceClass; // client type private String client; // url for peer-to-peer invocation private String url; // Service的方法列表 private List&lt;MethodConfig&gt; methods; // default config private ConsumerConfig consumer; private String protocol; // invoker 的代理类 private transient volatile T ref; //原生的service invoker private transient volatile Invoker&lt;?&gt; invoker; private transient volatile boolean initialized; private transient volatile boolean destroyed;&#125; ConsumerModel经过ReferenceConfig一番处理后，最终会得到：Reference Dubbo Service Name, InvokerRef, Service Methods, ReferenceConfig Instance。 这些信息会封装成ConsumerModel，放到ApplicationModel.class中去全局统一记录Consumer的情况。 1234567public class ConsumerModel &#123; private ReferenceConfig metadata; private Object proxyObject; private String serviceName; private final Map&lt;Method, ConsumerMethodModel&gt; methodModels = new IdentityHashMap&lt;Method, ConsumerMethodModel&gt;(); ...&#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[翻译] REST API必须是超文本驱动的]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%BF%BB%E8%AF%91-REST-API%E5%BF%85%E9%A1%BB%E6%98%AF%E8%B6%85%E6%96%87%E6%9C%AC%E9%A9%B1%E5%8A%A8%E7%9A%84%2F</url>
    <content type="text"><![CDATA[原文地址：Roy T. Fielding: REST APIs must be hypertext-driven I am getting frustrated by the number of people calling any HTTP-based interface a REST API. Today’s example is the SocialSite REST API. That is RPC. It screams RPC. There is so much coupling on display that it should be given an X rating. 我是越来越失望了，许多人把任何基于HTTP的接口叫做REST API，眼前的例子就是SocialSite REST API。那是RPC，实实在在的RPC。它与显示如此耦合，再差也莫过于此 What needs to be done to make the REST architectural style clear on the notion that hypertext is a constraint? In other words, if the engine of application state (and hence the API) is not being driven by hypertext, then it cannot be RESTful and cannot be a REST API. Period. Is there some broken manual somewhere that needs to be fixed? 基于超文本概念，如何才能确保清晰的REST架构风格呢？这样来说吧，如果应用程序状态引擎（即API）不是由超文本驱动的，那就不是RESTful也不是REST的API。就这么简单。某些REST方面的破手册是否该修正一下呢？ API designers, please note the following rules before calling your creation a REST API: API的设计者们，把你们的那些东西叫做REST API前请注意以下的规则： A REST API should not be dependent on any single communication protocol, though its successful mapping to a given protocol may be dependent on the availability of metadata, choice of methods, etc. In general, any protocol element that uses a URI for identification must allow any URI scheme to be used for the sake of that identification. [Failure here implies that identification is not separated from interaction.] REST API不应依赖于任何特定的通讯协议，在采用某个具体协议时可能受限于元数据的有效性、方法的选择等。通常，协议元素使用URI作标识时，对该标识必须允许运用任何URI方案。[ 不符合这一点意味着标识与交互没有分离 ] A REST API should not contain any changes to the communication protocols aside from filling-out or fixing the details of underspecified bits of standard protocols, such as HTTP’s PATCH method or Link header field. Workarounds for broken implementations (such as those browsers stupid enough to believe that HTML defines HTTP’s method set) should be defined separately, or at least in appendices, with an expectation that the workaround will eventually be obsolete. [Failure here implies that the resource interfaces are object-specific, not generic.] REST API不应修改通讯协议中预留出来作为补充或修正标准协议用途的资源，例如HTTP的PATCH方法和Link head域。违背了这一原则的方案应当单独定义，或者至少在附录中标注出来这样的方案最终会废弃掉。[ 不符合这一点意味着资源接口是对象相关的，不通用 ] A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state, or in defining extended relation names and/or hypertext-enabled mark-up for existing standard media types. Any effort spent describing what methods to use on what URIs of interest should be entirely defined within the scope of the processing rules for a media type (and, in most cases, already defined by existing media types). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] REST API应当将绝大部分精力放在媒体类型的定义上，或者是扩展关系名称的定义、已有超文本标记中的标准媒体类型等方面，以实现资源的表述、操作应用程序状态。任何类似于对某某URI应当使用什么样的方法等工作，都应当完全定义在特定媒体类型的处理规则范围中（绝大部分情况下已有媒体类型都已经定义好了这些规则）。[ 不符合这一点意味着交互是由其它信息驱动，而不是超文本 ] A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. [Failure here implies that clients are assuming a resource structure due to out-of band information, such as a domain-specific standard, which is the data-oriented equivalent to RPC’s functional coupling]. REST API决不能定义固定的资源名称或者层次关系（这是明显的客户端、服务器端耦合），服务器必须可以自由控制自己的名称空间。应当像HTML forms和URI模板一样，通过媒体类型和链接关系指示客户端如何构造正确的URI。[ 不符合这一点意味着客户端在通过其它信息（例如领域相关标准）猜测资源结构，这是数据导向，类似于RPC的函数耦合 ] A REST API should never have “typed” resources that are significant to the client. Specification authors may use resource types for describing server implementation behind the interface, but those types must be irrelevant and invisible to the client. The only types that are significant to a client are the current representation’s media type and standardized relation names. [ditto] REST API决不能使用对客户端有重要意义的类型化资源。规范的作者可能使用资源类型描述接口背后的服务器端实现，但这些类型必须与客户端无关，对客户端不可见。对客户端唯一有意义的类型是当前的表述性媒体类型和标准的关系名称。[ 同上 ] A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user’s manipulation of those representations. The transitions may be determined (or limited by) the client’s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] 使用REST API应该只需要知道初始URI（书签）和一系列针对目标用户的标准媒体类型（任何客户端都了解用来操作该媒体类型的API）。这样所有的应用程序状态转换都通过这样的方式进行：服务器在返回的表述性消息中提供选项，由客户端进行选择，或者是伴随着用户对表述性内容的操作而进行。状态转换由客户端对媒体类型的了解程度和资源通讯机制决定，或者受限于这些因素，这些问题都可以根据实际情况得以改善的（例如使用javascript这种code-on-demand技术）。[ 不符合这一点意味着交互是由其它信息驱动，而不是超文本 ] There are probably other rules that I am forgetting, but the above are the rules related to the hypertext constraint that are most often violated within so-called REST APIs. Please try to adhere to them or choose some other buzzword for your API. 也许还有其它一些规则我一时想不起来了，但在那些所谓的REST API中通常都违背了上面这些超文本约束相关的规则，请纠正这些错误或者改用其它称谓吧]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>REST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API命名实践]]></title>
    <url>%2F2018%2F05%2F18%2FRESTful-API%E5%91%BD%E5%90%8D%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[引言在互联网高度普及的今天，作为一名Web开发者，如果你还没听说过“REST”这个技术名词，出门都不好意思跟人打招呼。尽管如此，对于REST这个泊来品的理解，大多数人仍然停留在“盲人摸象”的阶段。 有人认为，在Web Controller层写的API就是REST API。而且，从开发角度对于URI的命名、HTTP Mehthod的选择没有建立起规范的意识。这样是不优雅的！（没有对错之分） 作为带着问题学习总结的我，未打算通过本篇文档全面的阐述清楚REST，而是尽量的总结一些理论和思考，一起探讨！ REST 的诞生Web 技术发展Web开发技术的发展可以粗略划分成以下几个阶段： 静态内容阶段：在这个最初的阶段，使用Web的主要是一些研究机构。Web由大量的静态HTML文档组成，其中大多是一些学术论文。Web服务器可以被看作是支持超文本的共享文件服务器。 可以想象下当时的HTTP请求只有“GET”，且MIME为“HTML或TEXT” CGI程序阶段：在这个阶段，Web服务器增加了一些编程API。通过这些API编写的应用程序，可以向客户端提供一些动态变化的内容。Web服务器与应用程序之间的通信，通过CGI（Common Gateway Interface）协议完成，应用程序被称作CGI程序。 脚本语言阶段：在这个阶段，服务器端出现了ASP、PHP、JSP、ColdFusion等支持session的脚本语言技术，浏览器端出现了Java Applet、JavaScript等技术。使用这些技术，可以提供更加丰富的动态内容。 瘦客户端应用阶段：在这个阶段，在服务器端出现了独立于Web服务器的应用服务器。同时出现了Web MVC开发模式，各种Web MVC开发框架逐渐流行，并且占据了统治地位。基于这些框架开发的Web应用，通常都是瘦客户端应用，因为它们是在服务器端生成全部的动态内容。 RIA应用阶段：在这个阶段，出现了多种RIA（Rich Internet Application）技术，大幅改善了Web应用的用户体验。应用最为广泛的RIA技术是DHTML+Ajax。Ajax技术支持在不刷新页面的情况下动态更新页面中的局部内容。同时诞生了大量的Web前端DHTML开发库，例如Prototype、Dojo、ExtJS、jQuery/jQuery UI等等，很多开发库都支持单页面应用（Single Page Application）的开发。其他的RIA技术还有Adobe公司的Flex、微软公司的Silverlight、Sun公司的JavaFX（现在为Oracle公司所有）等等。 移动Web应用阶段：在这个阶段，出现了大量面向移动设备的Web应用开发技术。除了Android、iOS、Windows Phone等操作系统平台原生的开发技术之外，基于HTML5的开发技术也变得非常流行。 REST 的诞生从上述Web开发技术的发展过程看，Web从最初其设计者所构思的主要支持静态文档的阶段，逐渐变得越来越动态化。Web应用的交互模式，变得越来越复杂：从静态文档发展到以内容为主的门户网站、电子商务网站、搜索引擎、社交网站，再到以娱乐为主的大型多人在线游戏、手机游戏。 Web发展到了1995年，在CGI、ASP等技术出现之后，沿用了多年、主要面向静态文档的HTTP/1.0协议已经无法满足Web应用的开发需求，因此需要设计新版本的HTTP协议。在HTTP/1.0协议专家组之中，有一位年轻人脱颖而出，显示出了不凡的洞察力，后来他成为了HTTP/1.1协议专家组的负责人。这位年轻人就是Apache HTTP服务器的核心开发者Roy Fielding，他还是Apache软件基金会的合作创始人。 所以，REST 并不是在互联网诞生之初就有的，它是在HTTP/1.1协议中才出现的，由Roy Thomas Fielding这位大神对Web技术做了深入的总结和分析，提出的一套网络软件的架构风格理论框架，当时Fielding为这种架构风格取了一个轻松愉快的名字：“REST” ———— Representational State Transfer（表述性状态转移） Roy Thomas Fielding 关于REST的论文 https://www.ics.uci.edu/~fielding/pubs/dissertation/fielding_dissertation.pdf REST 详解REST 架构风格问题：REST 究竟是什么？是一种新的技术、一种新的架构、还是一种新的规范？ 首先，REST是Web自身的架构风格，也是世界上最成功的分布式应用架构风格。它是为运行在互联网环境的分布式超媒体系统量身定制的。 REST是一种架构风格！ REST是一种架构风格！ REST是一种架构风格！ 所以，就会存在实际开发工作中即使没有正确的理解和应用REST，但也能顺利的完成开发工作。也正因为如此，给开发工作中推广正确实践和统一风格带来不小的困难。因为大多数程序员总是在寻找最快解决问题，最快完成需求的方式，怎么简单怎么来。 解读 RESTREST ———— Representational State Transfer (表现层状态转化) 从“Representational State Transfer”这个定义去理解REST架构风格原则。 1. 资源（Resources）REST 的名称“表现层状态转化”中，省略了主语。“表现层”其实指的是“资源（Resources）”的“表现层” 资源是一种看待服务器的方式，此处指的“资源”是一个抽象的概念，它不仅仅指服务器端真实存在的文件、数据库表，而是指任何可被名词表述的东西。所以在定义“资源”时可以要多抽象就多抽象。 对于客户端，可以将服务器端看作是由很多离散的资源组成。服务端可以用URI（统一资源定位符）指向资源，每种资源都对应一个特定的URI。要向获取这个资源，访问它的URI就可以了，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓“上网”，就是与互联网上一系列的“资源”互动，调用它的URI。 2. 表现层（Representation）“资源”是一种信息实体，它可以有多在的表现形式。我们把“资源”具体呈现出来的形式，叫做它的“表现层（Representation）” 比如，文本信息可以用txt格式表现，也可以用HTML格式 、XML格式、JSON格式表现，甚至可以用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的表现形式。资源的具体表现形式，应该在HTTP请求的的头部信息中用Accept和Content-Type字段指明，这两个字段才是对“表现层”的描述。 详见HTTP MIME明细 3. 状态转化（State Transfer）HTTP协议是一个无状态的协议，这意味着所有资源的状态都保存在服务器端。因为客户端想要操作服务器，必须通过某种手段，让服务器端资源发生“状态转化”。而这种转化是建立在表现层之上的，所以就是“表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体对应HTTP协议中的HTTP Method：GET、POST、PUT、PATCH、DELETE、HEAD、OPTIONS。每一种HTTP Method代表资源状态转化的一种约定的方式。 HTTP 动词 对于资源的具体操作类型，有HTTP动词表示。 常用的HTTP动词如下： 123456789- GET : 从服务器取出资源（一个或多个）- POST : 在服务器新建一个资源，并返回创建后的完整资源到客户端- PUT : 在服务器以覆盖形式，全量更新资源，并返回更新后的完整资源到客户端- PATCH : 在服务器端更新资源，但只更新指定的内容- DELETE : 在服务器端删除资源 其中，GET、PUT、PATCH、DELETE都应该是幂等的。 另外，HEAD、OPTIONS对于团队开发来说基本不用。 123- HEAD : 获取资源的元数据- OPTIONS : 获取信息，关于资源的哪些属性是客户端可以改变的 4. 综述综合上面的解读，总结一下什么是REST架构风格： (1) 服务器端的任何信息和数据都要被抽象资源化；(2) 资源用URI进行表述，每一个URI代表一种资源；(3) 客户端与服务器之间，基于某种表现层形式，互相传递资源；(4) 客户端与服务器之间，基于HTTP Method对服务器端资源的操作，实现“表现层状态转化”； REST 与 RESTful定义： 如果一个架构符合REST原则，就称它为RESTful架构 如果HTTP API的设计符合REST原则，那么可称它为RESTful API 所以，回到开篇讲的大多数人对于REST还是处于“盲人摸象”的阶段，回想下自己和身边的同事，在工作中经常交流到的REST API或RESTful API，其实只能算个HTTP API吧？ REST 风格优点架构风格不是非此即彼的是非题，在实际开发中可以自主的选择是否应用REST风格。那么，如果应用REST风格会带来哪些优势呢？ 从面向操作编程，转变为面向资源编程。更面向对象，架构更清晰、松耦合。 我们应该确定的认为系统由“资源+对资源的操作”组成，而不是由“操作”组成 面向操作编程会导致API膨胀，功能重复度高。 统一URI命名风格，URI具备很强的可读性，具备自解释的能力。服务器资源层次目录清晰。 状态无关。确保系统横向扩展的能力。 超文本驱动。确保系统演化的能力。 REST实践体会1. URI命名难度变大在没有要求URI必须用资源名词来组成URI时，URI的命名从来不是什么难事，常见的命名风格有： 动词+名词 /deposit/getUsers: 获取某个项目保证金用户列表 /orders/submitAudit: 订单提交审核 /cart/add: 商品加购物车 URI全局唯一即可 /finance/budget/getPurchaseplanNextAuditOrgList：我有点小无语… 为什么会这样： 我们平时搞系统是这样的： 有新建用户功能 新建用户需要一个URL 往这个URL发送的数据要定义好 开始写后端和前端 这是以操作为第一位的设计方法，首先确认了一个操作，然后围绕这个操作把周边需要的东西建设好，这种方式当然可以架构出一个系统，甚至是一个好系统，但是偶尔会有些问题： 操作之间是会有关联，你的设计容易变成“第2个操作要求第1个操作进行过”，这种关系多起来你的系统就乱了 你的URL设计会缺乏一致性 操作通常被认为是有副作用（Side Effect）的，所以很少有人基于操作去设计缓存之类的东西 该怎么应对？ 确实，REST是高度抽象的理论和风格，在实际开发中会面对各种复杂的功能和场景，导致很难完全的应用REST风格。当我们在争论REST风格到底如何设计才是正宗时，发现心中的困惑不仅没有降低，反而增加了。 我的想法：仍以真正的系统需求为出发点，使用REST风格让系统的架构更清晰，让系统的开发协作更高效。部分不适合REST的场景应该灵活变通。 回到URI的命名： 坚持URI仍以资源为导向，清晰的表述服务器端资源目录 保障URI资源层次清晰的情况下，只允许在URI最末一级添加动词，例如：/market/orders/1/audit 如果某些动作是HTTP动词表示不了的，考虑把动作抽象成一种资源 比如：网上汇款，从账户1向账户2汇款100元，错误的URI 1POST /accounts/1/transfer/500/to/2 正确的写法是把动词transfer改成名词transaction 1POST /transaction?from=1&amp;to=2&amp;amount=100 2. 用不用HTTP PATCHPATCH 作为HTTP的Method之一，其实它是2010年3月份才正式成为HTTP Method的，详见：RFC 5789 也正因为PATCH出现的晚, 所以并不是所有Web容器都支持，反而目前实现了PATCH方法的Web容器很少 几个常见Web容器实现PATCH方法的情况，供参考： Apache HttpComponents HttpClient version 4.2 or later 支持了 PATCH 目前 JDK7 的 HttpURLConnection 未实现 PATCH TOMCAT 7 也不行 PlayFramework 2 也不支持 Spring 3.2 开始支持 PATCH 方法，但要选对部署的容器 JBoss Netty 支持 PATCH，可见： http://docs.jboss.org/netty/3.2/api/org/jboss/netty/handler/codec/http/class-use/HttpMethod.html]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>RESTful</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引合并]]></title>
    <url>%2F2018%2F05%2F06%2FMySQL%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[历史背景MySQL 5.0版本之前，一个表一次只能选择并使用一个索引。 MySQL 5.1版本开始，引入了Index Merge Optimization技术，使得MySQL支持一个表一次查询同时使用多个索引。 官方文档：MySQL Index Merge Optimization Index Merge Optimization支持三种合并算法 The Index Merge Intersection Access Algorithm 对应SQL 中的 AND 场景 The Index Merge Union Access Algorithm 对应SQL中的 OR 场景（where条件是等值判断） The Index Merge Sort-Union Access Algorithm 对应SQL中的 OR 场景（where条件是范围查询） 注：索引合并(Index Merge)的使用取决于optimizer_switch系统变量的index_merge，index_merge_intersection，index_merge_union和index_merge_sort_union标志的值。默认情况下，所有这些标志都打开。 要仅启用特定算法，请将index_merge设置为关闭，并仅启用其他应允许的其他算法。 ##关于”Index Merge Intersection Access Algorithm”的疑问 针对 MySQL Index Merge Optimization Intersection Algorithm AND 场景的 index merge optimization为什么会比使用单个索引来的高效？ 设想： 使用单个索引的场景 选中选择性高的索引先获得一份数据 在再mysql服务器端用using where的方式，按第二条件进行过滤，得到最终满足所有条件的数据行。 同时使用表内多个索引的场景 按每个索引，在索引树里拿只满足本索引条件的行数据 将两份行数据，放一块进行交集运算。 从索引的次数、磁盘IO、内存交接运算来看，事情没变少、反而变多了。 自我初版解释合理的解释样例SQL1select * from table_sample where column_1 = A AND column_2 = B; 前提条件，SQL中不能有范围查询，如果存在范围查询，数据库优化器默认使用单索引方式，不用index merge optimization SQL的WHERE从句中的所有条件字段都有对应的索引，否则问题就来了，肯定会在内存中有次using_where的。 单表多Index并行检索时，拿到的是数据行地址，以上述SQL为例，即拿到了两份行数据地址：Index Column_1的行数据地址集，Index Column_2的行数据地址集 再在内存中完成两份行数据地址集的交集运算（只需要比地址） 此时，再决定是否回表拿更多的数据。 如果字段中有primary key，就不用回表啦！ 如上的执行步骤，就会比较合理。有效率上的优势。 【更进一步】 explain 显示type 为 index_merge时，到底要不要引起关注？【需要引起注意】 拿着SQL琢磨下，是否还有优化的空间，例如：采用组合索引；强制走单索引（需要对比测试看效果，还要看业务数据场景和增长趋势）； 注： 当索引本身信息可以覆盖select的字段时（或是select count(*)）,效率会很高，因为内存索引里已经能提供返回的数据了，不用回表。 当索引本身信息不能覆盖select的字段时，就要回表查行数据了，性能差别很大。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL子查询很慢的问题分析]]></title>
    <url>%2F2018%2F05%2F04%2FMySQL%E5%AD%90%E6%9F%A5%E8%AF%A2%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[慢查询案例1DELETE FROM settlement_invoice_attachment g1 WHERE demand_id in (SELECT id FROM settlement_invoice_demand g2 WHERE statement_id = 1802065000000074956) 乍眼一看，上述SQL如此简单，且demand_id和statement_id字段都是建了索引，即使是Review也会认为是OK没问题的。 然而，实际情况却是个慢查询，情况如下： explain明细 settlement_invoice_attachment是全表查 注：rows 2689 是因为用的测试环境，真线环境数据是几十万级别 子查询 原理分析(上述SQL子查询为什么这么慢)经验之谈 当看到SQL执行计划中select_type字段出现“DEPENDENT SUBQUERY”的时候，要打起精神了！着重分析下潜在风险！ 基础知识：Dependent SubQuery意味着什么？ 官方含义为： SUBQUERY: 子查询中的第一个SELECT； DEPENDENT SUBQUERY: 子查询中的第一个SELECT， 取决于外面的查询。 换句话说，就是子查询的g2查询执行方式依赖于外层g1的查询结果什么意思呢？它以为着两步走： 第一步：【先执行外部SQL查询】MySQL根据”DELETE FROM settlement_invoice_attachment g1 WHERE” 得到一个大结果集t1，其数据量就是全表所有行了，假设是85万行。 第二步：【后执行内部SQL子查询】第一步的大结果集t1中的每一条记录，都将与子查询SQL组成新的查询语句：SELECT id FROM settlement_invoice_demand g2 WHERE statement_id = 1802065000000074956 AND id = %t1.demand_id%。等于说，子查询要执行85万次……即使这两部查询都用到了索引，也是巨慢的。 优化策略 改写SQL为JOIN的方式 12DELETE ah FROM settlement_invoice_attachment ah INNER JOIN settlement_invoice_demand de ON ah.demand_id = de.id WHERE de.statement_id = 1802065000000074956; 拆成独立SQL多次执行 平时怎么识别？ 看子查询出现的位置 若子查询出现在WHERE从句中，而且是出现在IN（）中，则需要引起注意，用Explain瞧瞧（并不是子查询放IN（）里就一定是全表扫，本案例用，将DELETE改成SELECT就不是DEPENDENT SUBQUERY） 数据库原理 MySQL处理子查询时，会(优化)改写子查询，但优化的不是很友好，一直受业界批评比较多 有时候优化的挺糟糕的，特别是WHERE从句中的IN（）子查询 MySQL 子查询的弱点 mysql 在处理子查询时，会改写子查询。通常情况下，我们希望由内到外，先完成子查询的结果，然后再用子查询来驱动外查询的表，完成查询。 例如：select * from test where tid in(select fk_tid from sub_test where gid=10)通常我们会感性地认为该 sql 的执行顺序是： 1、sub_test 表中根据 gid 取得 fk_tid(2,3,4,5,6)记录。2、然后再到 test 中，带入 tid=2,3,4,5,6，取得查询数据。 但是实际mysql的处理方式为：select from test where exists (select from sub_test where gid=10 and sub_test.fk_tid=test.tid)mysql 将会扫描 test 中所有数据，每条数据都将会传到子查询中与 sub_test 关联，子查询不会先被执行，所以如果 test 表很大的话，那么性能上将会出现问题。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Provider暴露服务]]></title>
    <url>%2F2018%2F05%2F02%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Provider%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前言Dubbo Provider暴露服务的流程中，需要掌握几个核心抽象对象 过程中的重要类 ServiceConfig：记录了Dubbo Service所有相关的配置信息。ServiceConfig作用 DubboProtocol：以Dubbo协议的方式暴露服务，并以此为中心维护所有相关的动态服务数据。 RegisterProtocol: 内部会加载具体的注册中心Register,例如：ZookeeperRegister。完成服务向注册中心注册的动作。 ServiceConfig#loadRegistries：解析获得注册中心地址列表 过程中的重要对象 com.alibaba.dubbo.common.URL: 服务发布的地址 Invoker: 对原Service Interface进行了代理封装，屏蔽了具体Service Interface的差异，方便统一管理和调用。 Exporter： 一个ServiceBean每向一个注册中心Register注册一次，就会生成已各Exporter。Exporter用于连接暴露服务的Url与本地Invoker的对应关系。 ExporterMap: 记录着服务地址和Exporter的对应关系 来自Dubbo官方的几个架构设计图，先感觉下 ServiceBean核心流程 Spring容器启动，带动Dubbo Bean配置解析以及Bean实例化。 Dubbo启动 关键类： DubboNamespaceHandler ServiceBean ServiceConfig作用 ServiceBean 继承了ServiceConfig，所有的Provider服务的Dubbo配置都在ServiceConfig中。 Dubbo Service基本信息 Dubbo Service参数配置 注册中心地址信息。对应ServiceConfig中的loadRegistries(). ServiceBean 实现了InitializingBean, 实现了afterPropertiesSet()方法，在每个Dubbo Service Bean实例化后，在afterPropertiesSet()方法中进行所有Dubbo服务注册需要的操作。 afterPropertiesSet()中前置代码都是在做一些配置校验和默认值设置，最后会执行export()方法注册暴露服务。 afterPropertiesSet() export() doExport() doExportUrls() doExportUrlsFor1Protocol(DubboProtocol, regitsryURLs) DubboProtocol.export(wrapperInvoker) doExportUrlsFor1Protocol(ProtocolConfig protocolConfig, List registryURLs) 是真正执行export暴露服务的代码区 DubboProtocol#Export核心流程1234567891011121314151617181920212223242526272829public &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException &#123; URL url = invoker.getUrl(); // export service. String key = serviceKey(url); DubboExporter&lt;T&gt; exporter = new DubboExporter&lt;T&gt;(invoker, key, exporterMap); exporterMap.put(key, exporter); //export an stub service for dispatching event Boolean isStubSupportEvent = url.getParameter(Constants.STUB_EVENT_KEY, Constants.DEFAULT_STUB_EVENT); Boolean isCallbackservice = url.getParameter(Constants.IS_CALLBACK_SERVICE, false); if (isStubSupportEvent &amp;&amp; !isCallbackservice) &#123; String stubServiceMethods = url.getParameter(Constants.STUB_EVENT_METHODS_KEY); if (stubServiceMethods == null || stubServiceMethods.length() == 0) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(new IllegalStateException("consumer [" + url.getParameter(Constants.INTERFACE_KEY) + "], has set stubproxy support event ,but no stub methods founded.")); &#125; &#125; else &#123; stubServiceMethodsMap.put(url.getServiceKey(), stubServiceMethods); &#125; &#125; //获取一个服务端口，使用NettyServer绑定并监听，并设置Server监听事件处理回调为：DubboProtocol#requestHandler //Exchanger.bind的实际对象可配置，对应dubbo-remoting-api包 openServer(url); optimizeSerialization(url); return exporter;&#125; DubboProtocol核心数据123456789101112131415161718192021222324252627public class DubboProtocol extends AbstractProtocol &#123; ... //单例 private static DubboProtocol INSTANCE; //本地启动Server监听服务的Map private final Map&lt;String, ExchangeServer&gt; serverMap = new ConcurrentHashMap&lt;String, ExchangeServer&gt;(); // &lt;host:port,Exchanger&gt; //记录消费端的Exchanger private final Map&lt;String, ReferenceCountExchangeClient&gt; referenceClientMap = new ConcurrentHashMap&lt;String, ReferenceCountExchangeClient&gt;(); // &lt;host:port,Exchanger&gt; // private final ConcurrentMap&lt;String, LazyConnectExchangeClient&gt; ghostClientMap = new ConcurrentHashMap&lt;String, LazyConnectExchangeClient&gt;(); // private final Set&lt;String&gt; optimizers = new ConcurrentHashSet&lt;String&gt;(); //consumer side export a stub service for dispatching event //servicekey-stubmethods private final ConcurrentMap&lt;String, String&gt; stubServiceMethodsMap = new ConcurrentHashMap&lt;String, String&gt;(); private ExchangeHandler requestHandler = &#123;...&#125;; ...&#125; 12345public abstract class AbstractProtocol implements Protocol &#123; // ExporterMap protected final Map&lt;String, Exporter&lt;?&gt;&gt; exporterMap = new ConcurrentHashMap&lt;String, Exporter&lt;?&gt;&gt;();&#125; Dubbo Service是哪个时机注册到注册中心的？ 有关注到这个章节内容的小伙伴，说明你此时可能也还没想通吧，请听我道来。 这里会涉及到Dubbo的SPI机制，Dubbo 有好几个利用SPI+动态代理+Filter的处理责任链模式，ProtocolFilterWrapper.java算一个。 - Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); 在Dubbo源码中，Dubbo有自行注册几个protocol SPI （这里只列举Dubbo服务注册相关的） SPI机制用法详见 Dubbo Protocol SPI扩展详见 RegistryProtocol SPI: 注册位置：dubbo-registry-api包,resources下的com.alibaba.dubbo.rpc.Protocol 注册位置：dubbo-registry-zookeeper包,resources下的com.alibaba.dubbo.register.RegistryFactory 其实，在ServiceConfig中拿到的全局protocol并不直接是DubboProtocol，而是一串Protocol，DubboProtocol只是其中之一，这些Protocol会以责任链的方式逐一被调用 所以，在doExportUrlsFor1Protocol中protocol.export(…)时，会先执行DubboProtocol#export,再执行RegisterProtocol#export,各司其职。 RegisterProtocol中会根据Dubbo Service配置的register地址类型来决定加载哪个具体的RegisterFactory 123456789101112131415161718public void register(URL registryUrl, URL registedProviderUrl) &#123; //RegisterFactory根据注册中心类型，获取到注册实例，例如ZookeeperRegistry Registry registry = registryFactory.getRegistry(registryUrl); //执行注册，实际对应ZookeeperRegistry#register registry.register(registedProviderUrl); &#125; public &lt;T&gt; Exporter&lt;T&gt; export(final Invoker&lt;T&gt; originInvoker) throws RpcException &#123; ... if (register) &#123; register(registryUrl, registedProviderUrl); ProviderConsumerRegTable.getProviderWrapper(originInvoker).setReg(true); &#125; ... &#125; Netty Server当DubboProtocol.export.openServer()时，就是在本地启动Dubbo Service的Server服务并启动监听。 实现上是通过Exchanger拿到被配置的信息交换层的实现套件（一般是Netty）。 - 获取一个服务端口，使用NettyServer绑定并监听，并设置Server监听事件处理回调为：DubboProtocol#requestHandler - Exchanger.bind的实际对象可配置，对应dubbo-remoting-api包 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class NettyServer extends AbstractServer implements Server &#123; private static final Logger logger = LoggerFactory.getLogger(NettyServer.class); private Map&lt;String, Channel&gt; channels; // &lt;ip:port, channel&gt; private ServerBootstrap bootstrap; private org.jboss.netty.channel.Channel channel; public NettyServer(URL url, ChannelHandler handler); @Override protected void doOpen() throws Throwable &#123; NettyHelper.setNettyLoggerFactory(); ExecutorService boss = Executors.newCachedThreadPool(new NamedThreadFactory("NettyServerBoss", true)); ExecutorService worker = Executors.newCachedThreadPool(new NamedThreadFactory("NettyServerWorker", true)); ChannelFactory channelFactory = new NioServerSocketChannelFactory(boss, worker, getUrl().getPositiveParameter(Constants.IO_THREADS_KEY, Constants.DEFAULT_IO_THREADS)); bootstrap = new ServerBootstrap(channelFactory); final NettyHandler nettyHandler = new NettyHandler(getUrl(), this); channels = nettyHandler.getChannels(); // https://issues.jboss.org/browse/NETTY-365 // https://issues.jboss.org/browse/NETTY-379 // final Timer timer = new HashedWheelTimer(new NamedThreadFactory("NettyIdleTimer", true)); bootstrap.setPipelineFactory(new ChannelPipelineFactory() &#123; public ChannelPipeline getPipeline() &#123; NettyCodecAdapter adapter = new NettyCodecAdapter(getCodec(), getUrl(), NettyServer.this); ChannelPipeline pipeline = Channels.pipeline(); /*int idleTimeout = getIdleTimeout(); if (idleTimeout &gt; 10000) &#123; pipeline.addLast("timer", new IdleStateHandler(timer, idleTimeout / 1000, 0, 0)); &#125;*/ pipeline.addLast("decoder", adapter.getDecoder()); pipeline.addLast("encoder", adapter.getEncoder()); pipeline.addLast("handler", nettyHandler); return pipeline; &#125; &#125;); // bind channel = bootstrap.bind(getBindAddress()); &#125; @Override protected void doClose(); public Collection&lt;Channel&gt; getChannels(); public Channel getChannel(InetSocketAddress remoteAddress); public boolean isBound();&#125; ServiceConfig作用(见代码注释)12345678910111213141516171819202122232425262728293031323334public class ServiceConfig&lt;T&gt; extends AbstractServiceConfig &#123; ... // 采用的protocol远程调用层实现，用于封装RPC调用，默认是DubboProtocol，其余可选还有HttpProtocol,HessianProtocol,InjvmProtocol,RedisProtocol等 private static final Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); //对ServiceBean进行代理，包装成Dubbo内部通用的Invoker private static final ProxyFactory proxyFactory = ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension(); //ServiceBean作为Dubbo Provider启动时，会在本地起server服务，每个server服务都会绑定并监听端口。 private static final Map&lt;String, Integer&gt; RANDOM_PORT_MAP = new HashMap&lt;String, Integer&gt;(); //记录已暴露服务的服务地址 private final List&lt;URL&gt; urls = new ArrayList&lt;URL&gt;(); //一个ServiceBean每向一个注册中心Register注册一次，就会生成已各Exporter。Exporter用于连接暴露服务的Url与本地Invoker的对应关系。 private final List&lt;Exporter&lt;?&gt;&gt; exporters = new ArrayList&lt;Exporter&lt;?&gt;&gt;(); //关于本ServiceBean的Java Class信息 private String interfaceName; private Class&lt;?&gt; interfaceClass; // reference to interface impl private T ref; // service name private String path; // method configuration private List&lt;MethodConfig&gt; methods; private ProviderConfig provider; private transient volatile boolean exported; private transient volatile boolean unexported; private volatile String generic; Dubbo 启动Spring容器启动，带动Dubbo Bean配置实例化。Dubbo Bean配置来自于Dubbo Provider XML 文件。 1234567891011121314151617181920public class DubboNamespaceHandler extends NamespaceHandlerSupport &#123; static &#123; Version.checkDuplicate(DubboNamespaceHandler.class); &#125; public void init() &#123; registerBeanDefinitionParser("application", new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser("module", new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser("registry", new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser("monitor", new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser("provider", new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser("consumer", new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser("protocol", new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser("service", new DubboBeanDefinitionParser(ServiceBean.class, true)); //dubbo provider bean配置解析 registerBeanDefinitionParser("reference", new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser("annotation", new AnnotationBeanDefinitionParser()); &#125;&#125; 具体详见笔记：Dubbo源码解析-Spring Bean注册 ServiceBean实例化123public class ServiceBean&lt;T&gt; extends ServiceConfig&lt;T&gt; implements InitializingBean, DisposableBean, ApplicationContextAware, ApplicationListener&lt;ContextRefreshedEvent&gt;, BeanNameAware &#123; ......&#125; ServiceBean 继承了ServiceConfig，所有的Provider服务的Dubbo配置都在ServiceConfig中。 ServiceBean 实现了InitializingBean, 实现了afterPropertiesSet()方法，在每个Dubbo Service Bean实例化后，进行暴露服务的相关操作。 afterPropertiesSet()中前置代码都是在做一些配置校验和默认值设置，最后会执行export()方法注册暴露服务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145public void afterPropertiesSet() throws Exception &#123; //如果没有配置provider if (getProvider() == null) &#123; //获取IOC容器里的所有provider Map&lt;String, ProviderConfig&gt; providerConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProviderConfig.class, false, false); if (providerConfigMap != null &amp;&amp; providerConfigMap.size() &gt; 0) &#123; Map&lt;String, ProtocolConfig&gt; protocolConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProtocolConfig.class, false, false); if ((protocolConfigMap == null || protocolConfigMap.size() == 0) &amp;&amp; providerConfigMap.size() &gt; 1) &#123; // 兼容旧版本 List&lt;ProviderConfig&gt; providerConfigs = new ArrayList&lt;ProviderConfig&gt;(); for (ProviderConfig config : providerConfigMap.values()) &#123; if (config.isDefault() != null &amp;&amp; config.isDefault().booleanValue()) &#123; providerConfigs.add(config); &#125; &#125; //关联所有providers if (providerConfigs.size() &gt; 0) &#123; setProviders(providerConfigs); &#125; &#125; else &#123; ProviderConfig providerConfig = null; for (ProviderConfig config : providerConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (providerConfig != null) &#123; throw new IllegalStateException("Duplicate provider configs: " + providerConfig + " and " + config); &#125; providerConfig = config; &#125; &#125; if (providerConfig != null) &#123; setProvider(providerConfig); &#125; &#125; &#125; &#125; //如果没有配置application，且没有配置provider if (getApplication() == null &amp;&amp; (getProvider() == null || getProvider().getApplication() == null)) &#123; //获取所有applications Map&lt;String, ApplicationConfig&gt; applicationConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ApplicationConfig.class, false, false); if (applicationConfigMap != null &amp;&amp; applicationConfigMap.size() &gt; 0) &#123; ApplicationConfig applicationConfig = null; for (ApplicationConfig config : applicationConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (applicationConfig != null) &#123; throw new IllegalStateException("Duplicate application configs: " + applicationConfig + " and " + config); &#125; applicationConfig = config; &#125; &#125; //关联application if (applicationConfig != null) &#123; setApplication(applicationConfig); &#125; &#125; &#125; //如果没有配置module，且没有配置provider if (getModule() == null &amp;&amp; (getProvider() == null || getProvider().getModule() == null)) &#123; Map&lt;String, ModuleConfig&gt; moduleConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ModuleConfig.class, false, false); if (moduleConfigMap != null &amp;&amp; moduleConfigMap.size() &gt; 0) &#123; ModuleConfig moduleConfig = null; for (ModuleConfig config : moduleConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (moduleConfig != null) &#123; throw new IllegalStateException("Duplicate module configs: " + moduleConfig + " and " + config); &#125; moduleConfig = config; &#125; &#125; //关联module if (moduleConfig != null) &#123; setModule(moduleConfig); &#125; &#125; &#125; //如果没有配置registries，且没有配置provider if ((getRegistries() == null || getRegistries().size() == 0) &amp;&amp; (getProvider() == null || getProvider().getRegistries() == null || getProvider().getRegistries().size() == 0) &amp;&amp; (getApplication() == null || getApplication().getRegistries() == null || getApplication().getRegistries().size() == 0)) &#123; Map&lt;String, RegistryConfig&gt; registryConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, RegistryConfig.class, false, false); if (registryConfigMap != null &amp;&amp; registryConfigMap.size() &gt; 0) &#123; List&lt;RegistryConfig&gt; registryConfigs = new ArrayList&lt;RegistryConfig&gt;(); for (RegistryConfig config : registryConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; registryConfigs.add(config); &#125; &#125; //关联registries if (registryConfigs != null &amp;&amp; registryConfigs.size() &gt; 0) &#123; super.setRegistries(registryConfigs); &#125; &#125; &#125; //如果没有配置monitor，且没有配置provider if (getMonitor() == null &amp;&amp; (getProvider() == null || getProvider().getMonitor() == null) &amp;&amp; (getApplication() == null || getApplication().getMonitor() == null)) &#123; Map&lt;String, MonitorConfig&gt; monitorConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, MonitorConfig.class, false, false); if (monitorConfigMap != null &amp;&amp; monitorConfigMap.size() &gt; 0) &#123; MonitorConfig monitorConfig = null; for (MonitorConfig config : monitorConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; if (monitorConfig != null) &#123; throw new IllegalStateException("Duplicate monitor configs: " + monitorConfig + " and " + config); &#125; monitorConfig = config; &#125; &#125; //关联monitor if (monitorConfig != null) &#123; setMonitor(monitorConfig); &#125; &#125; &#125; //如果没有配置protocol，且没有配置provider if ((getProtocols() == null || getProtocols().size() == 0) &amp;&amp; (getProvider() == null || getProvider().getProtocols() == null || getProvider().getProtocols().size() == 0)) &#123; Map&lt;String, ProtocolConfig&gt; protocolConfigMap = applicationContext == null ? null : BeanFactoryUtils.beansOfTypeIncludingAncestors(applicationContext, ProtocolConfig.class, false, false); if (protocolConfigMap != null &amp;&amp; protocolConfigMap.size() &gt; 0) &#123; List&lt;ProtocolConfig&gt; protocolConfigs = new ArrayList&lt;ProtocolConfig&gt;(); for (ProtocolConfig config : protocolConfigMap.values()) &#123; if (config.isDefault() == null || config.isDefault().booleanValue()) &#123; protocolConfigs.add(config); &#125; &#125; //关联protocol if (protocolConfigs != null &amp;&amp; protocolConfigs.size() &gt; 0) &#123; super.setProtocols(protocolConfigs); &#125; &#125; &#125; //如果没有配置path if (getPath() == null || getPath().length() == 0) &#123; if (beanName != null &amp;&amp; beanName.length() &gt; 0 &amp;&amp; getInterface() != null &amp;&amp; getInterface().length() &gt; 0 &amp;&amp; beanName.startsWith(getInterface())) &#123; setPath(beanName); &#125; &#125; //暴露provider,重点！！！ if (! isDelay()) &#123; export(); &#125; &#125; Export暴露服务 export()方法会完成后续服务注册的所有流程 12345678910111213141516171819202122232425262728293031323334public synchronized void export() &#123; //如果provider没有配置 if (provider != null) &#123; //如果exporter没有配置使用provider所关联的exporter if (export == null) &#123; export = provider.getExport(); &#125; //如果delay（延迟暴露）没有配置，获取provider的delay if (delay == null) &#123; delay = provider.getDelay(); &#125; &#125; //如果不需要暴露接口则直接返回 if (export != null &amp;&amp; ! export.booleanValue()) &#123; return; &#125; //如果延迟暴露的时间（毫秒级）是存在的，开启线程并等待delay毫秒后开始暴露接口，否则直接执行暴露接口过程 if (delay != null &amp;&amp; delay &gt; 0) &#123; Thread thread = new Thread(new Runnable() &#123; public void run() &#123; try &#123; Thread.sleep(delay); &#125; catch (Throwable e) &#123; &#125; doExport(); &#125; &#125;); thread.setDaemon(true); thread.setName("DelayExportServiceThread"); thread.start(); &#125; else &#123; doExport(); &#125; &#125;]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务端业务处理不成功，应该返回HTTP 200 还是 HTTP 4XXX系列？]]></title>
    <url>%2F2018%2F04%2F23%2F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E4%B8%9A%E5%8A%A1%E5%A4%84%E7%90%86%E4%B8%8D%E6%88%90%E5%8A%9F%EF%BC%8C%E5%BA%94%E8%AF%A5%E8%BF%94%E5%9B%9EHTTP-200-%E8%BF%98%E6%98%AF-HTTP-4XXX%E7%B3%BB%E5%88%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[场景其实，纠结只出现在例如保存表单的场景，如果服务端因各种业务上的原因（校验不通过，状态不满足等）导致保存未成功，并要返回对应的提示信息，此时服务端回应此HTTP 请求时，是用 “200 + json” 还是用“400 + 错误信息”？ 在公司内不同项目间，两种风格都有，且小伙伴们各执己见。 我这么看首先，我先表达我赞同“200 + json”的方式。 更具体些，服务端所有的Controller Method对返回值做统一的Response包装 1234567891011121314样例1：&#123; "code": 200, "data": &#123;...&#125;, "message":"操作成功"&#125;样例2：&#123; "success": false "data": &#123;...&#125;, "code": 100409, "message":"数据已存在"&#125; 我的观点1. 协议分层对于RPC请求，存在两个层面的操作结果 [1] HTTP请求本身的结果 ———— 业务无关性，与网络、框架层面相关[2] 业务处理的结果 ———— 强业务逻辑相关性，与网络、框架层面无关 为什么我们会有本贴讨论的话题与分歧，或者说为什么大部分人觉得http code不够试用，是因为实际开发应用场景中，尝试着只用http code 去表达上述两个层面的结果。 分层表示的优点： RPC请求， 是可以基于不同的底层协议的， 比如我们用的HTTP协议，很容易替换成ZeroMQ, RabbitMQ, UDP， 基于TCP的自定义协议…… 只要能实现一问一答模型的协议，都是可以用的。这个时候， HTTP协议只是一种底层协议， 底层协议的错误号，并不应该被上层协议使用。 2. HTTP Code 表达能力局限性虽然HTTP协议非常友好的定义了诸多的HTTP Code码，但在实际开发应用中，对于繁多的应用场景，HTTP Code的表达能力显得力不从心，部分场景仍旧不能避免的辅以Response Body信息。加之这些HTTP Code并不是应用开发中的绝对标准。 3. HTTP Code 语义表达的不统一性[1] 同样是HTTP 4XX系列，不同系统的解释也是不一样的[2] 同样是”参数校验不通过”的业务问题，不同系统使用的HTTP码也是不一样的 4. Http Code数量有限，表达能力有限这个应该很好理解，大家应该也有体会。 5. 系统集成友好性如果我们把HTTP协议当作一种传输层协议看待，200 可以很好表达， 整个底层传输都是没有问题， 包括负载均衡系统， nginx， 反向代理， fast cgi守护程序都是工作正常的。 而返回各种HTTP Status Code经常会让外部使用者非常的困惑，特别是他们对HTTP Status Code有一定了解，却对你的系统不甚了解的情况下。 所以，除了考虑ajax请求的处理，还要考虑整个调用的中间链路以及框架集成方面的因素 返回200能避免CDN等中间商替换或缓存 国内的通信运营商画蛇添足根据HTTP状态码给替换成导航页或广告推广页面 对于系统审计程序不友好，例如 HTTP Response Code = 4XX的请求算请求成功？请求失败？请求异常？————无法区分！ 6. 扩展性 返回200OK，扩展性更强，修改的时候只需要修改字段而不需要特别处理Status Code 易于与真正的400错误区分，方便审计和分析。而实际上，当服务器能够正常返回，证明服务器已经正确的理解并得出相应的结果（并且这个结果也是预定义的，并非未知），这显然与400的定义不符。 返回200更优。保不准哪天某种状态是HTTP协议不支持的，保不准哪个需要新增的字段是HTTP协议没有的]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>WEB</tag>
        <tag>REST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Spring Bean注册]]></title>
    <url>%2F2018%2F04%2F22%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Spring-Bean%E6%B3%A8%E5%86%8C%2F</url>
    <content type="text"><![CDATA[相信大家对于Dubbo Provider/Consumer的配置非常熟练，但这背后的实现原理清楚吗？如果有不太清楚的朋友，可以再往下阅读下。 知识点 自定义 Spring XML Bean机制 背景据我们所知 Spring 注解方式声明Bean的方式是在Class上打上@Component注解（@Component的扩展注解也可），当Spring容器启动时，Spring会自动扫面所有带有@Component注解的Class，自动注册到Bean容器中。 例如： 1234@Componentpublic class Student &#123; //do something&#125; 也可以通过XML文件配置的方式声明Bean 例如： 1&lt;bean id="Student" class="com.test.spring.beans.Student"&gt;&lt;/bean&gt; 但，回过头来看Dubbo，我们并没有通过上述的方式去声明Dubbo配置中的Bean，却也能像使用Spring Bean一样用@Autowire去注入Dubbo服务的Bean，这其中的原理是什么呢？让我们从源码中找答案。 原理启动Spring容器在SpringBoot+Dubbo的搭配中，Java应用的启动入口main方法一般会这么写。通过此步骤去启动Java程序并将Dubbo Bean注册Spring容器。 12345678910111213141516package com.sample;@ComponentScan(basePackages = &#123; "com.sample.myapp"&#125;)@SpringBootApplication@EnableSchedulingpublic class MyApplication &#123; public static void main(String[] args) &#123; //启动Spring容器 SpringApplication application = new SpringApplication(MyApplication.class, "classpath:/spring/dubbo-config.xml"); //指定Dubbo配置文件 application.run(args); &#125;&#125; Spring如何识别Dubbo 自定义Bean标签Spring为了支持用户自定义类加载到Spring容器，提供了org.springframework.beans.factory.xml.NamespaceHandler接口和org.springframework.beans.factory.xml.NamespaceHandlerSupport抽象类，NamespaceHandler#init方法会在对象的构造函数调用之后、属性初始化之前被DefaultNamespaceHandlerResolver调用。dubbo的DubboNamespaceHandler类正是继承了NamespaceHandlerSupport，其代码实现如下： 1234567891011121314151617181920public class DubboNamespaceHandler extends NamespaceHandlerSupport &#123; static &#123; Version.checkDuplicate(DubboNamespaceHandler.class); &#125; public void init() &#123; registerBeanDefinitionParser("application", new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser("module", new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser("registry", new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser("monitor", new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser("provider", new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser("consumer", new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser("protocol", new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser("service", new DubboBeanDefinitionParser(ServiceBean.class, true)); registerBeanDefinitionParser("reference", new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser("annotation", new AnnotationBeanDefinitionParser()); &#125;&#125; registerBeanDefinitionParser方法使用的是父抽象类NamespaceHandlerSupport的默认实现，第一个参数是elementName，即元素名称，即告诉Spring你要解析哪个标签，第二个参数是BeanDefinitionParser的实现类，BeanDefinitionParser是Spring用来将xml元素转换成BeanDefinition对象的接口。dubbo的DubboBeanDefinitionParser类就实现了这个接口，负责将标签转换成bean定义对象BeanDefinition。 所以，以后想要了解Dubbo Bean初始化相关细节，可以查看DubboBeanDefinitionParser#parse的代码实现。 例如： Dubbo Bean 会有哪些默认设置 dubbo服务提供者使用dubbo:service标签时，如果既不设置id，也不设置name，则dubbo给ServiceBean在Spring容器中定义的ID是什么？ Dubbo xml文件中的配置是怎么作用到Dubbo Bean中去的 关于NamespaceHandlerSupport spring.handlers # 指定xml namespace的解析handler类 spring.schemas # 指定xml xsd文件位置 dubbo.xsd # 设计你要的xml配置格式 DubboNamespaceHandler # 自定义NamespaceHandler,完成从xml中读取配置内容，并转换成Spring Bean进行注册 Spring容器会默认加载classpath/META-INF下的spring.handlers和spring.schemas两个文件，来加载xsd和对应的NamespaceHandler,所以dubbo-config-spring包下的META-INF目录下也有这两个文件 练习DEMO1. 设计配置属性和JavaBean 设计好配置项，并通过JavaBean来建模，本例中需要配置People实体，配置属性name和age（id是默认需要的） 12345public class People &#123; private String id; private String name; private Integer age; &#125; 2. 编写XSD文件 为上一步设计好的配置项编写XSD文件，XSD是schema的定义文件，配置的输入和解析输出都是以XSD为契约，本例中XSD如下 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;xsd:schema xmlns="http://veryjj/cutesource/schema/people" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:beans="http://www.springframework.org/schema/beans" targetNamespace="http://veryjj/cutesource/schema/people" elementFormDefault="qualified" attributeFormDefault="unqualified"&gt; &lt;xsd:import namespace="http://www.springframework.org/schema/beans" /&gt; &lt;xsd:element name="people"&gt; &lt;xsd:complexType&gt; &lt;xsd:complexContent&gt; &lt;xsd:extension base="beans:identifiedType"&gt; &lt;xsd:attribute name="name" type="xsd:string" /&gt; &lt;xsd:attribute name="age" type="xsd:int" /&gt; &lt;/xsd:extension&gt; &lt;/xsd:complexContent&gt; &lt;/xsd:complexType&gt; &lt;/xsd:element&gt; &lt;/xsd:schema&gt; 关于xsd:schema的各个属性具体含义就不作过多解释，可以参见http://www.w3school.com.cn/schema/schema_schema.asp &lt;xsd:element name=”people”&gt;对应着配置项节点的名称，因此在应用中会用people作为节点名来引用这个配置 &lt;xsd:attribute name=”name” type=”xsd:string” /&gt;和&lt;xsd:attribute name=”age” type=”xsd:int” /&gt;对应着配置项people的两个属性名，因此在应用中可以配置name和age两个属性，分别是string和int类型 完成后需把xsd存放在classpath下，一般都放在META-INF目录下（本例就放在这个目录下） 3. 编写NamespaceHandler和BeanDefinitionParser完成解析工作 1234567891011121314151617181920212223242526 public class MyNamespaceHandler extends NamespaceHandlerSupport &#123; public void init() &#123; registerBeanDefinitionParser("people", new PeopleBeanDefinitionParser()); &#125; &#125; public class PeopleBeanDefinitionParser extends AbstractSingleBeanDefinitionParser &#123; protected Class getBeanClass(Element element) &#123; return People.class; &#125; protected void doParse(Element element, BeanDefinitionBuilder bean) &#123; String name = element.getAttribute("name"); String age = element.getAttribute("age"); String id = element.getAttribute("id"); if (StringUtils.hasText(id)) &#123; bean.addPropertyValue("id", id); &#125; if (StringUtils.hasText(name)) &#123; bean.addPropertyValue("name", name); &#125; if (StringUtils.hasText(age)) &#123; bean.addPropertyValue("age", Integer.valueOf(age)); &#125; &#125; &#125; 4. 编写spring.handlers和spring.schemas串联起所有部件 spring提供了 spring.handlers和spring.schemas这两个配置文件来完成这项工作，这两个文件需要我们自己编写并放入META-INF文件夹 中，这两个文件的地址必须是META-INF/spring.handlers和META-INF/spring.schemas，spring会默认去 载入它们，本例中spring.handlers如下所示： spring.handlers 1http\://veryjj/cutesource/schema/people=study.schemaExt.MyNamespaceHandler spring.schemas 1http\://veryjj/cutesource/schema/people.xsd=META-INF/people.xsd 以上就是载入xsd文件 5. 使用自定义schema定义Spring Bean 到此为止一个简单的自定义配置以完成，可以在具体应用中使用了。使用方法很简单，和配置一个普通的spring bean类似，只不过需要基于我们自定义schema，本例中引用方式如下所示： 12345678&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:cutesource="http://veryjj/cutesource/schema/people" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://veryjj/cutesource/schema/people http://veryjj/cutesource/schema/people.xsd"&gt; &lt;cutesource:people id="cutesource" name="黄老师" age="27"/&gt; &lt;/beans&gt; 其中xmlns:cutesource=”http://veryjj/cutesource/schema/people&quot; 是用来指定自定义schema，xsi:schemaLocation用来指定xsd文件。&lt;cutesource:people id=”cutesource” name=”黄老师” age=”27”/&gt;是一个具体的自定义配置使用实例。 6. 注入自定义schema定义的Spring Bean 跟Spring Bean的注入方式完全一样，按你喜欢的方式来。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码解析-Dubbo可以这么学]]></title>
    <url>%2F2018%2F04%2F22%2FDubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-Dubbo%E5%8F%AF%E4%BB%A5%E8%BF%99%E4%B9%88%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[最近面试工作颇多，Dubbo作为的微服务主流技术架构，也是分布式系统中面试的高频考题之一。但从面试的过程中得到的反馈，大家对于Dubbo的关注以及掌握程度基本都处于会基本使用的程度，基本没遇到有对Dubbo框架做学习研究的求职者。 求职者一般只会聊下面两个话题： Dubbo 是什么东西？ 答：RPC框架/微服务框架，在实际工作中用Dubbo做业务功能服务化。 Dubbo的工作原理是什么样的？ 答：Provider端将服务注册到Zookeeper中，Consumer端从Zookeeper获取Provider，然后就可以调用API了。 一般情况下关于Dubbo的基本都聊到此结束了，虽然说没回答错，但也忒简洁了吧，连Dubbo架构图中（下图）的内容都没说完整，而这并不是面试官想得到的讯息。 Dubbo作为主流的微服务技术框架，必然有其优秀的一面，也是学习RPC框架思想很好的素材 Dubbo 应该掌握哪些内容？（个人思路） 阅读Dubbo的用户手册以及开发手册。Dubbo.io 知晓Dubbo支持的功能 知晓Dubbo的各种扩展点 知晓Dubbo的设计思想（这里不得不说Dubbo.io的文档说明写的非常详细、到位，甚至一度让我觉得没有写Blog的必要） Dubbo 核心流程源码实现 Dubbo Bean的集成 Provider 注册、暴露服务 Consumer 注册、订阅服务 Consumer 调用实现 Provider 处理请求 Dubbo SPI机制 Dubbo Filter机制 思考些高级的 Dubbo各可配机制主流选择的优缺点 register remoting rpc Dubbo Cluster Dubbo 怎么做服务治理 策略路由 降级 熔断 Dubbo 性能基线&amp;性能调优 框架扩展 服务监控 流量分析 那么，逐步的去落实吧！如果开始、请务必坚持！]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty 核心对象梳理-1]]></title>
    <url>%2F2018%2F04%2F08%2FNetty-%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1%E6%A2%B3%E7%90%86-1%2F</url>
    <content type="text"><![CDATA[《Netty In Action》阅读笔记摘要 What is NettyNetty是一款用于快速开发高性能的网络应用程序的Java框架 它封装了网络编程的复杂性 Key words: 是一款Java语言的开发框架 封装、提供程序快速网络编程的能力 高性能 Netty是完全异步和事件驱动的 Netty 核心组件ChannelChannel 是Java NIO的一个基本构造 可以将Channel 看作是连接的载体。因此，它可以被打开、被关闭、连接、断开连接。 事件Channel连接上发生的事件。可以等价的理解为 epoll 中关于每个Socket事件，例如：EPOLLIN, EPOLLOUT, EPOLLHUP的回调 回调类似于常见的回调，Netty内部用回调来处理事件； 可理解为 epoll 中关于每个Socket事件的回调，例如：EPOLLIN, EPOLLOUT, EPOLLHUP的回调。 应用程序可以自定义回调，感知Netty网络通信的事件。 ChannelFutureFuture提供了另一种在操作完成时通知应用程序的方式。 JDK中的java.util.concurrent.Future 在使用上是阻塞调用的，不优雅。Netty 提供了另一种实现：ChannelFuture，用于在执行异步操作的时候使用。 Netty的每个出站I/O都将返回一个ChannelFuture。 使用ChannelFuture时，可以配合使用ChannelFutureListener。只要实现operationComplete() 回调即可，非常方便。且支持多ChannelFuture。 1234567891011121314151617Channel channel = ...;//Does not block ChannelFuture future = channel.connect(new InetSocketAddress("192.168.0.1", 25));​future.addListener(new ChannelFuturelistener()&#123; @Override public void operationComplete(ChannelFuture future)&#123; if (future.isSuccess())&#123; ByteBuf buffer = Unpooled.copiedBuffer("Hello", Charset.defaultCharset()); ChannelFuture wf = future.channel().writeAndFlush(buffer); ... &#125; else &#123; Throwable cause = future.cause(); cause.printStackTrace(); &#125; &#125;&#125;) ChannelHandler可以初步理解为每个ChannelHandler实例都类似于一种为了响应特定事件而被执行的回调。 PSNetty 的内部实现细节跟Linux epoll的用法很相似，熟悉Linux Epoll以及编程模型的朋友来说可以对比着来学习，寻找类同点、差异点以及差异的原因。 Netty 同时支持OIO, NIO, EPOLL等多路复用模式，我是以熟悉的epoll作为切入熟悉的内部流程原理。 Netty的组件和设计 Channel ———— Socket； EventLoop ———— 控制流、多线程处理、并发； ChannelFuture ———— 异步通知； Channel 接口Netty的Channel接口所提供的API，大大地降低了使用Socket类的复杂性。 EventLoopGroup 接口主要作用 用于注册Channel 执行部分Runnable任务 这里重点讲下“注册Channel”，在实际编程或应用时，每个Channel都是向EventLoopGroup注册的，由EventLoopGroup按照指定的策略方法，将Channel注册到EventLoopGroup下某个具体的EventLoop当中去。 12345678910111213141516171819202122232425262728293031323334public interface EventLoopGroup extends EventExecutorGroup &#123; ... /** * Register a &#123;@link Channel&#125; with this &#123;@link EventLoop&#125;. The returned &#123;@link ChannelFuture&#125; * will get notified once the registration was complete. */ ChannelFuture register(Channel channel); ...&#125;public abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup &#123; ... @Override public ChannelFuture register(Channel channel) &#123; return next().register(channel); &#125; ...&#125;public abstract class MultithreadEventExecutorGroup extends AbstractEventExecutorGroup &#123; ... @Override public EventExecutor next() &#123; return chooser.next(); &#125; ... EventLoop 接口EventLoop定义了Netty的核心抽象，用于处理连接的生命周期中所发生的事件。 一个EventLoopGroup包含一个或者多个EventLoop； 一个EventLoop在它的生命周期内只和一个Thread绑定； 所有由EventLoop处理的I/O事件都将在它专有的Thread上被处理； 一个Channel在它的生命周期内只注册于一个EventLoop； 一个EventLoop可能会被分配给一个或多个Channel； 在这种设计中，一个给定的Channel的I/O操作都是由相同的Thread执行的，实际上消除了对于同步的需要。 ChannelFuture 接口Netty中所有的I/O操作都是异步的。所有我们需要一种用于在之后的某个时间点确定其结果的方法。 为此，Netty提供了ChannelFuture接口，其addListener()方法注册了一个ChannelFutureListener，以便在某个操作完成时得到通知。 ChannelHandler 接口顾名思义，Channel的Handler，它充当了所有处理入站和出站数据的应用程序逻辑的容器。ChannelHandler的方法是由网络事件触发的。 ChannelPipeline 接口ChannelPipeline为ChannelHandler链提供了容器，并定义了用于在该链上传播入站和出站事件流的API。当Channel被创建时，它会被自动的分配到它专属的ChannelPipeline。 ChannelPipleline中的ChannelHandler的执行顺序是由它们被添加的顺序所决定的。 编码器和解码器Netty用于网络通信，天然需要编码和解码。也是用ChannelPipeline + ChannelHandler的机制实现的。 所有由Netty提供的编码器/解码器适配器类都实现了ChannelOutboundHandler或者ChannelInboundHandler接口。 引导（Bootstrap）Netty的引导类为应用程序的网络层配置提供了容器。 类别 Bootstrap ServerBootstrap 网络编程中的作用 连接到远程主机和端口 绑定到一个本地端口 EventLoopGroup的数目 1 2 使用Netty的ChannelOption和属性 在每个Channel创建时都手动配置它可能会变得相当乏味。幸运的是，你不必这样做。相反，你可以使用option()方法来将ChannelOption应用到Bootstrap上。你所提供的值将会被自动应用到Bootstrap所创建的所有Channel。 引导DatagramChannel Bootstrap除了引导基于TCP协议的SocketChannel，也可以用于引导无连接的协议。Netty提供了各种DatagramChannel的实现。与面向连接的TCP相比，唯一区别是不再调用connect()方法，而是只调用bind()方法 123456789101112131415161718192021222324//使用Bootstrap和DatagramChannelBootstrap bootstrap = new Bootstrap();bootstap.group(new OioEventLoopGroup()) .channel(OioDatagramChannel.class) .handler(new SimpleChannelInboundHandler&lt;DatagramPacket&gt;()&#123; @Override public void channelRead0(ChannelHandlerContext ctx, DatagramPacket msg) throws Exception &#123; //Do something with the packet &#125; &#125;);ChannelFuture future = bootstrap.bind(new InetSocketAddress(0));future.addListener(new ChannelFutureListener()&#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception&#123; if (channelFuture.isSuccess())&#123; System.out.println("Channel bound"); &#125; else &#123; System.out.println("Bind attempt failed"); channelFuture.cause().printStackTrace(); &#125; &#125;&#125;) 我的理解 引导的根对象是 EventLoopGroup，间接的负责监听、处理所有Channel的网络事件。 EventLoop是EventLoopGroup内的成员，每个EventLoop与具体的线程绑定。也可以理解一个线程，一个EventLoop。 EventLoop直接负责处理其下所有Channel的网络事件。 ChannelHadler是Channel网络事件逻辑处理的容器，应用逻辑开发的重点就在此。 当一个Channel上来一个网络事件时，对应的EventLoop首先进行响应，并找到Channel所属的ChannelPipeline，Channel作为输入驱动一次ChannelPipeline。 ChannelPipeline 遍历其下ChannelHandler，逐个处理Channel的网络事件。 ChannelFuture可以同步等结果，也可以异步通知结果，都支持，自己选！ ByteBuf网络数据的基本单位是字节。Java NIO提供了ByteBuffer作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。 Netty的ByteBuffer替代品是ByteBuf，一个强大的实现，既解决了JDK API的局限性，又为网络应用程序的开发者提供了更好的API。 ByteBuf优点： 对于同一个数据buffer，维护readIndex, writeIndex两份索引 ByteBuf模式 堆缓冲区模式： 将数据存储在JVM的堆空间中，应用代码可直接访问缓冲区中的数据。 直接缓冲区模式： JDK 1.4引入的ByteBuffer类允许JVM实现直接使用操作系统的本地内容，这就避免了JAVA 应用在每次调用本地I/O操作前/后 需要将缓冲区的内容复制到一个与操作系统结合的中间缓冲区中。 缺点：因为数据不是在堆上，所以业务代码处理时不得不经过一次复制。 复合缓冲区： 为多个ByteBuf提供一个统一的聚合视图，可以根据需要向复合缓冲区中添加或者删除ByteBuf实例。 字节级操作 可以以字节的操作方式使用ByteBuf 随机访问索引 顺序访问索引 可丢弃字节 可读字节 可写字节 索引管理 indexOf / ByteBufProcessor 派生缓冲区 读/写操作 ByteBuf池化分配 为了降低分配和释放内存的开销，Netty通过interface ByteBufAllocator实现了ByteBuf的池化。 Unpooled 缓冲区 如果未能获取到一个ByteBufAllocator的引用，Netty提供一个简单的Unpooled工具类，它提供创建未池化的ByteBuf实例。 关于 ChannelFuture 和 ChannelPromise ChannelFuture read-only 没有返回值的异步通知、调用 DefaultFutureListeners -&gt; listeners[N] ChannelPromise writeable 可写异步执行结果的通知、调用 notifyListenerNow -&gt; 回到Listeners -&gt; 取出对应的Channel进行回调操作]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅聊分布式事务]]></title>
    <url>%2F2018%2F03%2F30%2F%E6%B5%85%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前言 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近很久没有写博客了，一方面是因为公司事情最近比较忙，另外一方面是因为在进行 CAP 的下一阶段的开发工作，不过目前已经告一段落了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来还是开始我们今天的话题，说说分布式事务，或者说是我眼中的分布式事务，因为每个人可能对其的理解都不一样。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式事务是企业集成中的一个技术难点，也是每一个分布式系统架构中都会涉及到的一个东西，特别是在微服务架构中，几乎可以说是无法避免，本文就分布式事务来简单聊一下。 数据库事务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等… &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接着，我们就说一下分布式事务。 分布式理论 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？ CAP定理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。 这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个时候有同学可能会把数据库的2PC（两阶段提交）搬出来说话了。OK，我们就来看一下数据库的两阶段提交。 对数据库分布式事务有了解的同学一定知道数据库支持的2PC，又叫做 XA Transactions。 MySQL从5.5版本开始支持，SQL Server 2005 开始支持，Oracle 7 开始支持。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，XA 是一个两阶段提交协议，该协议分为以下两个阶段： 第一阶段：事务协调器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交. 第二阶段：事务协调器要求每个数据库提交数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，如果有任何一个数据库否决此次提交，那么所有数据库都会被要求回滚它们在此事务中的那部分信息。这样做的缺陷是什么呢? 咋看之下我们可以在数据库分区之间获得一致性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果CAP 定理是对的，那么它一定会影响到可用性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果说系统的可用性代表的是执行某项操作相关所有组件的可用性的和。那么在两阶段提交的过程中，可用性就代表了涉及到的每一个数据库中可用性的和。我们假设两阶段提交的过程中每一个数据库都具有99.9%的可用性，那么如果两阶段提交涉及到两个数据库，这个结果就是99.8%。根据系统可用性计算公式，假设每个月43200分钟，99.9%的可用性就是43157分钟, 99.8%的可用性就是43114分钟，相当于每个月的宕机时间增加了43分钟。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上，可以验证出来，CAP定理从理论上来讲是正确的，CAP我们先看到这里，等会再接着说。 BASE理论&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，我们往往追求的是可用性，它的重要程序比一致性要高，那么如何实现高可用性呢？ 前人已经给我们提出来了另外一个理论，就是BASE理论，它是用来对CAP定理进行进一步扩充的。BASE理论指的是： Basically Available（基本可用） Soft state（软状态） Eventually consistent（最终一致性） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有了以上理论之后，我们来看一下分布式事务的问题。 分布式事务&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，要实现分布式事务，无外乎那几种解决方案。 一、两阶段提交（2PC）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和上一节中提到的数据库XA事务一样，两阶段提交就是使用XA协议的原理，我们可以从下面这个图的流程来很容易的看出中间的一些比如commit和abort的细节。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;两阶段提交这种解决方案属于牺牲了一部分可用性来换取的一致性。在实现方面，在 .NET 中，可以借助 TransactionScop 提供的 API 来编程实现分布式系统中的两阶段提交，比如WCF中就有实现这部分功能。不过在多服务器之间，需要依赖于DTC来完成事务一致性，Windows下微软搞的有MSDTC服务，Linux下就比较悲剧了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外说一句，TransactionScop 默认不能用于异步方法之间事务一致，因为事务上下文是存储于当前线程中的，所以如果是在异步方法，需要显式的传递事务上下文。 优点： 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能100%保证强一致） 缺点： 实现复杂，牺牲了可用性，对性能影响较大，不适合高并发高性能场景，如果分布式系统跨接口调用，目前 .NET 界还没有实现方案。 二、补偿事务（TCC）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段： Try 阶段主要是对业务系统做检测及资源预留 Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 举个例子，假入 Bob 要向 Smith 转账，思路大概是：我们有一个本地方法，里面依次调用1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些 缺点： 缺点还是比较明显的，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。 三、本地消息表（异步确保）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于ebay。我们可以从下面的流程图中看出其中的一些细节： 基本思路就是： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。 优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。在 .NET中 有现成的解决方案。 缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。 四、MQ 事务消息&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有一些第三方的MQ是支持事务消息的，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。 以阿里的 RocketMQ 中间件为例，其思路大致为： 第一阶段Prepared消息，会拿到消息的地址。 第二阶段执行本地事务，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。 也就是说在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了RocketMQ会定期扫描消息集群中的事务消息，这时候发现了Prepared消息，它会向消息发送者确认，所以生产方需要实现一个check接口，RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。 优点： 实现了最终一致性，不需要依赖本地数据库事务。 缺点： 实现难度大，主流MQ不支持，没有.NET客户端，RocketMQ事务消息部分代码也未开源。 五、Sagas 事务模型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Saga事务模型又叫做长时间运行的事务（Long-running-transaction）, 它是由普林斯顿大学的H.Garcia-Molina等人提出，它描述的是另外一种在没有两阶段提交的的情况下解决分布式系统中复杂的业务事务问题。你可以在这里看到 Sagas 相关论文。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们这里说的是一种基于 Sagas 机制的工作流事务模型，这个模型的相关理论目前来说还是比较新的，以至于百度上几乎没有什么相关资料。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该模型其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，那么Sagas工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如我们一次关于购买旅游套餐业务操作涉及到三个操作，他们分别是预定车辆，预定宾馆，预定机票，他们分别属于三个不同的远程接口。可能从我们程序的角度来说他们不属于一个事务，但是从业务角度来说是属于同一个事务的。 他们的执行顺序如上图所示，所以当发生失败时，会依次进行取消的补偿操作。 因为长事务被拆分了很多个业务流，所以 Sagas 事务模型最重要的一个部件就是工作流或者你也可以叫流程管理器（Process Manager），工作流引擎和Process Manager虽然不是同一个东西，但是在这里，他们的职责是相同的。在选择工作流引擎之后，最终的代码也许看起来是这样的 12345678910111213SagaBuilder saga = SagaBuilder.newSaga(&quot;trip&quot;) .activity(&quot;Reserve car&quot;, ReserveCarAdapter.class) .compensationActivity(&quot;Cancel car&quot;, CancelCarAdapter.class) .activity(&quot;Book hotel&quot;, BookHotelAdapter.class) .compensationActivity(&quot;Cancel hotel&quot;, CancelHotelAdapter.class) .activity(&quot;Book flight&quot;, BookFlightAdapter.class) .compensationActivity(&quot;Cancel flight&quot;, CancelFlightAdapter.class) .end() .triggerCompensationOnAnyError();camunda.getRepositoryService().createDeployment() .addModelInstance(saga.getModel()) .deploy(); 这里有一个 C# 相关示例，有兴趣的同学可以看一下。 优缺点 这里我们就不说了，因为这个理论比较新，目前市面上还没有什么解决方案，即使是 Java 领域，我也没有搜索的太多有用的信息。 转自：https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识鱼骨图]]></title>
    <url>%2F2018%2F03%2F29%2F%E8%AE%A4%E8%AF%86%E9%B1%BC%E9%AA%A8%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[今天，偶然的机会在同事电脑上漂见“鱼骨图”。晚上吃完饭就在思考这个问题：“为什么我从来没用过鱼骨图分析问题？鱼骨图适用于什么类型的分析？” 查了些资料追求普及，介绍的内容大同小异 概念摘要鱼骨分析法，又名 因果分析法，是一种发现问题”根本原因”的分析方法。 鱼骨图可以进一步被划分为： 整理问题型鱼骨图（各要素与特性值间不存在原因关系，而是结构构成关系） 原因型鱼骨图（鱼头在右，特性值通常以“为什么……”来写） 对策型鱼骨图（鱼头在左，特性值通常以“如何提高/改善……”来写） 怎么作图分析结构 A、针对问题点，选择层别方法（如人机料法环等）； B、按头脑风暴分别对各层别类别找出所有可能原因（因素）； C、将找出的各要素进行归类、整理，明确其从属关系； D、分析选取重要因素； E、检查各要素的描述方法，确保语法简明、意思明确; 分析要点 A、确定大要因（大骨）时，现场作业一般从“人机料法环”着手,管理类问题一般从“人事时地物”层别，应视具体情况决定； B、大要因必须用中性词描述（不说明好坏），中、小要因必须使用价值判断（如…不良）； C、头脑风暴时，应尽可能多而全地找出所有可能原因，而不仅限于自己能完全掌控或正在执行的内容。对人的原因，宜从行动而非思想态度面着手分析； D、中要因跟特性值、小要因跟中要因间有直接的原因-问题关系，小要因应分析至可以直接下对策； E、如果某种原因可同时归属于两种或两种以上因素，请以关联性最强者为准（必要时考虑三现主义：即现时到现场看现物，通过相对条件的比较，找出相关性最强的要因归类。）； F、选取重要原因时，不要超过7项，且应标识在最未端原因。 绘图过程 A、填写鱼头（按为什么不好的方式描述），画出主骨； B、画出大骨，填写大要因； C、画出中骨、小骨，填写中小要因； D、用特殊符号标识重要因素； 使用步骤(1) 查找要解决的问题； (2) 把问题写在鱼骨的头上； (3) 召集同事共同讨论问题出现的可能原因，尽可能多地找出问题； (4) 把相同的问题分组，在鱼骨上标出； (5) 根据不同问题征求大家的意见，总结出正确的原因； (6) 拿出任何一个问题，研究为什么会产生这样的问题； (7) 针对问题的答案再问为什么？这样至少深入五个层次（连续问五个问题）； (8) 当深入到第五个层次后，认为无法继续进行时，列出这些问题的原因，而后列出至少20个解决方法。 看了鱼骨图的概念介绍后，在脑海中立马出现了新的问题：”鱼骨图和思维导图有什么区别？” 鱼骨图和思维导图有什么区别？类同点 都是基于主题逐步分解、细化的过程 都是类树形结构 我的理解理解-1首先、思维导图和鱼骨图都是图形思维，而图形思维最大的特点就是将我们的思维结构化，并由此实现图形化。 而所谓的结构化一般就是构建逻辑思维，任何的逻辑思维都是基于这两个出发点而来的：1、分类；2、顺序；所以只要你的分类与顺序是一样的，那么你的逻辑思维是一样的，由此产生的图形思维也是一样的。而图形、分列、表格……只是图形思维具体的呈现方式。换句话说，在图形思维一样的前提下，不同的人选择了自己认为好的呈现方式让图形思维更可视化、更清晰化、更感性化。 另外，还有不同之处在于思维导图不仅仅能用于构建逻辑思维，还能扩展发散思维，以及还能强化思维记忆。这是和鱼骨图的区别之一！ 理解-2 【思维导图】在实际应用中，思维导图常用于个人，侧重于个人思维的发散，以求思维的全面性，思维导图的层次感是逻辑思维自然的产出。应用场景广泛。 【鱼骨图】常用在多人同时参与的头脑风暴场景，侧重于发挥团队智力逐层解剖问题，找到问题原因。往往会以”问题原因分析 + 跟进Action”作为结果产出。 鱼骨图作图工具 Xmind]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>分析图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写作技巧]]></title>
    <url>%2F2018%2F03%2F17%2FHexo%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[Hexo 写作的技巧与备忘，不喜勿喷。 链接： Hexo命令 文章插入图片原生Markdown语法原生Markdown语法插入图片有三种方式 1. 插入本地图片只需要在基础语法的括号中填入图片的位置路径即可，支持绝对路径和相对路径。例如： 1![image](/home/picture/1.png) 评价：不灵活不好分享，本地图片的路径更改或丢失都会造成markdown文件调不出图。 2. 插入网络图片只需要在基础语法的括号中填入图片的网络链接即可，现在已经有很多免费/收费图床和方便传图的小工具可选。例如： 1![image](http://baidu.com/pic/doge.png) 评价：将图片存在网络服务器上，非常依赖网络和网络图片存储 3. 把图片存入markdown文件用base64转码工具把图片转成一段字符串，然后把字符串填到基础格式中链接的那个位置。基础用法： 1![avatar](data:image/png;base64,iVBORw0......) 这个时候会发现插入的这一长串字符串会把整个文章分割开，非常影响编写文章时的体验。如果能够把大段的base64字符串放在文章末尾，然后在文章中通过一个id来调用，文章就不会被分割的这么乱了。比如： 12![avatar][doge] [doge]:data:image/png;base64,iVBORw0...... 评价：麻烦，费劲。 Hexo方式安装插件与配置 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save，这是下载安装一个可以上传本地图片的插件 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 使用方式在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片 1![你想输入的替代文字](xxxx/图片名.jpg) 注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo装修记录]]></title>
    <url>%2F2018%2F03%2F17%2FHexo%E8%A3%85%E4%BF%AE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前篇《个人博客搭建-结缘Hexo》 从时间投入上来说，搭建Hexo可谓是分分钟的事情，装修可花了我近一天的时间。 Hexo 是一个开放、扩展性强的框架，样式风格、功能都可以通过主题包、插件来实现。完全可以根据个人口味来装修你的博客。 选主题关于Hexo的主题，你可以问度娘”Hexo Themes”网友好评度高的主题，也可以去官网自行挑选https://hexo.io/themes/ 需要提前说明的是：不是选上对应主题，所有功能就有了，这也是我为什么会单独写这篇备忘的原因。 经过各种试用和对比，我最终选择了：NexT这个主题 Go Github，理由如下： （Most Important）界面符合我口味。NexT 中的Pisces主题，样例：IIssNan’s Notes NexT主题集成的插件多，极大的方便了初级的小白用户，详见：NexT主题的_config.xml配置项 博客评论 Baidu Analytics / Google Analytics 文章阅读数量 baidu push algolia_search / local_search 背景画布特效 highlight_theme 等等 支持手机端 安装主题Hexo theme 统一放在Hexo根目录的themes目录下，每个主题一个子目录。 如果主题是Github上的，推荐使用如下命令下载 1git clone https://github.com/iissnan/hexo-theme-next.git themes/next 然后修改hexo根目录下的_config.xml，切换主题 OK, hexo g -&gt; hexo s 看下效果吧 主题配置找到 hexo/themes/next/_config.xml，一项项阅读熟悉吧，注释写的很详细。 基础配置基础配置项： 菜单配置：主页、关于、标签、分类、归档等 头像 打赏 社交主页 侧边栏 主题的_config.xml会引用hexo的_config.xml中基础配置，所以请同时配置hexo的_config.xml，例如：博客抬头、语种、相关数据目录等 背景特效效果图示 配置next主题的_config.xml 支持搜索 文章阅读计数NexT提供两种插件方式：1、leancloud_visitors（国内的） 和 2、firestore(谷歌的)我选用的是leancloud_visitors，配置相对简单一些 配置LeanCloud 注册：https://leancloud.cn打开LeanCloud官网，进入注册页面注册。完成邮箱激活后，点击头像，进入控制台页面创建新应用，如下： 创建名称为Counter的Class 修改NexT的_config.xml配置文件 123456# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: "你的App Id" app_key: "你的App Key" PV/UV Google Analytics12# Google Analyticsgoogle_analytics: '你的Google Analytics Code' 问题解决标签和分类页面不显示问题当时切换NexT主题后，侧边栏的标签、分类点击时，是无法正常显示标签和分类的，属：Cannot Get /tags/ 若出现此问题，请按下方式解决在hexo 目录下执行 步骤一： 1hexo new page 'tags' 步骤二： 编辑刚新建的页面，将页面的类型设置为tags，主题会自动为这个页面显示标签云。 123456---title: TagClouddate: 2018-03-17 15:31:21type: "tags"comments: false #注意：如果有启动多说或Disqus评论，需要关闭评论，添加comments字段并设置为false--- “分类”同理~ 其他推荐 hexo的next主题个性化配置教程]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客搭建-结缘Hexo]]></title>
    <url>%2F2018%2F03%2F17%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA-%E7%BB%93%E7%BC%98Hexo%2F</url>
    <content type="text"><![CDATA[Github Pages + Hexo 搭建个人博客偶然的机会，看到”***.github.io”域名的个人博客，瞬间就来了兴趣，莫非Github能方便的创建个人博客？ 咨询了下度娘，知道了用GitHub Pages + Hexo 搭建个人博客的方式，于是说干就干！ PS: 我用的是Macbook + Shell，某些操作Windows的朋友可能要转化翻译成Windows上的命令。 1. 创建Github仓库首先，个人Github账号应该有吧？如果没有，先去注册一个。 然后，在Github上新建一个仓库，如下图： 确保新建的仓库以Github Pages方式发布 OK，此步骤完成了！ 2. 安装Hexo正式使用Hexo前，请先安装Node.js 和Git 安装Node.js去官网下载并安装：Link 安装Git 作为玩Github的程序员，默认你已经安装了Git 安装Hexo 新建一个目录作为Hexo的根目录（PS: 后续Hexo相关的功能以及写博客都基于此目录） 进入新建的目录 12345npm install hexo-cli -ghexo init #初始化网站npm installhexo g #hexo generate的简写，意思生成博客站点hexo s #hexo server的简写，即启动运行hexo的站点，这一步之后就可以通过http://localhost:4000 查看了 常用Hexo命令 1234hexo c : hexo clean 清除hexo已生成的publichexo g : hexo generate 重新生成hexo站点hexo s : hexo server 运行hexo站点。注：本地运行时，在hexo上做的修改保存后即生效的，不用重新hexo ghexo d : hexo deploy 将hexo发布到github上去。 3. Hexo deploy 到Github 编辑根目录下_config.yml文件 1234deploy: type: git repo: https://github.com/VeryJJ/VeryJJ.github.io.git #这里的网址填你自己的 branch: master 安装hexo deploy插件： npm install hexo-deployer-git –save 在Hexo目录下执行hexo d hexo d 成功后，就大工告成拉！你可以在浏览器输入***.github.io(你新建的github.io仓库)，就能看到你的个人博客拉！ 以后写博客的步骤为： 在电脑本地hexo new ‘文章名’ 丰富你的文章 hexo g hexo d 发布 博客搭建好了，但相信你会觉得它好丑，没关系，请继续阅读下一篇Hexo的装修总结 参考链接 我是如何利用Github Pages搭建起我的博客，细数一路的坑]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 场景化下的命令备忘]]></title>
    <url>%2F2018%2F03%2F15%2FGit%20%E5%9C%BA%E6%99%AF%E5%8C%96%E4%B8%8B%E7%9A%84%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[查看分支1234git branchgit branch -a 查看远端所有分支git branch -v 查看本地分支，以及分支上最新的commit提交信息git branch -vv 在-v基础上，多现实本地分支和远程分支的关联关系 查看某次commit的修改1git show commit号 查看某个文件的历史修改记录12345678910111213141516171819202122232425262728293031323334git log 文件名git log -p 文件名git log --author=提交人 只查看提交人的提交记录git log --pretty=oneline 单行显示提交记录git log --name-only 显示每次commit修改的文件列表git log --name-status 查看commit记录里的文件修改状态git log --grep=&apos;abc&apos; 显示commit描述匹配abc的commit记录git log -S &quot;代码内容&quot; 按代码内容搜索commit记录，如果代码内容部分想用正则表达式，则将-S换成-Ggit log --pretty=&apos;%H %Cblue%cd %C(yellow)%cn %Cred%s&apos; 按commit号+提交日期+提交人+commit标题 显示pretty格式%H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字 %_ae 作者的电子邮件地址 （由于新浪博客显示问题，请去除 %_ae 中的 _ ）%_ad 作者修订日期（可以用 -date= 选项定制格式）（由于新浪博客显示问题，请去除 % ad 中的 _ ）%ar 作者修订日期，按多久以前的方式显示 %cn 提交者(committer)的名字 %_ce 提交者的电子邮件地址（由于新浪博客显示问题，请去除 %_ce 中的 _ ）%_cd 提交日期 （由于新浪博客显示问题，请去除 %_cd 中的 _ ）%cr 提交日期，按多久以前的方式显示 %d: ref名称%s: 提交的信息标题%b: 提交的信息内容%Cred: 切换到红色 %Cgreen: 切换到绿色 %_Cblue: 切换到蓝色 （由于新浪博客显示问题，请去除 %_Cblue 中的 _）%Creset: 重设颜色 %C(...): 制定颜色, as described in color.branch.* config option %n: 换行 查看未commit的本地修改12git diff git diff 文件名 git diff 比较commit之间的差异123git diff commit 比较HEAD与commit之间的差异git diff commit_1 commit_2 比较两个commit之间的差异git diff commit_1..commit_2 与git diff commit_1 commit_2 一样效果 从服务器拉代码git pull --rebase (推荐)会把本地未push得commit放到缓冲区，然后把远程最新版本拉过来，再应用本地commit，这样不会造成本地有新commit时，merge的效果。 git pull 直接更新，若本地和远端都有新commit，都执行自动merge。 拉分支git checkout -b branchName 创建本地新分支 git checkout -b branchName remotes/origin/branchName 以远端分支创建本地新分支 git push origin $newBranch:$newBranch 将本地分支提交到远端进行创建 删除远程分支123$ git push origin :master# 等同于$ git push origin --delete master 提交修改git add . 将修改加到stage状态区 git commint -m &quot;注释&quot; git push 推送所有分支 git push origin develop 只推送develop分支 添加文件git add -A 删除文件git rm 文件名 git rm -r 目录名 Pushgit push push所有分支 git push origin master 将本地主分支推到远程主分支 git push –u origin master 将本地主分支推到远程（如无远程主分支则创建，用于初始化远程仓库） git push origin &lt;local_branch&gt; 创建远程分支，origin是远程仓库名。 git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; 创建远程分支 强制push如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做git pull合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用–force选项。 git push --force origin 合并分支mergegit merge remotes/origin/mc-s-3 将远端mc-s-3分支merge到本地 rebasegit rebase develop git rebase remotes/origin/develop 配置mergetoolgit config –global merge.tool bc3 git config –global mergetool.bc3.path 软件执行文件地址 merge策略1234567891011Git merge 策略的总结:1、使用 -s 指定策略，使用 -X 指定策略的选项2、默认策略是recursive3、策略有 ours，但是没有theirs (Git老版本好像有)4、策略ours直接 忽略 合并分支的任何内容，只做简单的合并，保留分支改动的存在5、默认策略recursive有选项ours 和 theirs6、-s recursive -X ours 和 -s ours 不同，后者如第3点提到直接忽略内容，但是前者会做合并，遇到冲突时以自己的改动为主7、-s recursive -X theirs的对立面是 -s recursive -X ours`注：-s recursive -X ours 合并分支，冲突时以本地为主` 回退未commit的修改git checkout [path] 将指定路径的修改还原到最新版本 回退已commit，未push的修改git reset HEAD &lt;file&gt; --mixed 选项：默认的 --soft 选项：改动会回退到stage状态 --hard 选项：改动会直接丢失。 git rebase -i 想要删除的commit的前一个commit号。 出来的界面里，将想要删除的commit描述改为drop，保存即可。 回退已push的修改git revert 指定的commit号。跳出来的界面，选择要回退的commit内容（取消前面的#） 可以随便选某个commit删除 若revert一个merge的commit，则要指定parent 号 git revert commit 号 -m 1。 这样就选parent 1，那么parent 1又是哪一个呢？一般来说，如果你在master上mergezhc_branch,那么parent 1就是master，parent 2就是zhc_branch. 重排commit顺序git rebase -i commit号 出来的界面中，将列出来的commit行重新排序再保存，就等于修改commit顺序了。 修改commit的描述未push方法一： git rebase -i commit号 对应commit号前改为edit，保存。出来后git commit --amend。将commit描述修改掉，保存。 出来后再git rebase --continue即可。 方法二： git commit --amend 修改最近的一次commit 代码仓库迁移git clone --bare robbin_site robbin_site.git git remote remove origin git remote add origin git@120.27.160.167:ZCY/doc-round-1.git git push –-all -–progress origin 导出指定版本的代码版本 git archive -o ../updated.zip HEAD $(git diff --name-only HEAD^) 例如：git archive -o ./version.zip 指定commit号 或者 git archive --format zip -output &quot;./archive.zip&quot; HEAD tag功能创建taggit tag -a v1.0.0 -m &apos;备注&apos; 查看taggit tag 切换taggit checkout tag名 删除taggit tag -d v1.0.0 指定commit打taggit tag -a v1.0.0 commit号 发布标签git push origin v1.0.0 将本地v1.0.0标签推送到git服务器 git push origin -tags 将本地所有tag一次性推送到git服务器 创建补丁当前分支所有超前master的提交：git format-patch -M master 某次提交以后的所有patch:git format-patch 4e16 --4e16指的是commit名 从根到指定提交的所有patch:git format-patch --root 4e16 某两次提交之间的所有patch:git format-patch 365a..4e16 -o &lt;patch_dir&gt; --365a和4e16分别对应两次提交的名称 某次提交（含）之前的几次提交：git format-patch –n 07fe --n指patch数，07fe对应提交的名称 故，单次提交即为： git format-patch -1 07fe 应用补丁方法一（推荐）12345678910111、在同一个仓库下找到对应的commit号2、切换到对应分支下，git cherry-pick commit 号3、如果冲突，git mergetool 解决冲突。4、git status根据提示commit代码，并pushcherry-pick 一个commit区间git cherry-pick &lt;start-commit-id&gt;^..&lt;end-commit-id&gt; start-commit-id是版本树里较早的commitcherry-pick一个merge commitgit cherry-pick &lt;commit-id&gt; -m parent-number -m代表 --mainline实际例子：git cherry-pick 32b234 -m 1 1，2分别代表什么 查看未push到远程仓库的commit1、查看到未传送到远程代码库的提交次数12345 git status //只能看次数显示结果类似于这样：# On branch master# Your branch is ahead of &apos;origin/master&apos; by 2 commits. 2、查看到未传送到远程代码库的提交描述/说明12345git cherry -v显示结果类似于这样：+ b6568326134dc7d55073b289b07c4b3d64eff2e7 add default charset for table items_has_images+ 4cba858e87752363bd1ee8309c0048beef076c60 move Savant3 class into www/includes/class/ 3、查看到未传送到远程代码库的提交详情1234567891011121314git log master ^origin/master这是一个git log命令的过滤，^origin/master可改成其它分支。显示结果类似于这样：commit 4cba858e87752363bd1ee8309c0048beef076c60Author: Zam &lt;zam@iaixue.com&gt;Date: Fri Aug 9 16:14:30 2013 +0800 move Savant3 class into www/includes/class/commit b6568326134dc7d55073b289b07c4b3d64eff2e7Author: Zam &lt;zam@iaixue.com&gt;Date: Fri Aug 9 16:02:09 2013 +0800 add default charset for table items_has_images 查看两个分支的差异查看dev中有，而master中没有的1234git log dev ^master反之：查看master中有，dev中没有的git log master ^dev 查看dev中比master多了哪些提交（A比B多了哪些，就把A放..右边）1git log master..dev 不在乎谁多谁少，只想看差异的提交1git log --left-right dev...master #--left-right 会帮助显示差异的commit属于哪个分支 整个目录比较差异详情1git difftool develop..pre-online --dir Git stash 暂存1234567891011121314151617git stash 将当前工作区里未commit的修改放到暂存区，将代码恢复到最近的一次修改git stash list 查看暂存区的列表git show stash@&#123;0&#125; see the last stash git stash pop apply lastest stash and remove it from th list git stash clear 清空暂存栈git stash apply stash@&#123;1&#125; 指定暂存区里的某一次stash，应用到本地 删除本地git branch -a 能看到，而远程已经删掉的分支记录1git fetch -p 更改时间显示方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647--date=(relative|local|default|iso|rfc|short|raw) Only takes effect for dates shown in human-readable format, such as when using &quot;--pretty&quot;. log.date config variable sets a default value for log command’s --date option.--date=relative shows dates relative to the current time, e.g. &quot;2 hours ago&quot;.--date=local shows timestamps in user’s local timezone.--date=iso (or --date=iso8601) shows timestamps in ISO 8601 format.--date=rfc (or --date=rfc2822) shows timestamps in RFC 2822 format, often found in E-mail messages.--date=short shows only date but not time, in YYYY-MM-DD format.--date=raw shows the date in the internal raw git format %s %z format.--date=default shows timestamps in the original timezone (either committer’s or author’s).####格式化显示例子：--date=format:&apos;%Y-%m-%d %H:%M:%S&apos;参数：%a Abbreviated weekday name%A Full weekday name%b Abbreviated month name%B Full month name%c Date and time representation appropriate for locale%d Day of month as decimal number (01 – 31)%H Hour in 24-hour format (00 – 23)%I Hour in 12-hour format (01 – 12)%j Day of year as decimal number (001 – 366)%m Month as decimal number (01 – 12)%M Minute as decimal number (00 – 59)%p Current locale&apos;s A.M./P.M. indicator for 12-hour clock%S Second as decimal number (00 – 59)%U Week of year as decimal number, with Sunday as first day of week (00 – 53)%w Weekday as decimal number (0 – 6; Sunday is 0)%W Week of year as decimal number, with Monday as first day of week (00 – 53)%x Date representation for current locale%X Time representation for current locale%y Year without century, as decimal number (00 – 99)%Y Year with century, as decimal number%z, %Z Either the time-zone name or time zone abbreviation, depending on registry settings; no characters if time zone is unknown%% Percent sign 全局更改方式1git config --global log.date relative 代码量统计当天提交的代码量1git log --author=&quot;$(git config --get user.name)&quot; --no-merges --since=1am --stat 统计报告-gitstats 用GitStatX图形化工具查看 统计报告-gitinspector1gitinspector --format=html --since=2018-01-01 --until=2018-12-30 --timeline --localize-output -w ./ &gt; ~/tmp/gitinspector/zcy-payment-center-201801.html gitinspector命令说明123456789101112131415161718192021222324252627282930313233343536➜ car-manage git:(master) gitinspector --help用法：/usr/local/bin/gitinspector [选项]... [目录] 在目录列出有关库的信息,如果没有指定目录，那么将使用现目录。如果有多个目录，将采用指定的最后一个目录长选项的强制性参数对短选项也适用布尔参数只能给予长选项 -f, --file-types=EXTENSIONS 一串逗号分隔的文件类型 这些文件将会被用于计算统计数据. 默认文件类型: java,c,cc,cpp,h,hh,hpp,py,glsl,rb,js,sql -F, --format=FORMAT 指定生成的输出文件的格式； 默认格式是&apos;text&apos; 和 可选格式: html,htmlembedded,text,xml --grading[=BOOL] 按照学生成评判项目的格式， 显示统计数据和信息； 等同于 -HlmrTw 选项 -H, --hard[=BOOL] 记录行数并且寻找重复的内容; 如果数据库较大，这个可能会需要一些时间 -l, --list-file-types[=BOOL] 列出所有现在的数据库分支的文件格式 -L, --localize-output[=BOOL] 在翻译版本存在的前提下，将输出结果翻译到系统语言 -m --metrics[=BOOL] 在分析提交时，检查特定指标 -r --responsibilities[=BOOL] 显示每位作者主要职责 --since=DATE 只显示从特定时间起的结果 -T, --timeline[=BOOL] 显示提交时间轴, 包括作者名称 --until=DATE 只显示特定时间前的结果 -w, --weeks[=BOOL] 按周来显示统计数据，而非月 -x, --exclude=PATTERN 按特定格式排除不应该被统计 的文件，作者名字或邮箱;可以按文件名，作者名， 作者邮箱。可以重复 -h, --help 显示这个帮助信息并退出 --version 显示版本信息并退出gitinspector 会过滤信息并且仅统计那些修改，增加或减少，指定文件类型的提交，如需详细信息，请参考 -f 或 --file-types 选项]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
</search>
